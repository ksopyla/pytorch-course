{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa1af409",
   "metadata": {},
   "source": [
    "# Tensor Metamorphosis: Shape-Shifting Mastery\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb19fad",
   "metadata": {},
   "source": [
    "**Module 1 | Lesson 2b**\n",
    "\n",
    "---\n",
    "\n",
    "## Professor Torchenstein's Grand Directive\n",
    "\n",
    "Ah, my brilliant apprentice! Do you feel it? That electric tingle of mastery coursing through your neural pathways? You have learned to **slice** tensors with surgical precision and **fuse** them into magnificent constructions! But now... NOW we transcend mere cutting and pasting!\n",
    "\n",
    "Today, we unlock the ultimate power: **METAMORPHOSIS**! We shall transform the very **essence** of tensor structure without disturbing a single precious datum within! Think of it as the most elegant magic‚Äîchanging form while preserving the soul!\n",
    "\n",
    "**\"Behold! We shall `reshape()` reality itself and make dimensions `unsqueeze()` from the void! The tensors... they will obey our geometric commands!\"**\n",
    "\n",
    "![pytorch tensors everywhere](/assets/images/torchenstein_presenting_cube.png)\n",
    "\n",
    "---\n",
    "\n",
    "### Your Mission Briefing\n",
    "\n",
    "By the time you emerge from this metamorphosis chamber, you will command the arcane arts of:\n",
    "\n",
    "*   **üîÑ The Great Reshape & View Metamorphosis:** Transform tensor structures with `torch.reshape()` and `torch.view()` while understanding memory layout secrets.\n",
    "*   **üóúÔ∏è The Squeeze & Unsqueeze Dimension Dance:** Add and remove dimensions of size 1 with surgical precision using `squeeze()` and `unsqueeze()`.\n",
    "*   **üöÄ The Expand & Repeat Replication Magic:** Efficiently expand data with `torch.expand()` or fully replicate it with `torch.repeat()`.\n",
    "*   **üìä Specialized Shape Sorcery:** Flatten complex structures into submission with `torch.flatten()` and restore them with `torch.unflatten()`.\n",
    "\n",
    "**Estimated Time to Completion:** 20 minutes of pure shape-shifting enlightenment.\n",
    "\n",
    "**What You'll Need:**\n",
    "*   The wisdom from our previous experiments: [tensor summoning](01_introduction_to_tensors.ipynb) and [tensor surgery](02a_tensor_manipulation.ipynb).\n",
    "*   A willingness to bend reality to your computational will!\n",
    "*   Your PyTorch laboratory, humming with metamorphic potential.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b33231",
   "metadata": {},
   "source": [
    "## Part 1: The Great Reshape & View Metamorphosis üîÑ\n",
    "\n",
    "### The Theory Behind the Magic\n",
    "\n",
    "First, we must understand the incantation before we cast the spell! In the realm of tensor metamorphosis, we have two powerful spells for transforming shape:\n",
    "\n",
    "**What is `torch.reshape()` and `torch.view()`?**\n",
    "Both functions change the **shape** of a tensor without altering the data itself. Think of it like this: you have a chocolate bar that's broken into 12 pieces arranged in a 3√ó4 grid. You can rearrange these same 12 pieces into a 2√ó6 grid, or a 1√ó12 line, or even a 4√ó3 grid‚Äîyou still have exactly the same 12 pieces of chocolate, just in a different arrangement!\n",
    "\n",
    "**The Critical Rule:** The total number of elements must remain the same! If you start with 12 elements, you can only reshape to combinations that multiply to 12: (1√ó12), (2√ó6), (3√ó4), (4√ó3), (6√ó2), (12√ó1).\n",
    "\n",
    "**The Subtle Difference:**\n",
    "- **`torch.reshape()`**: The diplomatic wizard! If the current memory layout allows it, it returns a view (no copying). If not, it creates a new tensor (copying data). It always succeeds if the shape is valid.\n",
    "- **`torch.view()`**: The strict purist! It ONLY works if the tensor's memory is contiguous. If not, it throws an error and demands you fix the memory layout first. But when it works, it's guaranteed to be a view (no copying).\n",
    "\n",
    "### Visualizing the Metamorphosis\n",
    "\n",
    "Imagine your tensor as a magical scroll where the data is written in a single line, but you can choose to **read** it in different rectangular patterns!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2339e00e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Set the seed for cosmic consistency in our metamorphosis experiments\n",
    "torch.manual_seed(42)\n",
    "\n",
    "print(\"üî¨ THE GREAT RESHAPE & VIEW METAMORPHOSIS EXPERIMENT\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Create our test subject: a simple tensor with easily trackable values\n",
    "original_tensor = torch.arange(1, 13)  # Numbers 1 through 12\n",
    "print(f\"üìä Our original test subject:\")\n",
    "print(f\"   Shape: {original_tensor.shape}\")  \n",
    "print(f\"   Data: {original_tensor}\")\n",
    "print(f\"   Total elements: {original_tensor.numel()}\")\n",
    "\n",
    "print(f\"\\nüß™ METAMORPHOSIS 1: From 1D line to 2D grid!\")\n",
    "\n",
    "# Reshape to 3x4 grid (3 rows, 4 columns)\n",
    "grid_3x4 = original_tensor.reshape(3, 4)\n",
    "print(f\"üìê Reshaped to 3x4 grid:\")\n",
    "print(f\"   Shape: {grid_3x4.shape}\")\n",
    "print(f\"   Data:\\n{grid_3x4}\")\n",
    "\n",
    "# Try view (should work since original_tensor is contiguous)\n",
    "grid_view = original_tensor.view(4, 3)  # 4 rows, 3 columns\n",
    "print(f\"\\nüîÑ Using view() to create 4x3 arrangement:\")\n",
    "print(f\"   Shape: {grid_view.shape}\")\n",
    "print(f\"   Data:\\n{grid_view}\")\n",
    "\n",
    "print(f\"\\nüîç VERIFICATION: All contain the same data!\")\n",
    "print(f\"   Original total: {original_tensor.sum().item()}\")\n",
    "print(f\"   Reshape total: {grid_3x4.sum().item()}\")  \n",
    "print(f\"   View total: {grid_view.sum().item()}\")\n",
    "print(f\"   All equal? {original_tensor.sum() == grid_3x4.sum() == grid_view.sum()}\")\n",
    "\n",
    "print(f\"\\nüí° Key Insight: Same data, different perspectives!\")\n",
    "print(f\"   Original: {original_tensor.numel()} elements in shape {original_tensor.shape}\")\n",
    "print(f\"   Reshaped: {grid_3x4.numel()} elements in shape {grid_3x4.shape}\")  \n",
    "print(f\"   Viewed:   {grid_view.numel()} elements in shape {grid_view.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d32577",
   "metadata": {},
   "source": [
    "### The Contiguous Memory Mystery! üß©\n",
    "\n",
    "Now for a deeper secret that separates the masters from the apprentices! Sometimes `view()` will refuse to obey and throw an error. This happens when the tensor's memory is not **contiguous**‚Äîmeaning the data isn't laid out in a nice, sequential order in memory.\n",
    "\n",
    "**When does this happen?**\n",
    "- After certain operations like `transpose()`, `permute()`, or advanced slicing\n",
    "- The tensor's data becomes \"scattered\" in memory \n",
    "\n",
    "**The Solutions:**\n",
    "- Use `.contiguous()` to reorganize the memory layout, then `view()` will work\n",
    "- Or simply use `reshape()` which handles this automatically\n",
    "\n",
    "Let's witness this phenomenon in action!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc39bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"üß© THE CONTIGUOUS MEMORY MYSTERY EXPERIMENT\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "# Start with our 3x4 grid from before\n",
    "print(\"üìä Starting with a contiguous 3x4 tensor:\")\n",
    "matrix = torch.arange(1, 13).reshape(3, 4)\n",
    "print(f\"   Shape: {matrix.shape}\")\n",
    "print(f\"   Contiguous? {matrix.is_contiguous()}\")\n",
    "print(f\"   Data:\\n{matrix}\")\n",
    "\n",
    "# Transpose it - this creates a non-contiguous tensor!\n",
    "print(f\"\\nüîÑ After transpose() - memory becomes scattered:\")\n",
    "transposed = matrix.t()  # Same as matrix.transpose(0, 1)\n",
    "print(f\"   Shape: {transposed.shape}\")\n",
    "print(f\"   Contiguous? {transposed.is_contiguous()}\")\n",
    "print(f\"   Data:\\n{transposed}\")\n",
    "\n",
    "# Now try view() - this will FAIL!\n",
    "print(f\"\\n‚ùå Trying view() on non-contiguous tensor:\")\n",
    "try:\n",
    "    failed_view = transposed.view(-1)  # Try to flatten\n",
    "    print(\"   Success!\")\n",
    "except RuntimeError as e:\n",
    "    print(f\"   ERROR: {str(e)}\")\n",
    "\n",
    "# Solution 1: Make it contiguous first, then view\n",
    "print(f\"\\n‚úÖ Solution 1: Make contiguous, then view:\")\n",
    "contiguous_version = transposed.contiguous()\n",
    "print(f\"   After contiguous(): {contiguous_version.is_contiguous()}\")\n",
    "successful_view = contiguous_version.view(-1)\n",
    "print(f\"   Flattened shape: {successful_view.shape}\")\n",
    "print(f\"   Data: {successful_view}\")\n",
    "\n",
    "# Solution 2: Use reshape() - it handles this automatically!\n",
    "print(f\"\\n‚úÖ Solution 2: Use reshape() - it's smarter:\")\n",
    "auto_flattened = transposed.reshape(-1)\n",
    "print(f\"   Flattened shape: {auto_flattened.shape}\")\n",
    "print(f\"   Data: {auto_flattened}\")\n",
    "\n",
    "print(f\"\\nüéØ Professor's Wisdom: When in doubt, use reshape()!\")\n",
    "print(f\"   It's the diplomatic solution that always works!\")\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
