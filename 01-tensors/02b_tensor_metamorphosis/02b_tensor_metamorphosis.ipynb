{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa1af409",
   "metadata": {},
   "source": [
    "# Tensor Metamorphosis: Shape-Shifting Mastery\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb19fad",
   "metadata": {},
   "source": [
    "**Module 1 | Lesson 2b**\n",
    "\n",
    "---\n",
    "\n",
    "## Professor Torchenstein's Grand Directive\n",
    "\n",
    "Ah, my brilliant apprentice! Do you feel it? That electric tingle of mastery coursing through your neural pathways? You have learned to **slice** tensors with surgical precision and **fuse** them into magnificent constructions! But now... NOW we transcend mere cutting and pasting!\n",
    "\n",
    "Today, we unlock the ultimate power: **METAMORPHOSIS**! We shall transform the very **essence** of tensor structure without disturbing a single precious datum within! Think of it as the most elegant magic—changing form while preserving the soul!\n",
    "\n",
    "**\"Behold! We shall `reshape()` reality itself and make dimensions `unsqueeze()` from the void! The tensors... they will obey our geometric commands!\"**\n",
    "\n",
    "![Torchenstein holding motherboard](/assets/images/torchenstein_holding_motherboard.png)\n",
    "\n",
    "---\n",
    "\n",
    "### Your Mission Briefing\n",
    "\n",
    "By the time you emerge from this metamorphosis chamber, you will command the arcane arts of:\n",
    "\n",
    "*   **🔄 The Great Reshape & View Metamorphosis:** Transform tensor structures with `torch.reshape()` and `torch.view()` while understanding memory layout secrets.\n",
    "*   **🗜️ The Squeeze & Unsqueeze Dimension Dance:** Add and remove dimensions of size 1 with surgical precision using `squeeze()` and `unsqueeze()`.\n",
    "*   **🚀 The Expand & Repeat Replication Magic:** Efficiently expand data with `torch.expand()` or fully replicate it with `torch.repeat()`.\n",
    "*   **📊 Specialized Shape Sorcery:** Flatten complex structures into submission with `torch.flatten()` and restore them with `torch.unflatten()`.\n",
    "\n",
    "**Estimated Time to Completion:** 20 minutes of pure shape-shifting enlightenment.\n",
    "\n",
    "**What You'll Need:**\n",
    "*   The wisdom from our previous experiments: [tensor summoning](01_introduction_to_tensors.ipynb) and [tensor surgery](02a_tensor_manipulation.ipynb).\n",
    "*   A willingness to bend reality to your computational will!\n",
    "*   Your PyTorch laboratory, humming with metamorphic potential.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b33231",
   "metadata": {},
   "source": [
    "## Part 1: Memory Layout Foundations 🧱\n",
    "\n",
    "### The Deep Theory Behind Memory Layout Magic\n",
    "\n",
    "Ah, my curious apprentice! To truly master tensor metamorphosis, you must understand the **fundamental secret** that lies beneath: **how tensors live in your computer's memory**! This knowledge will separate you from the mere code-monkeys and elevate you to the ranks of true PyTorch sorcerers!\n",
    "\n",
    "***The Universal Truth: Everything is a 1D Array! 📏***\n",
    "\n",
    "**No matter what your computer architecture** (x86, ARM, M1, GPU), **all memory is fundamentally a giant 1D array**! Whether you're on Windows, Linux, or macOS, whether you have an Intel chip or Apple Silicon—memory is just a long, sequential line of storage locations:\n",
    "\n",
    "```\n",
    "Computer Memory (Always 1D):\n",
    "[addr_0][addr_1][addr_2][addr_3][addr_4][addr_5][addr_6][addr_7]...\n",
    "```\n",
    "\n",
    "**The Multi-Dimensional Illusion:**\n",
    "When we have a \"2D tensor\" or \"3D tensor,\" it's really just our **interpretation** of how to read this 1D memory! The computer doesn't care about rows and columns—that's just how WE choose to organize and access the data.\n",
    "\n",
    "### Row-Major vs Column-Major: The Ancient Battle! ⚔️\n",
    "\n",
    "There are two ways to store multi-dimensional data in this 1D memory:\n",
    "\n",
    "**🇨 Row-Major (C-style) - PyTorch's Choice:**\n",
    "Store data row by row, left to right, then move to the next row.\n",
    "\n",
    "**🇫 Column-Major (Fortran-style):**  \n",
    "Store data column by column, top to bottom, then move to the next column.\n",
    "\n",
    "Let's visualize this with a 3×4 matrix containing numbers 1-12:\n",
    "\n",
    "```\n",
    "Visual Matrix:\n",
    "[ 1  2  3  4]\n",
    "[ 5  6  7  8]  \n",
    "[ 9 10 11 12]\n",
    "\n",
    "Row-Major Memory Layout (PyTorch default):\n",
    "Memory: [1][2][3][4][5][6][7][8][9][10][11][12]\n",
    "        └─  row1  ─┘└─  row2  ─┘└─   row3   ──┘\n",
    "\n",
    "Column-Major Memory Layout (Not PyTorch):\n",
    "Memory: [1][5][9][2][6][10][3][7][11][4][8][12]\n",
    "        └ col1  ┘└ col2   ┘└─ col3 ─┘└─ col4 ─┘\n",
    "```\n",
    "\n",
    "**PyTorch uses Row-Major** because it's the standard for C/C++ and most modern systems! This is **not dependent on your OS or hardware**—it's a software design choice.\n",
    "\n",
    "### What Makes Memory \"Contiguous\"? 🧩\n",
    "\n",
    "**Contiguous Memory access:** You try to read the tensor's elements in the **expected sequential order** in the 1D memory array.\n",
    "\n",
    "**Non-Contiguous Memory access:** You try to get the tensor's elements which are scattered—they exist in memory but not in the order you'd expect when reading row by row.\n",
    "\n",
    "### The Transpose Tragedy - Why Memory Becomes Non-Contiguous\n",
    "\n",
    "Let's witness the moment when contiguous memory becomes scattered:\n",
    "\n",
    "```\n",
    "Original 3×4 Tensor (Contiguous):\n",
    "Visual:           Memory Layout:\n",
    "[ 1  2  3  4]     [1][2][3][4][5][6][7][8][9][10][11][12]\n",
    "[ 5  6  7  8]  →  \n",
    "[ 9 10 11 12]    \n",
    "\n",
    "After Transpose to 4×3 (Non-Contiguous):\n",
    "Visual:          Expected Memory for New Shape:\n",
    "[ 1  5  9]       [1][5][9][2][6][10][3][7][11][4][8][12]\n",
    "[ 2  6 10]  \n",
    "[ 3  7 11]       But ACTUAL memory is still:\n",
    "[ 4  8 12]       [1][2][3][4][5][6][7][8][9][10][11][12]\n",
    "```\n",
    "\n",
    "**The Problem:** To read row 1 of the transposed tensor `[1, 5, 9]`, PyTorch must jump around in memory: address 0 → address 4 → address 8. This \"jumping around\" makes it non-contiguous!\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "394507a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔬 MEMORY LAYOUT IN ACTION - ROW-MAJOR DEMONSTRATION\n",
      "=================================================================\n",
      "🧠 Raw Data in Computer Memory (1D Reality):\n",
      "   Memory: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "   Shape: torch.Size([12]) ← This is how it ACTUALLY lives!\n",
      "\n",
      "📐 ROW-MAJOR INTERPRETATION AS 3×4 MATRIX:\n",
      "   Same memory: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "   But interpreted as 3×4:\n",
      "tensor([[ 1,  2,  3,  4],\n",
      "        [ 5,  6,  7,  8],\n",
      "        [ 9, 10, 11, 12]])\n",
      "   💡 Row 1: [1,2,3,4] from memory positions 0-3\n",
      "   💡 Row 2: [5,6,7,8] from memory positions 4-7\n",
      "   💡 Row 3: [9,10,11,12] from memory positions 8-11\n",
      "\n",
      "🔄 DIFFERENT INTERPRETATION: 4×3 MATRIX:\n",
      "   Same memory: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "   But interpreted as 4×3:\n",
      "tensor([[ 1,  2,  3],\n",
      "        [ 4,  5,  6],\n",
      "        [ 7,  8,  9],\n",
      "        [10, 11, 12]])\n",
      "   💡 Row 1: [1,2,3], Row 2: [4,5,6], Row 3: [7,8,9], Row 4: [10,11,12]\n",
      "\n",
      "✨ THE FUNDAMENTAL INSIGHT:\n",
      "   - Memory never changes: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "   - Only our INTERPRETATION changes!\n",
      "   - This is the foundation of tensor metamorphosis!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Set the seed for cosmic consistency\n",
    "torch.manual_seed(42)\n",
    "\n",
    "print(\"🔬 MEMORY LAYOUT IN ACTION - ROW-MAJOR DEMONSTRATION\")\n",
    "print(\"=\" * 65)\n",
    "\n",
    "# Create our test subject: numbers 1-12 in sequential memory\n",
    "data = torch.arange(1, 13)  \n",
    "print(\"🧠 Raw Data in Computer Memory (1D Reality):\")\n",
    "print(f\"   Memory: {data.tolist()}\")\n",
    "print(f\"   Shape: {data.shape} ← This is how it ACTUALLY lives!\")\n",
    "\n",
    "print(f\"\\n📐 ROW-MAJOR INTERPRETATION AS 3×4 MATRIX:\")\n",
    "matrix_3x4 = data.reshape(3, 4)\n",
    "print(f\"   Same memory: {data.tolist()}\")\n",
    "print(f\"   But interpreted as 3×4:\")\n",
    "print(matrix_3x4)\n",
    "print(f\"   💡 Row 1: [1,2,3,4] from memory positions 0-3\")\n",
    "print(f\"   💡 Row 2: [5,6,7,8] from memory positions 4-7\")\n",
    "print(f\"   💡 Row 3: [9,10,11,12] from memory positions 8-11\")\n",
    "\n",
    "print(f\"\\n🔄 DIFFERENT INTERPRETATION: 4×3 MATRIX:\")\n",
    "matrix_4x3 = data.reshape(4, 3)  \n",
    "print(f\"   Same memory: {data.tolist()}\")\n",
    "print(f\"   But interpreted as 4×3:\")\n",
    "print(matrix_4x3)\n",
    "print(f\"   💡 Row 1: [1,2,3], Row 2: [4,5,6], Row 3: [7,8,9], Row 4: [10,11,12]\")\n",
    "\n",
    "print(f\"\\n✨ THE FUNDAMENTAL INSIGHT:\")\n",
    "print(f\"   - Memory never changes: {data.tolist()}\")\n",
    "print(f\"   - Only our INTERPRETATION changes!\")\n",
    "print(f\"   - This is the foundation of tensor metamorphosis!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dad10b8",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "🧟‍♂️ **Torchenstein's Insight!** ✨\n",
    "\n",
    "> **\"Remember, dear tensor alchemist:**\n",
    "> **Shape and form are but illusions!**  \n",
    "> **The memory remains unchanged—it's only our interpretation that morphs!**  \n",
    "> 🪄🔬\n",
    " _– Prof. Torchenstein_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852da0c9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "##  PyTorch's Memory Management System 🏭\n",
    "\n",
    "Now that we understand how memory is fundamentally organized, let's explore PyTorch's sophisticated system for managing that memory!\n",
    "\n",
    "### 🧠 **Storage and data_ptr() - The Complete Picture**\n",
    "\n",
    "PyTorch try to encapsulate how the tensor data is stored in memory and how we can access it.\n",
    "\n",
    "**📦 `tensor.storage()`** - The Storage Container of whole tensor data\n",
    "- **What it is:** PyTorch's high-level Storage object that holds the actual data buffer\n",
    "- **When shared:** Multiple tensors could reference the same underlying data (storage), but to its differen parts eg. last row \n",
    "- **Think of it as:** The entire memory \"warehouse\" that holds whole tensor data\n",
    "\n",
    "**🎯 `tensor.data_ptr()`** - The Memory Address of particular tensor or its part\n",
    "- **What it is:** Raw memory address (integer) pointing to where this tensor's data begins\n",
    "- **When different:** When tensors are views of different parts of the same storage\n",
    "- **Think of it as:** The specific \"shelf location\" within the warehouse\n",
    "\n",
    "**💡 The Memory Sharing Scenarios:**\n",
    "\n",
    "| Scenario | Same Storage? | Same data_ptr? | Example |\n",
    "|----------|---------------|----------------|---------|\n",
    "| **True copy** | ❌ No | ❌ No | `tensor.clone()` |\n",
    "| **Shape change** | ✅ Yes | ✅ Yes | `tensor.reshape(3,4)` |\n",
    "| **Slice view** | ✅ Yes | ❌ No | `tensor[2:]` |\n",
    "\n",
    "\n",
    "Let's witness this PyTorch memory system in action!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "486d5999",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏭 PYTORCH'S MEMORY MANAGEMENT IN ACTION\n",
      "=======================================================\n",
      "Original tensor: tensor([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12])\n",
      "\n",
      "📐 SCENARIO 1: Shape Change (reshape)\n",
      "   Reshaped: \n",
      "tensor([[ 1,  2,  3,  4],\n",
      "        [ 5,  6,  7,  8],\n",
      "        [ 9, 10, 11, 12]])\n",
      "   📦 Same storage? True\n",
      "   🎯 Same data_ptr? True\n",
      "\n",
      "✂️ SCENARIO 2: Slice View\n",
      "   Sliced tensor: tensor([ 5,  6,  7,  8,  9, 10, 11, 12])\n",
      "   📦 Same storage? True\n",
      "   🎯 Same data_ptr? False\n",
      "   🧮 Memory offset: 32 bytes = 4 elements\n",
      "\n",
      "📋 SCENARIO 3: True Copy (clone)\n",
      "   Cloned tensor: tensor([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12])\n",
      "   📦 Same storage? False\n",
      "   🎯 Same data_ptr? False\n",
      "\n",
      "💡 PYTORCH'S MEMORY EFFICIENCY:\n",
      "   - Reshape: FREE! (same memory, different interpretation)\n",
      "   - Slice: EFFICIENT! (same memory, different starting point)\n",
      "   - Clone: EXPENSIVE! (new memory allocation)\n"
     ]
    }
   ],
   "source": [
    "print(\"🏭 PYTORCH'S MEMORY MANAGEMENT IN ACTION\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "# Create original tensor  \n",
    "original = torch.arange(1, 13)\n",
    "print(f\"Original tensor: {original}\")\n",
    "\n",
    "# Scenario 1: Shape change (should share storage AND data_ptr)\n",
    "reshaped = original.reshape(3, 4)\n",
    "print(f\"\\n📐 SCENARIO 1: Shape Change (reshape)\")\n",
    "print(f\"   Reshaped: \\n{reshaped}\")\n",
    "\n",
    "print(f\"   📦 Same storage? {original.storage().data_ptr() == reshaped.storage().data_ptr()}\")\n",
    "print(f\"   🎯 Same data_ptr? {original.data_ptr() == reshaped.data_ptr()}\")\n",
    "\n",
    "# Scenario 2: Slice view (should share storage but DIFFERENT data_ptr)\n",
    "sliced = original[4:]  # Elements from index 4 onwards\n",
    "print(f\"\\n✂️ SCENARIO 2: Slice View\")\n",
    "print(f\"   Sliced tensor: {sliced}\")\n",
    "print(f\"   📦 Same storage? {original.storage().data_ptr() == sliced.storage().data_ptr()}\")\n",
    "print(f\"   🎯 Same data_ptr? {original.data_ptr() == sliced.data_ptr()}\")\n",
    "\n",
    "# Calculate the offset for sliced tensor\n",
    "element_size = original.element_size()\n",
    "offset = sliced.data_ptr() - original.data_ptr()\n",
    "print(f\"   🧮 Memory offset: {offset} bytes = {offset // element_size} elements\")\n",
    "\n",
    "# Scenario 3: True copy (different storage AND data_ptr)\n",
    "copied = original.clone()\n",
    "print(f\"\\n📋 SCENARIO 3: True Copy (clone)\")\n",
    "print(f\"   Cloned tensor: {copied}\")\n",
    "print(f\"   📦 Same storage? {original.storage().data_ptr() == copied.storage().data_ptr()}\")\n",
    "print(f\"   🎯 Same data_ptr? {original.data_ptr() == copied.data_ptr()}\")\n",
    "\n",
    "print(f\"\\n💡 PYTORCH'S MEMORY EFFICIENCY:\")\n",
    "print(f\"   - Reshape: FREE! (same memory, different interpretation)\")\n",
    "print(f\"   - Slice: EFFICIENT! (same memory, different starting point)\")  \n",
    "print(f\"   - Clone: EXPENSIVE! (new memory allocation)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f5b8a5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## torch.view() - The Memory-Efficient Shape Changer 👁️\n",
    "\n",
    "Now that we understand WHY we need shape transformation, let's master the first tool: `torch.view()`!\n",
    "\n",
    "### 🎯 **What is torch.view() and What is it FOR?**\n",
    "\n",
    "**`torch.view()`** is PyTorch's **memory-efficient** shape transformation method. It creates a new tensor with a different shape that **shares the same underlying data** as the original tensor.\n",
    "\n",
    "**🚀 Use `view()` when:**\n",
    "- You want **maximum performance** (no data copying)\n",
    "- You know your tensor has **contiguous memory** layout  \n",
    "- You need **guaranteed memory sharing** (changes to one tensor affect the other)\n",
    "\n",
    "**⚠️ Limitations:**\n",
    "- **Requires contiguous memory** - fails if memory is scattered\n",
    "- **Throws error** rather than automatically fixing problems\n",
    "- **Purist approach** - no fallback mechanisms\n",
    "\n",
    "### 📐 **How view() Works: The Shape Mathematics**\n",
    "\n",
    "The **Golden Rule:** Total elements must remain constant!\n",
    "\n",
    "```\n",
    "Original shape: (A, B, C, D)  → Total elements: A × B × C × D\n",
    "New shape:      (W, X, Y, Z)  → Total elements: W × X × Y × Z\n",
    "\n",
    "Valid only if: A × B × C × D = W × X × Y × Z\n",
    "```\n",
    "\n",
    "**🔢 The Magic `-1` Parameter:**\n",
    "Use `-1` in one dimension to let PyTorch calculate it automatically:\n",
    "```python\n",
    "tensor.view(batch_size, -1)  # PyTorch figures out the second dimension\n",
    "```\n",
    "\n",
    "Let's see `view()` in action with real examples!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d7f61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"👁️ TORCH.VIEW() MASTERCLASS\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Create a contiguous tensor for our experiments  \n",
    "data = torch.arange(24)  # 24 elements: 0, 1, 2, ..., 23\n",
    "print(f\"Original data: {data}\")\n",
    "print(f\"Shape: {data.shape}, Elements: {data.numel()}\")\n",
    "\n",
    "print(f\"\\n✅ SUCCESS SCENARIOS - view() works perfectly:\")\n",
    "\n",
    "# Scenario 1: 1D to 2D\n",
    "matrix_4x6 = data.view(4, 6)\n",
    "print(f\"   1D→2D: {data.shape} → {matrix_4x6.shape}\")\n",
    "print(f\"   Calculation: 24 elements = 4×6? {4*6 == 24} ✓\")\n",
    "\n",
    "# Scenario 2: Using -1 for automatic calculation\n",
    "auto_matrix = data.view(3, -1)  # PyTorch calculates: 24/3 = 8\n",
    "print(f\"   Auto-calc: {data.shape} → {auto_matrix.shape}\")\n",
    "print(f\"   PyTorch figured out: 24/3 = 8\")\n",
    "\n",
    "# Scenario 3: 1D to 3D (more complex)\n",
    "cube_2x3x4 = data.view(2, 3, 4)\n",
    "print(f\"   1D→3D: {data.shape} → {cube_2x3x4.shape}\")\n",
    "print(f\"   Calculation: 24 elements = 2×3×4? {2*3*4 == 24} ✓\")\n",
    "\n",
    "# Scenario 4: Memory sharing verification\n",
    "print(f\"\\n🔗 MEMORY SHARING TEST:\")\n",
    "print(f\"   Original data_ptr: {data.data_ptr()}\")\n",
    "print(f\"   Matrix data_ptr:   {matrix_4x6.data_ptr()}\")  \n",
    "print(f\"   Same memory? {data.data_ptr() == matrix_4x6.data_ptr()} ✓\")\n",
    "\n",
    "# Modify original - should affect the view!\n",
    "data[0] = 999\n",
    "print(f\"   Changed data[0] to 999...\")\n",
    "print(f\"   Matrix[0,0] is now: {matrix_4x6[0,0]} (shares memory!)\")\n",
    "\n",
    "print(f\"\\n❌ FAILURE SCENARIOS - view() throws errors:\")\n",
    "\n",
    "# Reset data\n",
    "data = torch.arange(24) \n",
    "\n",
    "# Error 1: Impossible shape (wrong total elements)\n",
    "try:\n",
    "    impossible = data.view(5, 5)  # 5×5=25, but we have 24 elements\n",
    "    print(\"   Impossible shape: Success?!\")\n",
    "except RuntimeError as e:\n",
    "    print(f\"   ❌ Impossible shape (5×5=25≠24): {str(e)[:50]}...\")\n",
    "\n",
    "# Error 2: Non-contiguous memory (after transpose)\n",
    "matrix = data.view(4, 6)\n",
    "transposed = matrix.t()  # Creates non-contiguous memory\n",
    "print(f\"   Non-contiguous tensor: {transposed.is_contiguous()}\")\n",
    "try:\n",
    "    flattened = transposed.view(-1)\n",
    "    print(\"   view() on non-contiguous: Success?!\")\n",
    "except RuntimeError as e:\n",
    "    print(f\"   ❌ Non-contiguous memory: {str(e)[:50]}...\")\n",
    "\n",
    "print(f\"\\n💡 view() GOLDEN RULES:\")\n",
    "print(f\"   1. Total elements must remain the same\")\n",
    "print(f\"   2. Memory must be contiguous\")  \n",
    "print(f\"   3. Use -1 for automatic dimension calculation\")\n",
    "print(f\"   4. Memory is always shared (super efficient!)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b078823",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## torch.reshape() - The Diplomatic Shape Changer 🤝\n",
    "\n",
    "Now let's master `torch.reshape()` - the more forgiving, intelligent cousin of `view()`!\n",
    "\n",
    "### 🎯 What is torch.reshape() and What is it FOR?\n",
    "\n",
    "**`torch.reshape()`** is PyTorch's **diplomatic** shape transformation method. It tries to return a view when possible, but creates a copy when necessary to ensure the operation always succeeds.\n",
    "\n",
    "**🤝 Use `reshape()` when:**\n",
    "- You want **reliability over maximum performance**\n",
    "- You're not sure if your tensor memory is contiguous\n",
    "- You want PyTorch to **handle memory layout automatically**\n",
    "- You're prototyping and want to avoid memory errors\n",
    "\n",
    "**✅ Advantages:**\n",
    "- **Always succeeds** (if the shape math is valid)\n",
    "- **Automatically handles** contiguous vs non-contiguous memory\n",
    "- **Beginner-friendly** - less likely to cause frustrating errors\n",
    "- **Smart fallback** - returns view when possible, copy when necessary\n",
    "\n",
    "**⚠️ Trade-offs:**\n",
    "- **Less predictable performance** - you don't know if it creates a copy\n",
    "- **Potentially slower** than `view()` in some cases\n",
    "- **Less explicit** about memory sharing\n",
    "\n",
    "### 📊 reshape() vs view() - When to Use Which?\n",
    "\n",
    "| Scenario | Use `view()` | Use `reshape()` |\n",
    "|----------|-------------|-----------------|\n",
    "| **Performance critical** | ✅ Guaranteed no copying | ❌ Might copy data |\n",
    "| **Beginner-friendly** | ❌ Can throw errors | ✅ Always works |\n",
    "| **Prototyping** | ❌ Interrupts workflow | ✅ Smooth development |\n",
    "| **Production code** | ✅ Predictable behavior | ⚠️ Less predictable |\n",
    "| **Memory sharing required** | ✅ Guaranteed sharing | ⚠️ Depends on layout |\n",
    "\n",
    "Let's see how `reshape()` handles the scenarios where `view()` fails!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "95aa1ae4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🤝 TORCH.RESHAPE() - THE DIPLOMATIC SOLUTION\n",
      "====================================================\n",
      "Original data: torch.Size([24]) → [0, 1, 2, 3, 4, 5]... (24 elements)\n",
      "\n",
      "✅ SCENARIO 1: Contiguous tensor (reshape returns view)\n",
      "   Original data_ptr: 5037313032192\n",
      "   Reshaped data_ptr: 5037313032192\n",
      "   Same memory (view)? True ✓\n",
      "\n",
      "⚠️ SCENARIO 2: Non-contiguous tensor (reshape creates copy)\n",
      "   Transposed contiguous? False\n",
      "   Transposed data_ptr: 5037313032192\n",
      "   Reshaped data_ptr:   5037313032384\n",
      "   Same memory? False\n",
      "   Conclusion: reshape() created a COPY to make it work ✓\n",
      "\n",
      "🆚 DIRECT COMPARISON: view() vs reshape()\n",
      "   Testing on the same non-contiguous tensor...\n",
      "   view(): FAILED ❌ - view size is not compatible with input t...\n",
      "   reshape(): SUCCESS ✅ - Shape: torch.Size([24])\n",
      "\n",
      "🔍 INVESTIGATING: When does reshape() return view vs copy?\n",
      "   Contiguous reshape → View: True\n",
      "   Non-contiguous reshape → View: False (Creates copy)\n",
      "\n",
      "💡 RESHAPE() WISDOM:\n",
      "   1. Always succeeds (if math is valid)\n",
      "   2. Returns view when memory layout allows\n",
      "   3. Creates copy when necessary\n",
      "   4. Perfect for beginners and prototyping\n",
      "   5. Use view() only when you need guaranteed performance\n"
     ]
    }
   ],
   "source": [
    "print(\"🤝 TORCH.RESHAPE() - THE DIPLOMATIC SOLUTION\")\n",
    "print(\"=\" * 52)\n",
    "\n",
    "# Create test data\n",
    "data = torch.arange(24)\n",
    "print(f\"Original data: {data.shape} → {data[:6].tolist()}... (24 elements)\")\n",
    "\n",
    "print(f\"\\n✅ SCENARIO 1: Contiguous tensor (reshape returns view)\")\n",
    "matrix_4x6 = data.reshape(4, 6)\n",
    "print(f\"   Original data_ptr: {data.data_ptr()}\")\n",
    "print(f\"   Reshaped data_ptr: {matrix_4x6.data_ptr()}\")\n",
    "print(f\"   Same memory (view)? {data.data_ptr() == matrix_4x6.data_ptr()} ✓\")\n",
    "\n",
    "print(f\"\\n⚠️ SCENARIO 2: Non-contiguous tensor (reshape creates copy)\")\n",
    "# First transpose to make it non-contiguous\n",
    "transposed = matrix_4x6.t()  # Now 6x4, non-contiguous\n",
    "print(f\"   Transposed contiguous? {transposed.is_contiguous()}\")\n",
    "\n",
    "# Now reshape the non-contiguous tensor\n",
    "flattened = transposed.reshape(-1)  # This works! (unlike view)\n",
    "print(f\"   Transposed data_ptr: {transposed.data_ptr()}\")\n",
    "print(f\"   Reshaped data_ptr:   {flattened.data_ptr()}\")\n",
    "print(f\"   Same memory? {transposed.data_ptr() == flattened.data_ptr()}\")\n",
    "print(f\"   Conclusion: reshape() created a COPY to make it work ✓\")\n",
    "\n",
    "print(f\"\\n🆚 DIRECT COMPARISON: view() vs reshape()\")\n",
    "print(\"   Testing on the same non-contiguous tensor...\")\n",
    "\n",
    "# Test view() - should FAIL\n",
    "try:\n",
    "    view_result = transposed.view(-1)\n",
    "    print(\"   view(): SUCCESS (unexpected!)\")\n",
    "except RuntimeError as e:\n",
    "    print(f\"   view(): FAILED ❌ - {str(e)[:40]}...\")\n",
    "\n",
    "# Test reshape() - should SUCCEED  \n",
    "try:\n",
    "    reshape_result = transposed.reshape(-1)\n",
    "    print(f\"   reshape(): SUCCESS ✅ - Shape: {reshape_result.shape}\")\n",
    "except RuntimeError as e:\n",
    "    print(f\"   reshape(): FAILED - {e}\")\n",
    "\n",
    "print(f\"\\n🔍 INVESTIGATING: When does reshape() return view vs copy?\")\n",
    "\n",
    "# Case 1: Simple reshape of contiguous tensor\n",
    "simple_data = torch.arange(12)\n",
    "reshaped_simple = simple_data.reshape(3, 4)\n",
    "shares_memory_1 = simple_data.data_ptr() == reshaped_simple.data_ptr()\n",
    "print(f\"   Contiguous reshape → View: {shares_memory_1}\")\n",
    "\n",
    "# Case 2: Reshape after making non-contiguous\n",
    "non_contig = reshaped_simple.t()  # Non-contiguous\n",
    "reshaped_non_contig = non_contig.reshape(-1)\n",
    "shares_memory_2 = non_contig.data_ptr() == reshaped_non_contig.data_ptr()\n",
    "print(f\"   Non-contiguous reshape → View: {shares_memory_2} (Creates copy)\")\n",
    "\n",
    "print(f\"\\n💡 RESHAPE() WISDOM:\")\n",
    "print(f\"   1. Always succeeds (if math is valid)\")\n",
    "print(f\"   2. Returns view when memory layout allows\")\n",
    "print(f\"   3. Creates copy when necessary\")\n",
    "print(f\"   4. Perfect for beginners and prototyping\")\n",
    "print(f\"   5. Use view() only when you need guaranteed performance\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eadadce1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "##  The Contiguous Memory Problem - When Things Break 💥\n",
    "\n",
    "Let's master the final piece of the puzzle: understanding exactly **when** and **why** tensors become non-contiguous, and **how** to fix it!\n",
    "\n",
    "### 🧩 **What Creates Non-Contiguous Memory?**\n",
    "\n",
    "Certain PyTorch operations change how we **access** the data without **moving** the data in memory. This creates the \"scattered access\" pattern that breaks `view()`:\n",
    "\n",
    "**🔄 Operations that create non-contiguous tensors:**\n",
    "- `tensor.transpose()` / `tensor.t()` - Swaps dimensions\n",
    "- `tensor.permute()` - Reorders multiple dimensions  \n",
    "- Advanced slicing - `tensor[:, ::2]` (every 2nd element)\n",
    "- Some indexing operations - `tensor[mask]` with boolean masks\n",
    "\n",
    "**✅ Operations that keep tensors contiguous:**\n",
    "- `tensor.reshape()` - Creates new contiguous tensor if needed\n",
    "- Basic slicing - `tensor[start:end]` \n",
    "- Element-wise operations - `tensor + 1`, `tensor * 2`\n",
    "- Most mathematical operations - `torch.sin()`, `torch.exp()`\n",
    "\n",
    "### 🛠️ **The Solutions Toolkit**\n",
    "\n",
    "When you encounter the dreaded \"view size is not compatible with input tensor's size and stride\" error, here are your weapons:\n",
    "\n",
    "1. **`.contiguous()`** - Reorganize memory to be contiguous\n",
    "2. **`.reshape()`** - Let PyTorch handle it automatically  \n",
    "3. **Check first** - Use `.is_contiguous()` to debug\n",
    "\n",
    "Let's see the complete problem-solving workflow!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfadbdc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"💥 THE COMPLETE CONTIGUOUS MEMORY TROUBLESHOOTING GUIDE\")\n",
    "print(\"=\" * 65)\n",
    "\n",
    "# Start with a clean, contiguous tensor\n",
    "data = torch.arange(24).reshape(4, 6)\n",
    "print(f\"Original 4×6 matrix: contiguous = {data.is_contiguous()}\")\n",
    "\n",
    "print(f\"\\n🔧 TROUBLESHOOTING WORKFLOW:\")\n",
    "\n",
    "# Step 1: Create a problematic non-contiguous tensor\n",
    "print(f\"1️⃣ Create the problem...\")\n",
    "transposed = data.t()  # Transpose creates non-contiguous memory\n",
    "print(f\"   After transpose: contiguous = {transposed.is_contiguous()}\")\n",
    "print(f\"   Shape: {transposed.shape}\")\n",
    "\n",
    "# Step 2: Try view() and see it fail\n",
    "print(f\"\\n2️⃣ Attempt view() and witness the failure...\")\n",
    "try:\n",
    "    flattened = transposed.view(-1)\n",
    "    print(\"   view() succeeded! (unexpected)\")\n",
    "except RuntimeError as e:\n",
    "    print(f\"   ❌ view() failed: {str(e)[:50]}...\")\n",
    "\n",
    "# Step 3: Diagnose the problem\n",
    "print(f\"\\n3️⃣ Diagnose what went wrong...\")\n",
    "print(f\"   Tensor shape: {transposed.shape}\")\n",
    "print(f\"   Is contiguous: {transposed.is_contiguous()}\")\n",
    "print(f\"   Expected elements after flatten: {transposed.numel()}\")\n",
    "print(f\"   💡 Problem: Memory is scattered due to transpose\")\n",
    "\n",
    "print(f\"\\n🛠️ SOLUTION OPTIONS:\")\n",
    "\n",
    "# Solution 1: Use .contiguous() then .view()\n",
    "print(f\"✅ SOLUTION 1: .contiguous() + .view()\")\n",
    "contiguous_tensor = transposed.contiguous()\n",
    "flattened_v1 = contiguous_tensor.view(-1)\n",
    "print(f\"   Step 1: .contiguous() → contiguous = {contiguous_tensor.is_contiguous()}\")\n",
    "print(f\"   Step 2: .view(-1) → shape = {flattened_v1.shape}\")\n",
    "print(f\"   Memory cost: NEW allocation (copies data)\")\n",
    "\n",
    "# Solution 2: Use .reshape() directly  \n",
    "print(f\"\\n✅ SOLUTION 2: .reshape() (automatic)\")\n",
    "flattened_v2 = transposed.reshape(-1)\n",
    "print(f\"   Direct: .reshape(-1) → shape = {flattened_v2.shape}\")\n",
    "print(f\"   Memory cost: NEW allocation (reshapes automatically)\")\n",
    "\n",
    "# Solution 3: For advanced users - check first\n",
    "print(f\"\\n✅ SOLUTION 3: Check-first pattern\")\n",
    "def smart_flatten(tensor):\n",
    "    if tensor.is_contiguous():\n",
    "        print(\"   Tensor is contiguous → using view() (fast)\")\n",
    "        return tensor.view(-1)\n",
    "    else:\n",
    "        print(\"   Tensor is non-contiguous → using reshape() (safe)\")\n",
    "        return tensor.reshape(-1)\n",
    "\n",
    "print(\"   For contiguous tensor:\")\n",
    "contiguous_test = torch.arange(12).reshape(3, 4)\n",
    "result1 = smart_flatten(contiguous_test)\n",
    "\n",
    "print(\"   For non-contiguous tensor:\")\n",
    "non_contiguous_test = contiguous_test.t()\n",
    "result2 = smart_flatten(non_contiguous_test)\n",
    "\n",
    "print(f\"\\n🎯 PERFORMANCE COMPARISON:\")\n",
    "import time\n",
    "\n",
    "# Create larger tensors for timing\n",
    "large_tensor = torch.randn(1000, 1000)\n",
    "large_transposed = large_tensor.t()\n",
    "\n",
    "# Time .contiguous() + .view()\n",
    "start = time.time()\n",
    "for _ in range(100):\n",
    "    result = large_transposed.contiguous().view(-1)\n",
    "time1 = (time.time() - start) * 1000  # Convert to milliseconds\n",
    "\n",
    "# Time .reshape()  \n",
    "start = time.time()\n",
    "for _ in range(100):\n",
    "    result = large_transposed.reshape(-1)\n",
    "time2 = (time.time() - start) * 1000\n",
    "\n",
    "print(f\"   .contiguous().view(): {time1:.2f}ms\")\n",
    "print(f\"   .reshape():           {time2:.2f}ms\")\n",
    "print(f\"   Winner: {'reshape' if time2 < time1 else 'contiguous+view'}\")\n",
    "\n",
    "print(f\"\\n💡 FINAL WISDOM:\")\n",
    "print(f\"   - For beginners: Always use .reshape() (safe, automatic)\")\n",
    "print(f\"   - For experts: Use .view() with .contiguous() when needed\")\n",
    "print(f\"   - For debugging: Always check .is_contiguous() first\")\n",
    "print(f\"   - Remember: Non-contiguous ≠ broken, just different memory layout!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6106fff",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Why Do We Need Shape Transformation? 🤔\n",
    "\n",
    "Excellent question, my inquisitive apprentice! Before we learn the HOW, let's understand the WHY. Shape transformation is not academic wizardry—it's **essential for real neural networks**!\n",
    "\n",
    "### 🧠 **Real-World Neural Network Problems**\n",
    "\n",
    "**Problem 1: Data Format Mismatches**\n",
    "\n",
    "```python\n",
    "# You have image data as (height, width, channels) - but PyTorch wants (channels, height, width)\n",
    "image_data = torch.randn(224, 224, 3)      # ❌ Wrong format!\n",
    "# Need to rearrange to: torch.randn(3, 224, 224)  # ✅ PyTorch format\n",
    "```\n",
    "\n",
    "**Problem 2: Layer Input Requirements**\n",
    "\n",
    "```python\n",
    "# You have a batch of images: (batch, channels, height, width)\n",
    "batch_images = torch.randn(32, 3, 224, 224)   # 32 images, 3 channels, 224x224 pixels\n",
    "# But Linear layer expects: (batch, features) - need to flatten!\n",
    "# Target: torch.randn(32, 150528)  # 32 images, 3*224*224 = 150,528 features\n",
    "```\n",
    "\n",
    "**Problem 3: Broadcasting for Operations**  \n",
    "\n",
    "```python\n",
    "# You want to add bias to each channel separately\n",
    "images = torch.randn(32, 3, 224, 224)     # Batch of RGB images\n",
    "channel_bias = torch.randn(3)              # Bias for each channel: [R_bias, G_bias, B_bias]\n",
    "# But shapes don't match for broadcasting! Need: (1, 3, 1, 1)\n",
    "```\n",
    "\n",
    "**Problem 4: Attention Mechanism Reshaping**\n",
    "\n",
    "```python\n",
    "# Multi-head attention needs to split the embedding dimension\n",
    "embeddings = torch.randn(32, 128, 512)    # (batch, sequence, embedding)\n",
    "# Need to reshape to: (32, 128, 8, 64) for 8 attention heads of size 64\n",
    "```\n",
    "\n",
    "\n",
    "Let's see a concrete neural network example:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "968f0746",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"🧠 REAL-WORLD NEURAL NETWORK SCENARIO\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Scenario: Processing a batch of images through a CNN then a Linear layer\n",
    "print(\"📸 Problem: Batch of images → CNN → Linear layer\")\n",
    "\n",
    "# Step 1: Batch of RGB images (common format)\n",
    "batch_images = torch.randn(8, 3, 32, 32)  # 8 images, 3 channels, 32x32 pixels\n",
    "print(f\"   Input images shape: {batch_images.shape}\")\n",
    "print(f\"   Total elements: {batch_images.numel()}\")\n",
    "\n",
    "# Step 2: After CNN layers (let's say we get feature maps)\n",
    "# Imagine this came from conv layers\n",
    "feature_maps = torch.randn(8, 64, 8, 8)  # 8 images, 64 feature maps, 8x8 size\n",
    "print(f\"   After CNN shape: {feature_maps.shape}\")\n",
    "\n",
    "# Step 3: Problem! Linear layer expects (batch_size, features)\n",
    "# But we have (batch_size, channels, height, width)\n",
    "print(f\"   📋 Linear layer expects: (batch_size, num_features)\")\n",
    "print(f\"   ❌ But we have: {feature_maps.shape}\")\n",
    "\n",
    "# Step 4: Solution - Flatten the spatial dimensions!\n",
    "flattened = feature_maps.reshape(8, -1)  # Keep batch dim, flatten everything else\n",
    "print(f\"   ✅ After reshaping: {flattened.shape}\")\n",
    "print(f\"   Now compatible with Linear layer!\")\n",
    "\n",
    "# Verify the calculation\n",
    "expected_features = 64 * 8 * 8  # channels × height × width\n",
    "print(f\"\\n🧮 Verification:\")\n",
    "print(f\"   64 channels × 8 height × 8 width = {expected_features}\")\n",
    "print(f\"   Actual flattened features: {flattened.shape[1]}\")\n",
    "print(f\"   Correct? {expected_features == flattened.shape[1]}\")\n",
    "\n",
    "print(f\"\\n💡 The Magic: reshape() made incompatible tensors compatible!\")\n",
    "print(f\"   Without this, neural networks couldn't work!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d32577",
   "metadata": {},
   "source": [
    "\n",
    "## to remove\n",
    "\n",
    "### The Contiguous Memory Mystery Revealed! 🧩\n",
    "\n",
    "Now we shall witness the **exact moment** when our perfectly organized memory becomes scattered! This is where the theory meets cold, hard reality—and where `view()` throws its tantrum!\n",
    "\n",
    "**The Setup:** We start with contiguous memory, then **transpose()** creates the chaos that confounds apprentice programmers everywhere!\n",
    "\n",
    "**Visual Prediction - What Will Happen:**\n",
    "\n",
    "```\n",
    "Before Transpose (Contiguous):\n",
    "Memory: [1][2][3][4][5][6][7][8][9][10][11][12]\n",
    "Matrix interpretation: \n",
    "  [ 1  2  3  4]  ← Row 1: memory[0-3]\n",
    "  [ 5  6  7  8]  ← Row 2: memory[4-7]  \n",
    "  [ 9 10 11 12]  ← Row 3: memory[8-11]\n",
    "\n",
    "After Transpose (Non-Contiguous):\n",
    "Memory: [1][2][3][4][5][6][7][8][9][10][11][12] ← SAME!\n",
    "But NEW interpretation should be:\n",
    "  [ 1  5  9]     ← Row 1 needs: memory[0], memory[4], memory[8]\n",
    "  [ 2  6 10]     ← Row 2 needs: memory[1], memory[5], memory[9]  \n",
    "  [ 3  7 11]     ← Row 3 needs: memory[2], memory[6], memory[10]\n",
    "  [ 4  8 12]     ← Row 4 needs: memory[3], memory[7], memory[11]\n",
    "```\n",
    "\n",
    "**The Problem:** To read the new \"Row 1\" `[1, 5, 9]`, we must jump around in memory: positions 0 → 4 → 8. This jumping makes it **non-contiguous**!\n",
    "\n",
    "Let's witness this memory drama unfold!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bc39bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"🧩 THE MEMORY SCATTER CATASTROPHE EXPERIMENT\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "# Start with our nicely organized 3x4 matrix\n",
    "print(\"📊 BEFORE: Contiguous 3×4 tensor\")\n",
    "matrix = torch.arange(1, 13).reshape(3, 4)\n",
    "print(f\"   Memory layout: {matrix.flatten().tolist()}\")\n",
    "print(f\"   Contiguous? {matrix.is_contiguous()}\")\n",
    "print(f\"   Matrix view:\\n{matrix}\")\n",
    "print(f\"   💡 Row 1 [1,2,3,4] = memory positions [0,1,2,3] - SEQUENTIAL!\")\n",
    "\n",
    "# The moment of chaos - TRANSPOSE!\n",
    "print(f\"\\n🌀 THE TRANSPOSE CATASTROPHE:\")\n",
    "transposed = matrix.t()  # Same as matrix.transpose(0, 1)\n",
    "print(f\"   Memory STILL: {matrix.flatten().tolist()} ← Same memory!\")\n",
    "print(f\"   Contiguous? {transposed.is_contiguous()}\")\n",
    "print(f\"   New shape: {transposed.shape}\")\n",
    "print(f\"   Transposed view:\\n{transposed}\")\n",
    "print(f\"   💡 New Row 1 [1,5,9] needs memory positions [0,4,8] - SCATTERED!\")\n",
    "\n",
    "# Show the memory addresses are the same\n",
    "print(f\"\\n🧠 MEMORY VERIFICATION:\")\n",
    "print(f\"   Original memory pointer: {matrix.data_ptr()}\")\n",
    "print(f\"   Transposed memory pointer: {transposed.data_ptr()}\")\n",
    "print(f\"   Same memory location? {matrix.data_ptr() == transposed.data_ptr()}\")\n",
    "\n",
    "# The view() failure!\n",
    "print(f\"\\n❌ THE INEVITABLE FAILURE - view() throws tantrum:\")\n",
    "try:\n",
    "    failed_view = transposed.view(-1)  # Try to flatten\n",
    "    print(\"   Unexpectedly succeeded!\")\n",
    "except RuntimeError as e:\n",
    "    print(f\"   ERROR: {str(e)}\")\n",
    "    print(f\"   💭 Translation: 'The memory is scattered, I can't create a simple view!'\")\n",
    "\n",
    "# The solutions\n",
    "print(f\"\\n🛠️ SOLUTION 1: .contiguous() - Reorganize memory\")\n",
    "contiguous_version = transposed.contiguous()\n",
    "print(f\"   Original memory: {transposed.data_ptr()}\")\n",
    "print(f\"   New memory:      {contiguous_version.data_ptr()}\")  \n",
    "print(f\"   Different memory? {transposed.data_ptr() != contiguous_version.data_ptr()}\")\n",
    "print(f\"   New memory layout: {contiguous_version.flatten().tolist()}\")\n",
    "successful_view = contiguous_version.view(-1)\n",
    "print(f\"   view() now works! Shape: {successful_view.shape}\")\n",
    "\n",
    "print(f\"\\n🛠️ SOLUTION 2: reshape() - The diplomatic wizard\")\n",
    "auto_flattened = transposed.reshape(-1)\n",
    "print(f\"   reshape() handles everything automatically!\")\n",
    "print(f\"   Result: {auto_flattened.tolist()}\")\n",
    "\n",
    "print(f\"\\n🎯 PROFESSOR'S WISDOM:\")\n",
    "print(f\"   - view(): Fast but picky (needs contiguous memory)\")\n",
    "print(f\"   - reshape(): Diplomatic (handles any memory layout)\")\n",
    "print(f\"   - When in doubt, use reshape()!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-course-YUNTNAZF-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
