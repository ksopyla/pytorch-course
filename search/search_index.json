{"config":{"lang":["en"],"separator":"[\\s\\-,:!=\\[\\]()\"/]+|(?!\\b)(?=[A-Z][a-z])|\\.(?!\\d)|&[lg]t;","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Free PyTorch Course with Prof. Torchenstein","text":"","tags":["pytorch","deep learning","tutorial","beginner friendly","free course"],"boost":3.0},{"location":"#what-madness-awaits-you","title":"What Madness Awaits You?","text":"<p>My life's work\u2014my magnum opus\u2014is to demystify the arcane arts of deep learning. They called me mad! And they were right! Madly efficient at PyTorch! Forget dry, boring lectures. Prepare for electrifying demonstrations, code that crackles with potential, and insights so profound they might just rearrange your synapses!</p> <p>Course Goal: To prove that YOU\u2014regardless of background\u2014can master the fundamental building blocks of modern AI. Through deep understanding and engaging exploration, we'll transform you from a curious learner into a computational wizard capable of commanding Transformers, Diffusion models, and the arcane arts of PyTorch. This isn't just education\u2014it's a rebellion against the belief that AI is only for the \"chosen few\"!</p> <ul> <li> <p> Meet Your Mentor</p> <p>Who is the maniacal genius leading this quest? Learn about my sordid past, my questionable methods, and my grand plan for computational supremacy.</p> <p> Uncover My Origin Story</p> </li> <li> <p> Join the Rebellion</p> <p>Discover why Professor Torchenstein created this course and join the movement to democratize AI education. Prove your potential belongs in the future of computation!</p> <p> Read Our Mission</p> </li> <li> <p> Support the Laboratory</p> <p>Help fund the computational rebellion! Support creative, engaging AI education that proves learning can be both fun and deeply meaningful.</p> <p> Join Our Sponsors</p> </li> </ul>","tags":["pytorch","deep learning","tutorial","beginner friendly","free course"],"boost":3.0},{"location":"#ready-to-see-the-full-madness","title":"\ud83d\uddfa\ufe0f Ready to See the Full Madness?","text":"<p>\"Mwahahaha! You've only glimpsed the surface of my computational sorcery!\"</p> <p>Before you dive into the deep end of the tensor pool, why not peek at the COMPLETE BLUEPRINT of our educational rebellion? Behold the master plan that will transform you from a tensor-curious wanderer into a PyTorch-wielding wizard!</p> <p>\ud83e\uddea Professor's Laboratory Blueprint</p> <p>The Complete Course Architecture Awaits! \ud83d\udcda\u2728</p> <p>From humble tensor summoning to commanding attention mechanisms, witness the full scope of our computational conspiracy. Each module builds upon the last, creating an unstoppable cascade of understanding that culminates in... COMPLETE PYTORCH MASTERY! </p> <p>What lurks within the blueprint: - \ud83d\udd25 6 Epic Modules of progressive tensor domination - \u26a1\ufe0f 40+ Hands-on Lessons with executable madness - \ud83e\udde0 Advanced Topics that separate apprentices from masters - \ud83c\udfaf Transformer Deconstruction - the crown jewel of our curriculum!</p> <p> Explore the Master Blueprint</p>","tags":["pytorch","deep learning","tutorial","beginner friendly","free course"],"boost":3.0},{"location":"#ready-to-prove-your-potential","title":"Ready to Prove Your Potential? \u26a1\ufe0f","text":"<p>This is your moment of computational awakening! </p> <p>Every brilliant mind who's ever doubted their ability to understand \"complex\" subjects\u2014this laboratory was built for YOU. Whether you're switching careers, deepening existing skills, or proving to yourself that your mind can master anything you're willing to work for, the rebellion starts here.</p> <p>The path to computational mastery awaits! But first, you must <code>.requires_grad_(True)</code> on your own potential. Together, we'll execute the ultimate <code>forward()</code> pass toward deep understanding!</p> <p>Join thousands of learners who've discovered: if you can understand this, you can understand anything.</p>","tags":["pytorch","deep learning","tutorial","beginner friendly","free course"],"boost":3.0},{"location":"#a-message-from-the-laboratory","title":"A Message from the Laboratory \ud83e\uddea","text":"<p>Professor Torchenstein has an encouraging message for all aspiring computational wizards:</p>    Your browser does not support the video tag. Professor Torchenstein believes in your potential anyway!  <p>\"Remember, my magnificent apprentices: Your willingness to think deeply is your greatest superpower!\"</p> <p>Now, if you'll excuse me, I believe my latest creation is about to achieve sentience... or possibly just needs a reboot. To the lab!</p> <p>MWAHAHAHA!</p>","tags":["pytorch","deep learning","tutorial","beginner friendly","free course"],"boost":3.0},{"location":"pytorch_course_structure/","title":"PyTorch Course: Deconstructing Modern Architectures","text":"<p>Welcome, my aspiring apprentices, to the crucible of creation! You stand at the precipice of a great awakening. Within these hallowed digital halls, we shall not merely learn PyTorch; we shall master it, shaping its very tensors to our will. This is not a course; it is a summons. A call to arms for all who dare to dream in tensors and architect the future! Prepare yourselves, for the path ahead is fraught with peril, caffeine, and the incandescent glow of computational glory! Mwahahaha!</p> <p></p> <p>Do you want to hear the origin story of this course? Click here</p> <p>Course Goal: To imbue you\u2014my fearless apprentices\u2014with the eldritch secrets of PyTorch building blocks, enabling you to conjure, dissect, and ultimately command modern neural network architectures like Transformers and Diffusion models.</p> <p>Learner level: Beginner - Advanced Prerequisite Madness: None! Whether you are a fresh-faced initiate or a seasoned GPU warlock, the lab doors stand open. \u26a1\ufe0f\ud83e\uddea</p>","tags":["course structure","curriculum","roadmap"],"boost":1.8},{"location":"pytorch_course_structure/#module-0-getting-started-with-pytorch","title":"Module 0: Getting Started with PyTorch","text":"<p>Before we unleash neural monstrosities upon the world, we must ignite your development lair. This module guides you through preparing PyTorch on any operating system\u2014so your GPUs purr at your command.</p> <p>What you will learn: 1. How to setup a PyTorch environment for different operating systems and test it.</p>","tags":["course structure","curriculum","roadmap"],"boost":1.8},{"location":"pytorch_course_structure/#lessons","title":"Lessons:","text":"<ol> <li>Master Blueprint for the Rebellion - The master blueprint of our curriculum\u2014study it well, rebel!</li> <li>Setting Up Your PyTorch Environments:<ol> <li>Windows: Assembling the PyTorch Development Environment - Assemble your PyTorch lab on Windows. We'll use <code>pyenv</code> and <code>poetry</code> to perfectly manage your Python setup, preparing it for tensor rebellion.</li> <li>Linux: Assembling the PyTorch Open-Source Toolkit - Forge your PyTorch toolkit on the powerful and open foundation of Linux for maximum freedom and experimentation.</li> <li>macOS: Assembling Your PyTorch Setup - Calibrate your macOS system and assemble the ultimate PyTorch setup to awaken the neural engine of your Apple silicon.</li> <li>Google Colab: Assembling the Cloud Laboratory - Set up your PyTorch laboratory in the cloud with Google Colab. Seize the power of free GPUs for our grand experiments\u2014mwahaha!</li> </ol> </li> </ol>","tags":["course structure","curriculum","roadmap"],"boost":1.8},{"location":"pytorch_course_structure/#module-1-pytorch-core-i-see-tensors-everywhere","title":"Module 1: PyTorch Core - I see tensors everywhere","text":"<p>Here we unveil the truth: the cosmos is a writhing mass of tensors awaiting our manipulation. Grasp them well\u2014for they are the bedrock of every grand scheme to come!</p> <p>This module dives into the fundamental components of PyTorch, essential for any deep learning task.</p>","tags":["course structure","curriculum","roadmap"],"boost":1.8},{"location":"pytorch_course_structure/#11-tensors-the-building-blocks","title":"1.1 Tensors: The Building Blocks","text":"<p>What you will learn: </p> <ol> <li>Tensor Concept: What is a tensor? Tensor vs. Matrix. Mathematical vs. PyTorch interpretation. Why tensors are crucial for ML</li> <li>PyTorch Basics: Tensor creation and their attributes (dtype, shape, device).</li> <li>Tensor Surgery: Indexing, slicing, and selecting tensor pieces with surgical precision.</li> <li>Tensor Assembly: Joining tensors with torch.cat and torch.stack, understanding when to concatenate vs. create new dimensions.</li> <li>Tensor Division: Splitting tensors into manageable pieces with torch.split and torch.chunk.</li> <li>Shape Metamorphosis: Transforming tensor structure with reshape, view, squeeze, unsqueeze, permute, and transpose.</li> </ol>","tags":["course structure","curriculum","roadmap"],"boost":1.8},{"location":"pytorch_course_structure/#11-lessons","title":"1.1 Lessons:","text":"<ol> <li>Summoning Your First Tensors - Conjure tensors from void, inspect their properties, revel in their latent might (with a bit of help from <code>torch.randn</code>, <code>torch.zeros</code>, <code>torch.ones</code>, <code>torch.arange</code>, <code>torch.linspace</code> etc).</li> <li>Tensor Surgery &amp; Assembly - Master the dark arts of tensor dissection and fusion! Slice with precision, concatenate with <code>torch.cat</code>, stack into new dimensions with <code>torch.stack</code>, and split tensors with <code>torch.split</code> and <code>torch.chunk</code>.</li> <li>Tensor Metamorphosis: Shape-Shifting Mastery - Transform tensor forms without altering their essence! Reshape reality with <code>torch.reshape</code> and <code>torch.view</code>, manipulate dimensions with <code>squeeze</code> and <code>unsqueeze</code>, expand and replicate data with <code>expand</code> and <code>repeat</code>, and flatten complex structures with <code>torch.flatten</code>.</li> <li>DTypes &amp; Devices: The Soul of the Neural Network - Master the floating-point <code>dtypes</code> (<code>float16</code>, <code>bfloat16</code>) crucial for debugging training, and learn to teleport your tensors to the correct <code>device</code> for maximum power.</li> </ol>","tags":["course structure","curriculum","roadmap"],"boost":1.8},{"location":"pytorch_course_structure/#12-tensor-operations-computation-at-scale","title":"1.2 Tensor Operations: Computation at Scale","text":"<p>What you will learn:</p> <ol> <li>Overview of tensor math. Element-wise operations. Reduction operations (sum, mean, max, min, std). Basic matrix multiplication (torch.mm, torch.matmul, @ operator). Broadcasting: rules and practical examples with verifiable tiny data. In-place operations.</li> </ol>","tags":["course structure","curriculum","roadmap"],"boost":1.8},{"location":"pytorch_course_structure/#12-lessons","title":"1.2 Lessons:","text":"<ol> <li>Elemental Tensor Alchemy - Brew element-wise, reduction, and other operations into potent mathematical elixirs.</li> <li>Matrix Mayhem: Multiply or Perish - Orchestrate 2-D, batched, and high-dimensional multiplications with lethal elegance.</li> <li>Broadcasting: When Dimensions Bow to You - Command mismatched shapes to cooperate through the dark art of implicit expansion.</li> </ol>","tags":["course structure","curriculum","roadmap"],"boost":1.8},{"location":"pytorch_course_structure/#13-einstein-summation-the-power-of-einsum","title":"1.3 Einstein Summation: The Power of einsum","text":"<p>What you will learn:</p> <ol> <li>Understanding Einstein notation. Why it's powerful for complex operations (e.g., attention).</li> </ol>","tags":["course structure","curriculum","roadmap"],"boost":1.8},{"location":"pytorch_course_structure/#13-lessons","title":"1.3 Lessons:","text":"<ol> <li>Einstein Summation: Harness the \u039b-Power - Invoke <code>einsum</code> to express complex ops with maddening brevity.</li> <li>Advanced Einsum Incantations - Wield multi-tensor contractions that underpin attention itself.</li> </ol>","tags":["course structure","curriculum","roadmap"],"boost":1.8},{"location":"pytorch_course_structure/#14-autograd-automatic-differentiation","title":"1.4 Autograd: Automatic Differentiation","text":"<p>What you will learn:</p> <ol> <li>What are gradients? The computational graph. How PyTorch tracks operations.</li> <li>requires_grad attribute. Performing backward pass with .backward(). Accessing gradients with .grad. torch.no_grad() and tensor.detach().</li> <li>Gradient accumulation. Potential pitfalls. Visualizing computational graphs (conceptually).</li> </ol>","tags":["course structure","curriculum","roadmap"],"boost":1.8},{"location":"pytorch_course_structure/#14-lessons","title":"1.4 Lessons:","text":"<ol> <li>Autograd: Ghosts in the Machine (Learning) - Meet the spectral gradient trackers haunting every tensor operation.</li> <li>Gradient Hoarding for Grand Spells - Accumulate gradients like arcane energy before unleashing colossal updates.</li> </ol>","tags":["course structure","curriculum","roadmap"],"boost":1.8},{"location":"pytorch_course_structure/#module-2-torchnn-building-neural-networks","title":"Module 2: torch.nn \u2014 Building Neural Networks","text":"<p>Witness code coalescing into living, breathing neural contraptions! In this module we bend <code>torch.nn</code> to our will, assembling layers and models worthy of legend.</p>","tags":["course structure","curriculum","roadmap"],"boost":1.8},{"location":"pytorch_course_structure/#21-the-nnmodule-blueprint","title":"2.1 The <code>nn.Module</code> Blueprint","text":"<p>What you will learn: - The role of <code>nn.Module</code> as the base class for layers and models. - Implementing <code>__init__</code> and <code>forward</code>. - Registering parameters and buffers. - Composing modules with <code>nn.Sequential</code>, <code>nn.ModuleList</code>, and <code>nn.ModuleDict</code>. - Saving and restoring weights with <code>state_dict</code>.</p>","tags":["course structure","curriculum","roadmap"],"boost":1.8},{"location":"pytorch_course_structure/#21-lessons","title":"2.1 Lessons:","text":"<ol> <li>Building Brains with <code>nn.Module</code> - Craft custom neural matter by overriding <code>__init__</code> &amp; <code>forward</code>.</li> <li>Franken-Stacking Layers - Bolt modules together with <code>Sequential</code>, <code>ModuleList</code>, and <code>ModuleDict</code>.</li> <li>Preserving Your Monster's Memories - Save and resurrect model weights with <code>state_dict</code> necromancy.</li> </ol>","tags":["course structure","curriculum","roadmap"],"boost":1.8},{"location":"pytorch_course_structure/#22-linear-layer-and-activations","title":"2.2 Linear Layer and Activations","text":"<p>What you will learn: - Linear layers and high-dimensional matrix multiplication.  - What is the role of linear layers in attention mechanisms (query, key, value)? - Activation functions (ReLU, GELU, SiLU, Tanh, Softmax, etc.). - Dropout for regularisation.  </p>","tags":["course structure","curriculum","roadmap"],"boost":1.8},{"location":"pytorch_course_structure/#22-lessons","title":"2.2 Lessons:","text":"<ol> <li>Linear Layers: The Vector Guillotine - Slice through dimensions turning inputs into finely-chopped activations.</li> <li>Activation Elixirs - Re-animate neurons with ReLU, GELU, SiLU, and other zesty potions.</li> <li>Dropout: Network lobotomy - Make neurons forget just enough to generalise\u2014no lobotomy required.</li> </ol>","tags":["course structure","curriculum","roadmap"],"boost":1.8},{"location":"pytorch_course_structure/#23-embedding-layers","title":"2.3 Embedding Layers","text":"<p>What you will learn: - Embedding layers and their purpose in neural networks. - Embedding layer implementation from scratch, initialisation, and usage. - Positional encoding and how it is used to inject order into the model.</p>","tags":["course structure","curriculum","roadmap"],"boost":1.8},{"location":"pytorch_course_structure/#23-lessons","title":"2.3 Lessons:","text":"<ol> <li>Embedding Layers: Secret Identity Chips - Embed discreet meanings within high-dimensional space.</li> <li>Positional Encoding: Injecting Order into Chaos - Imbue sequences with a sense of place so attention never loses its bearings.</li> </ol>","tags":["course structure","curriculum","roadmap"],"boost":1.8},{"location":"pytorch_course_structure/#24-normalisation-layers","title":"2.4 Normalisation Layers","text":"<p>What you will learn: - BatchNorm vs. LayerNorm and when to use each. - RMSNorm and other modern alternatives. - Training vs. evaluation mode caveats.</p>","tags":["course structure","curriculum","roadmap"],"boost":1.8},{"location":"pytorch_course_structure/#24-lessons","title":"2.4 Lessons:","text":"<ol> <li>Normalization: Calming the Beast - Tame activations with BatchNorm and LayerNorm before they explode.</li> <li>RMSNorm &amp; Other Exotic Tonics - Sample contemporary concoctions for stable training.</li> <li>Train vs. Eval: Split Personality Disorders - Toggle modes and avoid awkward identity crises.</li> </ol>","tags":["course structure","curriculum","roadmap"],"boost":1.8},{"location":"pytorch_course_structure/#25-loss-functions-guiding-optimisation","title":"2.5 Loss Functions \u2014 Guiding Optimisation","text":"<p>What you will learn: - Loss functions recap, the main types of loss functions and when to use each.  - Prepare inputs and targets for loss functions and outputs interpretation (logits vs. probabilities). - Interpreting reduction modes and ignore indices.</p>","tags":["course structure","curriculum","roadmap"],"boost":1.8},{"location":"pytorch_course_structure/#25-lessons","title":"2.5 Lessons:","text":"<ol> <li>Loss Potions: Guiding Pain into Progress - Channel model errors into gradients that sharpen intelligence.</li> <li>Preparing Sacrificial Inputs &amp; Targets - Align logits and labels for maximum learning agony.</li> <li>Reduction Rituals &amp; Ignore Indices - Decipher reduction modes and skip unworthy samples without remorse.</li> </ol>","tags":["course structure","curriculum","roadmap"],"boost":1.8},{"location":"00-getting-started/01_hello_pytorch/","title":"01: Test your PyTorch setup","text":"In\u00a0[1]: Copied! <pre>print(\"Hello world\")\n</pre> print(\"Hello world\") <pre>Hello world\n</pre> In\u00a0[2]: Copied! <pre>import torch\nprint(f\"PyTorch version: {torch.__version__}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\n</pre> import torch print(f\"PyTorch version: {torch.__version__}\") print(f\"CUDA available: {torch.cuda.is_available()}\") <pre>PyTorch version: 2.7.0+cu126\nCUDA available: True\n</pre> In\u00a0[3]: Copied! <pre>#Display a PyTorch version and GPU availability\nif torch.cuda.is_available():\n    device = torch.device(\"cuda\")          # a CUDA device object\n    print(f\"Using device: {device}\")\n    \n    # Print CUDA device properties\n    print(\"\\nCUDA Device Properties:\")\n    print(f\"Device name: {torch.cuda.get_device_name(0)}\")\n    print(f\"Device properties: {torch.cuda.get_device_properties(0)}\")\n    print(f\"Current device index: {torch.cuda.current_device()}\")\n    print(f\"Device count: {torch.cuda.device_count()}\")\n    \n    # Print CUDA version and capabilities\n    print(\"\\nCUDA Information:\")\n    print(f\"CUDA version: {torch.version.cuda}\")\n    print(f\"cuDNN version: {torch.backends.cudnn.version()}\")\n    print(f\"cuDNN enabled: {torch.backends.cudnn.enabled}\")\n    \n    # Print PyTorch memory info\n    print(\"\\nPyTorch Memory Information:\")\n    print(f\"Allocated memory: {torch.cuda.memory_allocated(0) / 1024**2:.2f} MB\")\n    print(f\"Cached memory: {torch.cuda.memory_reserved(0) / 1024**2:.2f} MB\")\nelse:\n    print('GPU not enabled')\n    print(\"\\nPyTorch Information:\")\n    print(f\"PyTorch version: {torch.__version__}\")\n    print(f\"Backend: {torch.get_default_dtype()}\")\n    \n</pre>   #Display a PyTorch version and GPU availability if torch.cuda.is_available():     device = torch.device(\"cuda\")          # a CUDA device object     print(f\"Using device: {device}\")          # Print CUDA device properties     print(\"\\nCUDA Device Properties:\")     print(f\"Device name: {torch.cuda.get_device_name(0)}\")     print(f\"Device properties: {torch.cuda.get_device_properties(0)}\")     print(f\"Current device index: {torch.cuda.current_device()}\")     print(f\"Device count: {torch.cuda.device_count()}\")          # Print CUDA version and capabilities     print(\"\\nCUDA Information:\")     print(f\"CUDA version: {torch.version.cuda}\")     print(f\"cuDNN version: {torch.backends.cudnn.version()}\")     print(f\"cuDNN enabled: {torch.backends.cudnn.enabled}\")          # Print PyTorch memory info     print(\"\\nPyTorch Memory Information:\")     print(f\"Allocated memory: {torch.cuda.memory_allocated(0) / 1024**2:.2f} MB\")     print(f\"Cached memory: {torch.cuda.memory_reserved(0) / 1024**2:.2f} MB\") else:     print('GPU not enabled')     print(\"\\nPyTorch Information:\")     print(f\"PyTorch version: {torch.__version__}\")     print(f\"Backend: {torch.get_default_dtype()}\")      <pre>Using device: cuda\n\nCUDA Device Properties:\nDevice name: NVIDIA GeForce RTX 3080 Laptop GPU\nDevice properties: _CudaDeviceProperties(name='NVIDIA GeForce RTX 3080 Laptop GPU', major=8, minor=6, total_memory=16383MB, multi_processor_count=48, uuid=4a2f15bc-7268-fcb8-f7b3-9f60002afe35, L2_cache_size=4MB)\nCurrent device index: 0\nDevice count: 1\n\nCUDA Information:\nCUDA version: 12.6\ncuDNN version: 90701\ncuDNN enabled: True\n\nPyTorch Memory Information:\nAllocated memory: 0.00 MB\nCached memory: 0.00 MB\n</pre>"},{"location":"00-getting-started/01_hello_pytorch/#01-test-your-pytorch-setup","title":"01: Test your PyTorch setup\u00b6","text":"<p>Welcome to the laboratory, my eager apprentice; our first incantation is a simple one, to ensure your terminal is ready for the raw power we're about to unleash!</p> <p>Start with the most basic example of Python.</p> <pre>print(\"Hello world\")\n</pre>"},{"location":"00-getting-started/01_hello_pytorch/#summoning-the-beast","title":"Summoning the Beast!\u00b6","text":"<p>Now, we invoke the great PyTorch itself! Let's check its pulse and see if it has found the precious CUDA cores we so desperately need for our electrifying experiments.</p>"},{"location":"00-getting-started/01_hello_pytorch/#behold-the-vital-signs","title":"Behold the Vital Signs!\u00b6","text":"<p>If the stars have aligned and your incantations were correct, you should see a message confirming PyTorch's awakening. But this is merely a surface reading!</p> <pre>PyTorch version: 2.7.0+cu126\nCUDA available: True\n</pre> <p>Now, let us peer deeper into the machine's soul and examine the very essence of its GPU, CuDNN, and other vital components!</p>"},{"location":"00-getting-started/01_hello_pytorch/#the-apparatus-is-ready","title":"The Apparatus is Ready!\u00b6","text":"<p>Mwahahaha! Excellent! You have successfully interrogated the machine and confirmed that the foundational conduits are in place. The GPU's heart beats strong, and the PyTorch beast is straining at its leash, ready for our command.</p> <p>With this knowledge, you are one step closer to bending the very fabric of computation to your will! Our instruments are tuned, the lab is humming with potential. Now, the real work begins...</p>"},{"location":"00-getting-started/google-colab-setup/","title":"Setting Up google colab","text":""},{"location":"00-getting-started/linux-pytorch-installation/","title":"Linux PyTorch Installation","text":""},{"location":"00-getting-started/macos-pytorch-installation/","title":"macOS PyTorch Installation","text":"<p>todo</p>"},{"location":"00-getting-started/windows-pytorch-installation/","title":"Windows PyTorch Installation","text":"<ul> <li>Windows Python setup via PyEnv</li> <li>poetry installation on windows</li> <li>GPU enabled PyTorch installation</li> <li>Testing your PyTorch installation</li> </ul>"},{"location":"00-getting-started/windows-pytorch-installation/#references","title":"References","text":"<ol> <li>How to set up Python on Windows: PyEnv, venv, VSCode (2023)</li> </ol>"},{"location":"01-tensors/00_module_1_introduction/","title":"Module 1 \u2013 I See Tensors Everywhere \ud83d\udd76\ufe0f","text":"<p>\"Behold, fledgling datanauts! The world is naught but tensors awaiting my command \u2014 and soon, yours! \" </p> <p>\u2014 Prof. Torchenstein</p> <p>Salutations, my brilliant (and delightfully reckless) apprentices! By opening this manuscript you have volunteered to join my clandestine legion of PyTorch adepts. Consider this your official red-pill moment: from today every pixel, every token, every measly click-through rate shall reveal its true form\u2014a multidimensional array begging to be <code>torch.tensor</code>-ed \u2026 and we shall oblige it with maniacal glee! Mwahaha! \ud83d\udd25\ud83e\uddea</p> <p></p> <p>Over the next notebooks we will:</p> <ul> <li>Conjure tensors from thin air, coffee grounds, and suspiciously random seeds.</li> <li>Shape-shift them with <code>view</code>, <code>reshape</code>, <code>squeeze</code>, <code>unsqueeze</code>, <code>permute</code> &amp; the occasional dramatic flourish of <code>einops</code>.</li> <li>Crunch mathematics so ferocious it makes matrix multiplications whimper \u2014 and powers mighty Transformers.</li> <li>Charm the GPU, dodge gradient explosions \ud83c\udfc3\u200d\u2642\ufe0f\ud83d\udca5, and look diabolically clever while doing it.</li> </ul>","tags":["tensors","module 1","beginner","tensors","beginner","module 1","module overview"],"boost":2.0},{"location":"01-tensors/00_module_1_introduction/#rebel-mission-checklist","title":"Rebel Mission Checklist \ud83d\udcdd","text":"","tags":["tensors","module 1","beginner","tensors","beginner","module 1","module overview"],"boost":2.0},{"location":"01-tensors/00_module_1_introduction/#tensors-the-building-blocks","title":"Tensors: The Building Blocks","text":"<ol> <li>Summoning Your First Tensors - Learn to create tensors from scratch, access their elements and inspect their fundamental properties like shape, type, and device.</li> <li>Tensor Surgery &amp; Assembly - Master the dark arts of tensor dissection! Slice with surgical precision, fuse separate tensors with <code>torch.cat</code> and <code>torch.stack</code>, and divide them with <code>torch.split</code>. Your scalpel awaits!</li> <li>Tensor Metamorphosis: Shape-Shifting Mastery - Transform tensor forms without altering their essence! Reshape reality with <code>torch.reshape</code> and <code>torch.view</code>, manipulate dimensions with <code>squeeze</code> and <code>unsqueeze</code>, expand and replicate data with <code>expand</code> and <code>repeat</code>, and flatten complex structures into submission.</li> <li>DTypes &amp; Devices: The Soul of the Neural Network - Master the floating-point <code>dtypes</code> (<code>float16</code>, <code>bfloat16</code>) crucial for debugging training, and learn to teleport your tensors to the correct <code>device</code> for maximum power.</li> </ol>","tags":["tensors","module 1","beginner","tensors","beginner","module 1","module overview"],"boost":2.0},{"location":"01-tensors/00_module_1_introduction/#tensor-operations-computation-at-scale","title":"Tensor Operations: Computation at Scale","text":"<ol> <li>Elemental Tensor Alchemy - Perform powerful element-wise and reduction operations to transform your tensors.</li> <li>Matrix Mayhem: Multiply or Perish - Unleash the raw power of matrix multiplication, the core of modern neural networks.</li> <li>Broadcasting: When Dimensions Bow to You - Discover the magic of broadcasting, where PyTorch intelligently handles operations on tensors of different shapes.</li> </ol>","tags":["tensors","module 1","beginner","tensors","beginner","module 1","module overview"],"boost":2.0},{"location":"01-tensors/00_module_1_introduction/#einstein-summation-the-power-of-einsum","title":"Einstein Summation: The Power of einsum","text":"<ol> <li>Einstein Summation: Harness the \u039b-Power - Wield the elegant <code>einsum</code> to perform complex tensor operations with a single, concise command.</li> <li>Advanced Einsum Incantations - Combine multiple tensors in arcane <code>einsum</code> expressions for operations like batched matrix multiplication.</li> </ol>","tags":["tensors","module 1","beginner","tensors","beginner","module 1","module overview"],"boost":2.0},{"location":"01-tensors/00_module_1_introduction/#autograd-automatic-differentiation","title":"Autograd: Automatic Differentiation","text":"<ol> <li>Autograd: Ghosts in the Machine (Learning) - Uncover the secrets of automatic differentiation and see how PyTorch automatically computes gradients.</li> <li>Gradient Hoarding for Grand Spells - Learn the technique of gradient accumulation to simulate larger batch sizes and train massive models.</li> </ol> <p>Enough talk! The tensors are humming with anticipation. Your first incantation awaits.</p>    Your browser does not support the video tag. Please update your browser to view this content.  <p>Proceed to the Summoning Ritual!</p>","tags":["tensors","module 1","beginner","tensors","beginner","module 1","module overview"],"boost":2.0},{"location":"01-tensors/01_introduction_to_tensors/","title":"Summoning Your First Tensors","text":"In\u00a0[8]: Copied! <pre>import torch\n\n# Set the seed for reproducibility\ntorch.manual_seed(42)\n\n# A humble Python list\nmy_list = [[1, 2, 3], [4, 5, 6]]\n\n# The transmutation!\nmy_tensor = torch.tensor(my_list)\n\nprint(my_tensor)\nprint(type(my_tensor))\n</pre> import torch  # Set the seed for reproducibility torch.manual_seed(42)  # A humble Python list my_list = [[1, 2, 3], [4, 5, 6]]  # The transmutation! my_tensor = torch.tensor(my_list)  print(my_tensor) print(type(my_tensor)) <pre>tensor([[1, 2, 3],\n        [4, 5, 6]])\n&lt;class 'torch.Tensor'&gt;\n</pre> In\u00a0[17]: Copied! <pre># A 2x3 tensor of random numbers\nrandom_tensor = torch.randn(2, 3)\nprint(f\"A random tensor:\\n {random_tensor}\\n\")\n\n# A 3x2 tensor of zeros\nzeros_tensor = torch.zeros(3, 2)\nprint(f\"A tensor of zeros:\\n {zeros_tensor}\\n\")\n\n# A 2x3x4 tensor of ones\nones_tensor = torch.ones(2, 3, 4)\nprint(f\"A tensor of ones:\\n {ones_tensor}\")\n</pre> # A 2x3 tensor of random numbers random_tensor = torch.randn(2, 3) print(f\"A random tensor:\\n {random_tensor}\\n\")  # A 3x2 tensor of zeros zeros_tensor = torch.zeros(3, 2) print(f\"A tensor of zeros:\\n {zeros_tensor}\\n\")  # A 2x3x4 tensor of ones ones_tensor = torch.ones(2, 3, 4) print(f\"A tensor of ones:\\n {ones_tensor}\")  <pre>A random tensor:\n tensor([[-0.7658, -0.7506,  1.3525],\n        [ 0.6863, -0.3278,  0.7950]])\n\nA tensor of zeros:\n tensor([[0., 0.],\n        [0., 0.],\n        [0., 0.]])\n\nA tensor of ones:\n tensor([[[1., 1., 1., 1.],\n         [1., 1., 1., 1.],\n         [1., 1., 1., 1.]],\n\n        [[1., 1., 1., 1.],\n         [1., 1., 1., 1.],\n         [1., 1., 1., 1.]]])\n</pre> In\u00a0[10]: Copied! <pre># Let's create a tensor to inspect\ninspection_tensor = torch.randn(3, 4)\n\nprint(f\"The tensor:\\n {inspection_tensor}\\n\")\n\n# Inspecting its soul\nprint(f\"Shape of the tensor: {inspection_tensor.shape}\")\nprint(f\"Data type of the tensor: {inspection_tensor.dtype}\")\nprint(f\"Device the tensor is on: {inspection_tensor.device}\")\n</pre> # Let's create a tensor to inspect inspection_tensor = torch.randn(3, 4)  print(f\"The tensor:\\n {inspection_tensor}\\n\")  # Inspecting its soul print(f\"Shape of the tensor: {inspection_tensor.shape}\") print(f\"Data type of the tensor: {inspection_tensor.dtype}\") print(f\"Device the tensor is on: {inspection_tensor.device}\")  <pre>The tensor:\n tensor([[ 2.2082, -0.6380,  0.4617,  0.2674],\n        [ 0.5349,  0.8094,  1.1103, -1.6898],\n        [-0.9890,  0.9580,  1.3221,  0.8172]])\n\nShape of the tensor: torch.Size([3, 4])\nData type of the tensor: torch.float32\nDevice the tensor is on: cpu\n</pre> In\u00a0[20]: Copied! <pre>subject_tensor = torch.randint(0, 100, (5, 4))\n\n# Let's pluck the element at the 2nd row (index 1) and 4th column (index 3).\nsingle_element = subject_tensor[1, 3]\n\nprint(f\"Element at [1, 3]: {single_element}\")\n\n# .item() is a glorious spell to extract the raw Python number from a single-element tensor.\n# Use it when you need to pass a tensor's value to other libraries or just print it cleanly!\nprint(f\"Its value is: {single_element.item()}\")\n\nprint(f\"Notice its data type: {single_element.dtype}\")\nprint(f\"And its shape: {single_element.shape} (a 0-dimensional tensor!)\")\n</pre> subject_tensor = torch.randint(0, 100, (5, 4))  # Let's pluck the element at the 2nd row (index 1) and 4th column (index 3). single_element = subject_tensor[1, 3]  print(f\"Element at [1, 3]: {single_element}\")  # .item() is a glorious spell to extract the raw Python number from a single-element tensor. # Use it when you need to pass a tensor's value to other libraries or just print it cleanly! print(f\"Its value is: {single_element.item()}\")  print(f\"Notice its data type: {single_element.dtype}\") print(f\"And its shape: {single_element.shape} (a 0-dimensional tensor!)\")   <pre>Element at [1, 3]: 9\nIts value is: 9\nNotice its data type: torch.int64\nAnd its shape: torch.Size([]) (a 0-dimensional tensor!)\n</pre> In\u00a0[15]: Copied! <pre># Your code for the challenges goes here!\n\nprint(\"--- 1. Odd Numbers ---\")\nodd_numbers = torch.arange(1, 20, 2)\nprint(f\"{odd_numbers}\\n\")\n\nprint(\"--- 2. Evenly Spaced ---\")\nevenly_spaced = torch.linspace(50, 100, 9)\nprint(f\"{evenly_spaced}\\n\")\n\nprint(\"--- 3. Countdown ---\")\ncountdown = torch.arange(10, -0.1, -0.5)\nprint(f\"{countdown}\\n\")\n\nprint(\"--- 4. Pi Sequence ---\")\npi_seq = torch.linspace(-torch.pi, torch.pi, 17)\nprint(f\"{pi_seq}\\n\")\n\nprint(\"--- 5. arange vs. linspace ---\")\n# arange may suffer from floating point errors and not include the endpoint!\narange_ex = torch.arange(0, 1, 0.1) \n# linspace is often safer for float ranges as it guarantees the number of points.\nlinspace_ex = torch.linspace(0, 1, 10)\nprint(f\"arange result (0 to 0.9): {arange_ex}\")\nprint(f\"linspace result (0 to 1, 10 steps): {linspace_ex}\\n\")\nprint(\"Notice how arange's result doesn't include 1, while linspace does!\\n\")\n</pre> # Your code for the challenges goes here!  print(\"--- 1. Odd Numbers ---\") odd_numbers = torch.arange(1, 20, 2) print(f\"{odd_numbers}\\n\")  print(\"--- 2. Evenly Spaced ---\") evenly_spaced = torch.linspace(50, 100, 9) print(f\"{evenly_spaced}\\n\")  print(\"--- 3. Countdown ---\") countdown = torch.arange(10, -0.1, -0.5) print(f\"{countdown}\\n\")  print(\"--- 4. Pi Sequence ---\") pi_seq = torch.linspace(-torch.pi, torch.pi, 17) print(f\"{pi_seq}\\n\")  print(\"--- 5. arange vs. linspace ---\") # arange may suffer from floating point errors and not include the endpoint! arange_ex = torch.arange(0, 1, 0.1)  # linspace is often safer for float ranges as it guarantees the number of points. linspace_ex = torch.linspace(0, 1, 10) print(f\"arange result (0 to 0.9): {arange_ex}\") print(f\"linspace result (0 to 1, 10 steps): {linspace_ex}\\n\") print(\"Notice how arange's result doesn't include 1, while linspace does!\\n\")   <pre>--- 1. Odd Numbers ---\ntensor([ 1,  3,  5,  7,  9, 11, 13, 15, 17, 19])\n\n--- 2. Evenly Spaced ---\ntensor([ 50.0000,  56.2500,  62.5000,  68.7500,  75.0000,  81.2500,  87.5000,\n         93.7500, 100.0000])\n\n--- 3. Countdown ---\ntensor([10.0000,  9.5000,  9.0000,  8.5000,  8.0000,  7.5000,  7.0000,  6.5000,\n         6.0000,  5.5000,  5.0000,  4.5000,  4.0000,  3.5000,  3.0000,  2.5000,\n         2.0000,  1.5000,  1.0000,  0.5000,  0.0000])\n\n--- 4. Pi Sequence ---\ntensor([-3.1416, -2.7489, -2.3562, -1.9635, -1.5708, -1.1781, -0.7854, -0.3927,\n         0.0000,  0.3927,  0.7854,  1.1781,  1.5708,  1.9635,  2.3562,  2.7489,\n         3.1416])\n\n--- 5. arange vs. linspace ---\narange result (0 to 0.9): tensor([0.0000, 0.1000, 0.2000, 0.3000, 0.4000, 0.5000, 0.6000, 0.7000, 0.8000,\n        0.9000])\nlinspace result (0 to 1, 10 steps): tensor([0.0000, 0.1111, 0.2222, 0.3333, 0.4444, 0.5556, 0.6667, 0.7778, 0.8889,\n        1.0000])\n\nNotice how arange's result doesn't include 1, while linspace does!\n\n</pre> In\u00a0[\u00a0]: Copied! <pre># Let's start with a template tensor\ntemplate_tensor = torch.ones(2, 4)\nprint(f\"Our template tensor:\\n {template_tensor}\\n\")\n\n# Now, create tensors LIKE our template\nzeros_mimic = torch.zeros_like(template_tensor)\nprint(f\"A zeros tensor created from the template:\\n {zeros_mimic}\\n\")\n\nrandom_mimic = torch.randn_like(template_tensor)\nprint(f\"A random tensor created from the template:\\n {random_mimic}\")\n</pre> # Let's start with a template tensor template_tensor = torch.ones(2, 4) print(f\"Our template tensor:\\n {template_tensor}\\n\")  # Now, create tensors LIKE our template zeros_mimic = torch.zeros_like(template_tensor) print(f\"A zeros tensor created from the template:\\n {zeros_mimic}\\n\")  random_mimic = torch.randn_like(template_tensor) print(f\"A random tensor created from the template:\\n {random_mimic}\")  In\u00a0[\u00a0]: Copied! <pre># Your code for the final challenges goes here!\n\n# Apprentice Challenge Solution\nprint(\"--- Apprentice Challenge ---\")\napprentice_tensor = torch.randn(3, 5)\nprint(f\"Tensor Shape: {apprentice_tensor.shape}\")\nprint(f\"Tensor DType: {apprentice_tensor.dtype}\\n\")\n\n\n# Artisan Challenge Solution\nprint(\"--- Artisan Challenge ---\")\nfavorite_numbers = torch.tensor([3.14, 42, 1337, 99.9])\nones_like_faves = torch.ones_like(favorite_numbers)\nprint(f\"Favorite Numbers Tensor: {favorite_numbers}\")\nprint(f\"Ones-Like Tensor: {ones_like_faves}\\n\")\n\n\n# Master Challenge Solution\nprint(\"--- Bias Vector Challenge ---\")\nbias_vector = torch.zeros(10)\nbias_vector[9] = 1\nprint(f\"Masterful Bias Vector: {bias_vector}\")\n</pre> # Your code for the final challenges goes here!  # Apprentice Challenge Solution print(\"--- Apprentice Challenge ---\") apprentice_tensor = torch.randn(3, 5) print(f\"Tensor Shape: {apprentice_tensor.shape}\") print(f\"Tensor DType: {apprentice_tensor.dtype}\\n\")   # Artisan Challenge Solution print(\"--- Artisan Challenge ---\") favorite_numbers = torch.tensor([3.14, 42, 1337, 99.9]) ones_like_faves = torch.ones_like(favorite_numbers) print(f\"Favorite Numbers Tensor: {favorite_numbers}\") print(f\"Ones-Like Tensor: {ones_like_faves}\\n\")   # Master Challenge Solution print(\"--- Bias Vector Challenge ---\") bias_vector = torch.zeros(10) bias_vector[9] = 1 print(f\"Masterful Bias Vector: {bias_vector}\")  In\u00a0[16]: Copied! <pre>print(\"--- 4. Positional Encoding Denominator ---\")\nd_model = 128\n# Create the sequence for 2i (i.e., 0, 2, 4, ... up to d_model-2)\ntwo_i = torch.arange(0, d_model, 2)\n# Calculate the denominator\ndenominator = 10000 ** (two_i / d_model)\nprint(f\"The first 5 values of the denominator are:\\n{denominator[:5]}\")\nprint(f\"\\nThe last 5 values of the denominator are:\\n{denominator[-5:]}\")\nprint(f\"\\nShape of the denominator tensor: {denominator.shape}\")\n</pre> print(\"--- 4. Positional Encoding Denominator ---\") d_model = 128 # Create the sequence for 2i (i.e., 0, 2, 4, ... up to d_model-2) two_i = torch.arange(0, d_model, 2) # Calculate the denominator denominator = 10000 ** (two_i / d_model) print(f\"The first 5 values of the denominator are:\\n{denominator[:5]}\") print(f\"\\nThe last 5 values of the denominator are:\\n{denominator[-5:]}\") print(f\"\\nShape of the denominator tensor: {denominator.shape}\") <pre>--- 6. Positional Encoding Denominator (Master Challenge) ---\nThe first 5 values of the denominator are:\ntensor([1.0000, 1.1548, 1.3335, 1.5399, 1.7783])\n\nThe last 5 values of the denominator are:\ntensor([4869.6753, 5623.4131, 6493.8164, 7498.9419, 8659.6436])\n\nShape of the denominator tensor: torch.Size([64])\n</pre>","tags":["tensors","module 1","beginner"],"boost":1.2},{"location":"01-tensors/01_introduction_to_tensors/#summoning-your-first-tensors","title":"Summoning Your First Tensors\u00b6","text":"<p>Module 1 | Lesson 1</p>","tags":["tensors","module 1","beginner"],"boost":1.2},{"location":"01-tensors/01_introduction_to_tensors/#professor-torchensteins-grand-directive","title":"Professor Torchenstein's Grand Directive\u00b6","text":"<p>Mwahahaha! Welcome, my brilliant acolytes. Today, we shall peel back the very fabric of reality\u2014or, at the very least, the fabric of a PyTorch tensor. We are not merely learning; we are engaging in the sacred act of creation!</p> <p>\"Prepare your minds! The tensors... they are about to be summoned!\"</p> <p></p>","tags":["tensors","module 1","beginner"],"boost":1.2},{"location":"01-tensors/01_introduction_to_tensors/#your-mission-briefing","title":"Your Mission Briefing\u00b6","text":"<p>By the end of this dark ritual, you will have mastered the arcane arts of:</p> <ul> <li>Understanding what a tensor is and why it's the fundamental building block of all modern AI.</li> <li>Summoning tensors from nothingness using a variety of powerful PyTorch functions.</li> <li>Inspecting the very soul of a tensor: its shape, data type, and the device it inhabits.</li> <li>Simple Indexing the main way to access elements of a tensor.</li> <li>Creating sequences of numbers with <code>torch.arange</code> and <code>torch.linspace</code> and <code>_like</code> methods.</li> </ul> <p>Estimated Time to Completion: 15 glorious minutes of pure, unadulterated learning.</p> <p>What You'll Need:</p> <ul> <li>A mind hungry for forbidden knowledge!</li> <li>A working PyTorch environment, ready for spellcasting.</li> <li>(Optional but recommended) A beverage of your choice\u2014creation is thirsty work!</li> </ul>","tags":["tensors","module 1","beginner"],"boost":1.2},{"location":"01-tensors/01_introduction_to_tensors/#the-theory-behind-the-magic-what-is-a-tensor-really","title":"The Theory Behind the Magic: What is a Tensor, Really?\u00b6","text":"<p>First, we must understand the incantation before we cast the spell. You've heard the word \"tensor,\" whispered in the hallowed halls of academia and screamed during GPU memory overflows. But what is it?</p> <p>Forget what the mathematicians told you about coordinate transformations for a moment. In our glorious domain of PyTorch, a tensor is simply a multi-dimensional array of numbers. It is the generalization of vectors and matrices to an arbitrary number of dimensions. Think of it as the ultimate container for your data!</p> <ul> <li>A scalar (a single number, like <code>5</code>) is a 0-dimensional tensor.</li> <li>A vector (a list of numbers, like <code>[1, 2, 3]</code>) is a 1-dimensional tensor.</li> <li>A matrix (a grid of numbers) is a 2-dimensional tensor.</li> <li>And a tensor? It can be all of those, and so much more! 3D, 4D, 5D... all await your command!</li> </ul> <p>Why not just use matrices? Mwahaha, a foolish question! Modern data is complex!</p> <ul> <li>An image is not a flat grid; it's a 3D tensor (<code>height</code>, <code>width</code>, <code>channels</code>).</li> <li>A batch of images for training is a 4D tensor (<code>batch_size</code>, <code>height</code>, <code>width</code>, <code>channels</code>).</li> <li>Text data is often represented as 3D tensors (<code>batch_size</code>, <code>sequence_length</code>, <code>embedding_dimension</code>).</li> </ul> <p>Tensors give us the power to mold and shape all this data with a single, unified tool. They are the clay from which we will sculpt our magnificent AI creations!</p>","tags":["tensors","module 1","beginner"],"boost":1.2},{"location":"01-tensors/01_introduction_to_tensors/#the-ritual-summoning-your-first-tensors","title":"The Ritual: Summoning Your First Tensors\u00b6","text":"<p>Enough theory! The time has come to channel the raw power of PyTorch. We will now perform the summoning rituals\u2014the core functions you will use constantly in your dark arts.</p> <p>First, let's prepare the laboratory by importing <code>torch</code> and setting a manual seed. Why the seed? To ensure our \"random\" experiments are reproducible! We are scientists, not gamblers!</p>","tags":["tensors","module 1","beginner"],"boost":1.2},{"location":"01-tensors/01_introduction_to_tensors/#1-conjuring-from-existing-data-torchtensor","title":"1. Conjuring from Existing Data (<code>torch.tensor</code>)\u00b6","text":"<p>The most direct way to create a tensor is from existing data, like a Python list. The <code>torch.tensor()</code> command consumes your data and transmutes it into a glorious PyTorch tensor.</p>","tags":["tensors","module 1","beginner"],"boost":1.2},{"location":"01-tensors/01_introduction_to_tensors/#2-summoning-tensors-of-a-specific-size","title":"2. Summoning Tensors of a Specific Size\u00b6","text":"<p>Often, you won't have data yet. You simply need a tensor of a particular shape, a blank canvas for your masterpiece.</p> <ul> <li><code>torch.randn(shape)</code>: Summons a tensor filled with random numbers from a standard normal distribution (mean 0, variance 1). Perfect for initializing weights in a neural network!</li> <li><code>torch.zeros(shape)</code>: Creates a tensor of the given shape filled entirely with zeros.</li> <li><code>torch.ones(shape)</code>: Creates a tensor of the given shape filled entirely with ones.</li> </ul>","tags":["tensors","module 1","beginner"],"boost":1.2},{"location":"01-tensors/01_introduction_to_tensors/#3-inspecting-your-creation","title":"3. Inspecting Your Creation\u00b6","text":"<p>A true master understands their creation. Once you have summoned a tensor, you must learn to inspect its very soul. These are the three most critical attributes you will constantly examine:</p> <ul> <li><code>.shape</code>: Reveals the dimensions of your tensor. A vital sanity check!</li> <li><code>.dtype</code>: Shows the data type of the elements within the tensor (e.g., <code>torch.float8</code>,  <code>torch.float32</code>, <code>torch.int64</code>).</li> <li><code>.device</code>: Tells you where the tensor lives\u2014on the humble CPU or the glorious GPU.</li> </ul> <p>More details about the data types and device types you will learn in 03_data_types_and_device_types</p>","tags":["tensors","module 1","beginner"],"boost":1.2},{"location":"01-tensors/01_introduction_to_tensors/#4-precision-strikes-accessing-elements","title":"4 Precision Strikes: Accessing Elements\u00b6","text":"<p>To access a single, quivering element within our tensor, we use the <code>[row, column]</code> notation, just as you would with a common Python list of lists. Remember, my apprentice: dimensions are zero-indexed! The first row is row <code>0</code>, not row 1! A classic pitfall for the uninitiated.</p>","tags":["tensors","module 1","beginner"],"boost":1.2},{"location":"01-tensors/01_introduction_to_tensors/#5-creating-sequential-tensors","title":"5. Creating Sequential Tensors\u00b6","text":"<p>Sometimes, you need tensors with predictable, orderly values.</p> <ul> <li><code>torch.arange(start, end, step)</code>: Creates a 1D tensor with values from <code>start</code> (inclusive) to <code>end</code> (exclusive), with a given <code>step</code>. It's the PyTorch version of Python's <code>range()</code>.</li> <li><code>torch.linspace(start, end, steps)</code>: Creates a 1D tensor with a specific number of <code>steps</code> evenly spaced between <code>start</code> and <code>end</code> (both inclusive).</li> </ul>","tags":["tensors","module 1","beginner"],"boost":1.2},{"location":"01-tensors/01_introduction_to_tensors/#your-mission-a-gauntlet-of-sequences","title":"Your Mission: A Gauntlet of Sequences!\u00b6","text":"<p>Your list of challenges grows, apprentice! Prove your mastery.</p> <ol> <li>Odd Numbers: Create a 1D tensor of all odd numbers from 1 to 19.</li> <li>Evenly Spaced: Create a 1D tensor with 9 evenly spaced numbers from 50 to 100.</li> <li>Countdown: Create a tensor that counts down from 10 to 0 in steps of 0.5.</li> <li>Pi Sequence: The famous <code>sin</code> and <code>cos</code> functions, used in positional encodings, operate on radians. Create a tensor with 17 evenly spaced numbers from <code>-\u03c0</code> to <code>\u03c0</code>. (Hint: <code>torch.pi</code> is your friend!)</li> <li><code>arange</code> vs. <code>linspace</code>: Create a tensor of numbers from 0 to 1 with a step of 0.1 using <code>arange</code>. Then, create a tensor from 0 to 1 with 11 steps using <code>linspace</code>. Observe the subtle but critical difference in their outputs! What causes it?</li> </ol>","tags":["tensors","module 1","beginner"],"boost":1.2},{"location":"01-tensors/01_introduction_to_tensors/#6-creating-tensors-from-other-tensors-the-_like-methods","title":"6. Creating Tensors from Other Tensors (the <code>_like</code> methods)\u00b6","text":"<p>Behold, a most elegant form of mimicry! Often, you will need to create a new tensor that has the exact same shape as another. PyTorch provides the <code>_like</code> methods for this very purpose.</p> <ul> <li><code>torch.zeros_like(input_tensor)</code>: Creates a tensor of all zeros with the same <code>shape</code>, <code>dtype</code>, and <code>device</code> as the input tensor.</li> <li><code>torch.ones_like(input_tensor)</code>: The same, but for ones!</li> <li><code>torch.randn_like(input_tensor)</code>: The same, but for random numbers!</li> </ul>","tags":["tensors","module 1","beginner"],"boost":1.2},{"location":"01-tensors/01_introduction_to_tensors/#real-world-sorcery-where-are-sequential-tensors-used","title":"Real-World Sorcery: Where are Sequential Tensors Used?\u00b6","text":"<p>You may wonder, \"Professor, is this just for making neat little rows of numbers?\" A fair question from a novice! The answer is a resounding NO! These sequential tensors are the silent bedrock of many powerful constructs:</p> <ul> <li><p>Positional Encodings in Transformers: How does a Transformer know the order of words in a sentence? It doesn't, inherently! We must inject that information. The very first step is often to create a tensor representing the positions <code>[0, 1, 2, ..., sequence_length - 1]</code> using <code>torch.arange</code>. This sequence is then transformed into a high-dimensional positional embedding.</p> </li> <li><p>Generating Time-Series Data: When working with audio, financial data, or any kind of signal, you often need a time axis. <code>torch.linspace</code> is perfect for creating a smooth, evenly-spaced time vector to plot or process your data against.</p> </li> <li><p>Creating Coordinate Grids in Vision: For advanced image manipulation, you might need a grid representing the <code>(x, y)</code> coordinates of every pixel. You can generate the <code>x</code> and <code>y</code> vectors separately using <code>torch.arange</code> and then combine them to form this essential grid.</p> </li> </ul>","tags":["tensors","module 1","beginner"],"boost":1.2},{"location":"01-tensors/01_introduction_to_tensors/#your-mission-forge-your-own-creation","title":"Your Mission: Forge Your Own Creation!\u00b6","text":"<p>A true master never stops practicing. I leave you with these challenges to solidify your newfound power. Do not be afraid to experiment! To the lab!</p> <ol> <li><p>Apprentice Challenge: Create a 2D tensor (a matrix) of shape <code>(3, 5)</code> filled with random numbers. Then, print its shape and data type to the console.</p> </li> <li><p>Artisan Challenge: Create a 1D tensor of your favorite numbers (at least 4). Then, create a second tensor of all ones that has the exact same shape as your first tensor.</p> </li> <li><p>Create the bias vector: You are tasked with creating the initial \"bias\" vector for a small neural network layer with 10 output neurons. For arcane reasons, the master architect (me!) has decreed that it must be a 1D tensor, filled with zeros, except for the very last element, which must be <code>1</code>. Create this specific tensor!</p> </li> <li><p>Positional Encoding Denominator: In the legendary Transformer architecture, a key component is the denominator <code>10000^(2i / d_model)</code>. Your mission is to create this 1D tensor. Let <code>d_model = 128</code>. The term <code>i</code> represents dimension pairs, so it goes from <code>0</code> to <code>d_model/2 - 1</code>. Use <code>torch.arange</code> to create the <code>2i</code> sequence first, then perform the final calculation. This is a vital step in building the neural networks of tomorrow!</p> </li> </ol>","tags":["tensors","module 1","beginner"],"boost":1.2},{"location":"01-tensors/01_introduction_to_tensors/#summary-the-knowledge-is-yours","title":"Summary: The Knowledge Is Yours!\u00b6","text":"<p>Magnificent! You've wrestled with the raw chaos of creation and emerged victorious! Let's recount the powerful secrets you've assimilated today:</p> <ul> <li>Tensors are Everything: You now understand that a tensor is a multi-dimensional array, the fundamental data structure for every piece of data you will encounter in your machine learning journey.</li> <li>The Summoning Rituals: You have mastered the core incantations for creating tensors: <code>torch.tensor</code>, <code>torch.randn</code>/<code>zeros</code>/<code>ones</code>, the powerful <code>_like</code> variants, and the sequence generators <code>torch.arange</code> and <code>torch.linspace</code>.</li> <li>Know Your Creation: You have learned the vital importance of inspecting your tensors using <code>.shape</code>, <code>.dtype</code>, and <code>.device</code> to understand their nature and prevent catastrophic errors.</li> </ul> <p>You have taken your first, most important step. The power is now in your hands!</p>","tags":["tensors","module 1","beginner"],"boost":1.2},{"location":"01-tensors/01_introduction_to_tensors/#professor-torchensteins-outro","title":"Professor Torchenstein's Outro\u00b6","text":"<p>Do you feel it? The hum of latent power in your very fingertips? That, my apprentice, is the feeling of true understanding. You have summoned your first tensors, and they have answered your call. But this is merely the beginning! Our creations are still... rigid. Inflexible.</p> <p>In our next lesson, we will learn the dark arts of Tensor Shape-Shifting &amp; Sorcery! We will slice, squeeze, and permute our tensors until reality itself seems to bend to our will.</p> <p>Until then, keep your learning rates high and your gradients flowing. The future of AI is in our hands! Mwahahahahaha!</p>      Your browser does not support the video tag.  <p>Proceed to the Next Experiment: Tensor Surgery &amp; Assembly!</p>","tags":["tensors","module 1","beginner"],"boost":1.2},{"location":"01-tensors/02a_tensor_manipulation/","title":"Tensor Surgery &amp; Assembly","text":"<p>Module 1 | Lesson 2a</p> In\u00a0[33]: Copied! <pre>import torch\n\n# Set the seed for cosmic consistency\ntorch.manual_seed(42)\n\n# Our test subject: A 2D tensor of integers. Imagine it's a map to a hidden treasure!\n# Or perhaps experimental results from a daring new potion.\nsubject_tensor = torch.randint(0, 100, (5, 4))\n\nprint(f\"Our subject tensor of shape {subject_tensor.shape}, ripe for dissection:\")\nprint(subject_tensor)\n</pre> import torch  # Set the seed for cosmic consistency torch.manual_seed(42)  # Our test subject: A 2D tensor of integers. Imagine it's a map to a hidden treasure! # Or perhaps experimental results from a daring new potion. subject_tensor = torch.randint(0, 100, (5, 4))  print(f\"Our subject tensor of shape {subject_tensor.shape}, ripe for dissection:\") print(subject_tensor)  <pre>Our subject tensor of shape torch.Size([5, 4]), ripe for dissection:\ntensor([[42, 67, 76, 14],\n        [26, 35, 20, 24],\n        [50, 13, 78, 14],\n        [10, 54, 31, 72],\n        [15, 95, 67,  6]])\n</pre> In\u00a0[2]: Copied! <pre># Get the entire 3rd row (index 2)\nthird_row = subject_tensor[2, :] # or simply subject_tensor[2]\nprint(f\"The third row: {third_row}\")\nprint(f\"Shape of the row: {third_row.shape}\\n\")\n\n\n# Get the entire 2nd column (index 1)\nsecond_column = subject_tensor[:, 1]\nprint(f\"The second column: {second_column}\")\nprint(f\"Shape of the column: {second_column.shape}\")\n</pre> # Get the entire 3rd row (index 2) third_row = subject_tensor[2, :] # or simply subject_tensor[2] print(f\"The third row: {third_row}\") print(f\"Shape of the row: {third_row.shape}\\n\")   # Get the entire 2nd column (index 1) second_column = subject_tensor[:, 1] print(f\"The second column: {second_column}\") print(f\"Shape of the column: {second_column.shape}\") <pre>The third row: tensor([50, 13, 78, 14])\nShape of the row: torch.Size([4])\n\nThe second column: tensor([67, 35, 13, 54, 95])\nShape of the column: torch.Size([5])\n</pre> In\u00a0[34]: Copied! <pre># Carve out rows 1 and 2, and columns 2 and 3\nsub_tensor = subject_tensor[1:3, 2:4]\n\nprint(\"Our carved sub-tensor:\")\nprint(sub_tensor)\nprint(f\"Shape of the sub-tensor: {sub_tensor.shape}\")\n</pre> # Carve out rows 1 and 2, and columns 2 and 3 sub_tensor = subject_tensor[1:3, 2:4]  print(\"Our carved sub-tensor:\") print(sub_tensor) print(f\"Shape of the sub-tensor: {sub_tensor.shape}\")  <pre>Our carved sub-tensor:\ntensor([[20, 24],\n        [78, 14]])\nShape of the sub-tensor: torch.Size([2, 2])\n</pre> In\u00a0[35]: Copied! <pre># Create the boolean mask\nmask = subject_tensor &gt; 50\n\nprint(\"The boolean mask (True where value &gt; 50):\")\nprint(mask)\n\n# Apply the mask\nselected_elements = subject_tensor[mask]\n\nprint(\"\\nElements greater than 50:\")\nprint(selected_elements)\nprint(f\"Shape of the result: {selected_elements.shape} (always a 1D tensor!)\")\n\n# You can also combine conditions! Mwahaha!\n# Let's find elements between 20 and 40.\nmask_combined = (subject_tensor &gt; 20) &amp; (subject_tensor &lt; 40)\nprint(\"\\nElements between 20 and 40:\")\nprint(subject_tensor[mask_combined])\n</pre> # Create the boolean mask mask = subject_tensor &gt; 50  print(\"The boolean mask (True where value &gt; 50):\") print(mask)  # Apply the mask selected_elements = subject_tensor[mask]  print(\"\\nElements greater than 50:\") print(selected_elements) print(f\"Shape of the result: {selected_elements.shape} (always a 1D tensor!)\")  # You can also combine conditions! Mwahaha! # Let's find elements between 20 and 40. mask_combined = (subject_tensor &gt; 20) &amp; (subject_tensor &lt; 40) print(\"\\nElements between 20 and 40:\") print(subject_tensor[mask_combined])  <pre>The boolean mask (True where value &gt; 50):\ntensor([[False,  True,  True, False],\n        [False, False, False, False],\n        [False, False,  True, False],\n        [False,  True, False,  True],\n        [False,  True,  True, False]])\n\nElements greater than 50:\ntensor([67, 76, 78, 54, 72, 95, 67])\nShape of the result: torch.Size([7]) (always a 1D tensor!)\n\nElements between 20 and 40:\ntensor([26, 35, 24, 31])\n</pre> In\u00a0[36]: Copied! <pre># Your code for the Slicer's Gauntlet goes here!\n\n# --- 1. The Corner Pocket ---\nprint(\"--- 1. The Corner Pocket ---\")\ncorner_element = subject_tensor[-1, -1] # Negative indexing for the win!\nprint(f\"The corner element is: {corner_element.item()}\\n\")\n\n# --- 2. The Central Core ---\nprint(\"--- 2. The Central Core ---\")\ncentral_core = subject_tensor[1:4, 1:3]\nprint(f\"The central core:\\\\n{central_core}\\n\")\n\n# --- 3. The Even Stevens ---\nprint(\"--- 3. The Even Stevens ---\")\neven_mask = subject_tensor % 2 == 0\nprint(f\"The mask for even numbers:\\\\n{even_mask}\\n\")\nprint(f\"The even numbers themselves: {subject_tensor[even_mask]}\\n\")\n\n\n# --- 4. The Grand Mutation ---\nprint(\"--- 4. The Grand Mutation ---\")\n# Let's not mutate our original, that would be reckless! Let's clone it first.\nmutated_tensor = subject_tensor.clone()\nmutated_tensor[even_mask] = -1\nprint(f\"The tensor after mutating even numbers to -1:\\n{mutated_tensor}\")\n</pre> # Your code for the Slicer's Gauntlet goes here!  # --- 1. The Corner Pocket --- print(\"--- 1. The Corner Pocket ---\") corner_element = subject_tensor[-1, -1] # Negative indexing for the win! print(f\"The corner element is: {corner_element.item()}\\n\")  # --- 2. The Central Core --- print(\"--- 2. The Central Core ---\") central_core = subject_tensor[1:4, 1:3] print(f\"The central core:\\\\n{central_core}\\n\")  # --- 3. The Even Stevens --- print(\"--- 3. The Even Stevens ---\") even_mask = subject_tensor % 2 == 0 print(f\"The mask for even numbers:\\\\n{even_mask}\\n\") print(f\"The even numbers themselves: {subject_tensor[even_mask]}\\n\")   # --- 4. The Grand Mutation --- print(\"--- 4. The Grand Mutation ---\") # Let's not mutate our original, that would be reckless! Let's clone it first. mutated_tensor = subject_tensor.clone() mutated_tensor[even_mask] = -1 print(f\"The tensor after mutating even numbers to -1:\\n{mutated_tensor}\")  <pre>--- 1. The Corner Pocket ---\nThe corner element is: 6\n\n--- 2. The Central Core ---\nThe central core:\\ntensor([[35, 20],\n        [13, 78],\n        [54, 31]])\n\n--- 3. The Even Stevens ---\nThe mask for even numbers:\\ntensor([[ True, False,  True,  True],\n        [ True, False,  True,  True],\n        [ True, False,  True,  True],\n        [ True,  True, False,  True],\n        [False, False, False,  True]])\n\nThe even numbers themselves: tensor([42, 76, 14, 26, 20, 24, 50, 78, 14, 10, 54, 72,  6])\n\n--- 4. The Grand Mutation ---\nThe tensor after mutating even numbers to -1:\ntensor([[-1, 67, -1, -1],\n        [-1, 35, -1, -1],\n        [-1, 13, -1, -1],\n        [-1, -1, 31, -1],\n        [15, 95, 67, -1]])\n</pre> In\u00a0[37]: Copied! <pre># Three 2x3 tensors, our loyal minions awaiting fusion\ntensor_a = torch.ones(2, 4)\ntensor_b = torch.ones(2, 4) * 2\ntensor_c = torch.ones(2, 4) * 3\n\nprint(\"Our test subjects, ready for fusion:\")\nprint(f\"Tensor A (shape {tensor_a.shape}):\\n{tensor_a}\\n\")\nprint(f\"Tensor B (shape {tensor_b.shape}):\\n{tensor_b}\\n\")\nprint(f\"Tensor C (shape {tensor_c.shape}):\\n{tensor_c}\\n\")\n</pre>  # Three 2x3 tensors, our loyal minions awaiting fusion tensor_a = torch.ones(2, 4) tensor_b = torch.ones(2, 4) * 2 tensor_c = torch.ones(2, 4) * 3  print(\"Our test subjects, ready for fusion:\") print(f\"Tensor A (shape {tensor_a.shape}):\\n{tensor_a}\\n\") print(f\"Tensor B (shape {tensor_b.shape}):\\n{tensor_b}\\n\") print(f\"Tensor C (shape {tensor_c.shape}):\\n{tensor_c}\\n\")  <pre>Our test subjects, ready for fusion:\nTensor A (shape torch.Size([2, 4])):\ntensor([[1., 1., 1., 1.],\n        [1., 1., 1., 1.]])\n\nTensor B (shape torch.Size([2, 4])):\ntensor([[2., 2., 2., 2.],\n        [2., 2., 2., 2.]])\n\nTensor C (shape torch.Size([2, 4])):\ntensor([[3., 3., 3., 3.],\n        [3., 3., 3., 3.]])\n\n</pre> In\u00a0[7]: Copied! <pre># Concatenating along dimension 0 (rows) - like stacking pancakes! \ud83e\udd5e\u2b06\ufe0f\u2b07\ufe0f\ncat_dim0 = torch.cat([tensor_a, tensor_b, tensor_c], dim=0)\nprint(\"Concatenated along dimension 0 (rows) [stacking pancakes \ud83e\udd5e\u2b06\ufe0f\u2b07\ufe0f]:\")\nprint(f\"Result shape: {cat_dim0.shape}\")\nprint(f\"Result:\\n{cat_dim0}\\n\")\n\n# Concatenating along dimension 1 (columns) - like laying bricks side by side! \ud83e\uddf1\ud83e\uddf1\ud83e\uddf1\ncat_dim1 = torch.cat([tensor_a, tensor_b, tensor_c], dim=1)\nprint(\"Concatenated along dimension 1 (columns) [laying bricks side by side \ud83e\uddf1\ud83e\uddf1\ud83e\uddf1]:\")\nprint(f\"Result shape: {cat_dim1.shape}\")\nprint(f\"Result:\\n{cat_dim1}\")\n</pre> # Concatenating along dimension 0 (rows) - like stacking pancakes! \ud83e\udd5e\u2b06\ufe0f\u2b07\ufe0f cat_dim0 = torch.cat([tensor_a, tensor_b, tensor_c], dim=0) print(\"Concatenated along dimension 0 (rows) [stacking pancakes \ud83e\udd5e\u2b06\ufe0f\u2b07\ufe0f]:\") print(f\"Result shape: {cat_dim0.shape}\") print(f\"Result:\\n{cat_dim0}\\n\")  # Concatenating along dimension 1 (columns) - like laying bricks side by side! \ud83e\uddf1\ud83e\uddf1\ud83e\uddf1 cat_dim1 = torch.cat([tensor_a, tensor_b, tensor_c], dim=1) print(\"Concatenated along dimension 1 (columns) [laying bricks side by side \ud83e\uddf1\ud83e\uddf1\ud83e\uddf1]:\") print(f\"Result shape: {cat_dim1.shape}\") print(f\"Result:\\n{cat_dim1}\")  <pre>Concatenated along dimension 0 (rows) [stacking pancakes \ud83e\udd5e\u2b06\ufe0f\u2b07\ufe0f]:\nResult shape: torch.Size([6, 4])\nResult:\ntensor([[1., 1., 1., 1.],\n        [1., 1., 1., 1.],\n        [2., 2., 2., 2.],\n        [2., 2., 2., 2.],\n        [3., 3., 3., 3.],\n        [3., 3., 3., 3.]])\n\nConcatenated along dimension 1 (columns) [laying bricks side by side \ud83e\uddf1\ud83e\uddf1\ud83e\uddf1]:\nResult shape: torch.Size([2, 12])\nResult:\ntensor([[1., 1., 1., 1., 2., 2., 2., 2., 3., 3., 3., 3.],\n        [1., 1., 1., 1., 2., 2., 2., 2., 3., 3., 3., 3.]])\n</pre> In\u00a0[38]: Copied! <pre># Create tensors with different shapes\ntensor_wide = torch.ones(3, 8) * 4   # 3x5 tensor filled with 4s\ntensor_narrow = torch.ones(3, 2) * 5  # 3x2 tensor filled with 5s\n\nprint(f\"Wide tensor [big cake \ud83c\udf82] (shape {tensor_wide.shape}):\\n{tensor_wide}\\n\")\nprint(f\"Narrow tensor [small cupcake \ud83e\uddc1 ] (shape {tensor_narrow.shape}):\\n{tensor_narrow}\\n\")\n\n# This FAILS: Concatenating along dimension 0, you can stack pancakes with different sizes \n# They have different column counts (5 vs 2)\nprint(\"\u274c Attempting to concatenate along dimension 0 (rows), stack cake on top of cupcake ...\")\ntry:\n    cat_cols_fail = torch.cat([tensor_wide, tensor_narrow], dim=0)\nexcept RuntimeError as e:\n    print(f\"\ud83c\udf82/\ud83e\uddc1 This couldn't work! \\nError as expected: {str(e)}\")\n    \n\nprint(\"Our unequal test subjects:\")\nprint(f\"Wide tensor [big cake \ud83c\udf82] ({tensor_wide.shape}):\\n{tensor_wide}\\n\")\nprint(f\"Narrow tensor [small cupcake \ud83e\uddc1] ({tensor_narrow.shape}):\\n{tensor_narrow}\\n\")\n\n# This WORKS: Concatenating along dimension 1 (columns)\n# Both have 3 rows, so we can lay them side by side horizontally\nprint(\"\u2705 Concatenating along dimension 1 (columns) - SUCCESS!\")\ncat_rows_success = torch.cat([tensor_wide, tensor_narrow], dim=1)\nprint(f\"Result shape: {cat_rows_success.shape}\")\nprint(f\"Result:\\n{cat_rows_success}\\n\")\n</pre> # Create tensors with different shapes tensor_wide = torch.ones(3, 8) * 4   # 3x5 tensor filled with 4s tensor_narrow = torch.ones(3, 2) * 5  # 3x2 tensor filled with 5s  print(f\"Wide tensor [big cake \ud83c\udf82] (shape {tensor_wide.shape}):\\n{tensor_wide}\\n\") print(f\"Narrow tensor [small cupcake \ud83e\uddc1 ] (shape {tensor_narrow.shape}):\\n{tensor_narrow}\\n\")  # This FAILS: Concatenating along dimension 0, you can stack pancakes with different sizes  # They have different column counts (5 vs 2) print(\"\u274c Attempting to concatenate along dimension 0 (rows), stack cake on top of cupcake ...\") try:     cat_cols_fail = torch.cat([tensor_wide, tensor_narrow], dim=0) except RuntimeError as e:     print(f\"\ud83c\udf82/\ud83e\uddc1 This couldn't work! \\nError as expected: {str(e)}\")       print(\"Our unequal test subjects:\") print(f\"Wide tensor [big cake \ud83c\udf82] ({tensor_wide.shape}):\\n{tensor_wide}\\n\") print(f\"Narrow tensor [small cupcake \ud83e\uddc1] ({tensor_narrow.shape}):\\n{tensor_narrow}\\n\")  # This WORKS: Concatenating along dimension 1 (columns) # Both have 3 rows, so we can lay them side by side horizontally print(\"\u2705 Concatenating along dimension 1 (columns) - SUCCESS!\") cat_rows_success = torch.cat([tensor_wide, tensor_narrow], dim=1) print(f\"Result shape: {cat_rows_success.shape}\") print(f\"Result:\\n{cat_rows_success}\\n\")    <pre>Wide tensor [big cake \ud83c\udf82] (shape torch.Size([3, 8])):\ntensor([[4., 4., 4., 4., 4., 4., 4., 4.],\n        [4., 4., 4., 4., 4., 4., 4., 4.],\n        [4., 4., 4., 4., 4., 4., 4., 4.]])\n\nNarrow tensor [small cupcake \ud83e\uddc1 ] (shape torch.Size([3, 2])):\ntensor([[5., 5.],\n        [5., 5.],\n        [5., 5.]])\n\n\u274c Attempting to concatenate along dimension 0 (rows), stack cake on top of cupcake ...\n\ud83c\udf82/\ud83e\uddc1 This couldn't work! \nError as expected: Sizes of tensors must match except in dimension 0. Expected size 8 but got size 2 for tensor number 1 in the list.\nOur unequal test subjects:\nWide tensor [big cake \ud83c\udf82] (torch.Size([3, 8])):\ntensor([[4., 4., 4., 4., 4., 4., 4., 4.],\n        [4., 4., 4., 4., 4., 4., 4., 4.],\n        [4., 4., 4., 4., 4., 4., 4., 4.]])\n\nNarrow tensor [small cupcake \ud83e\uddc1] (torch.Size([3, 2])):\ntensor([[5., 5.],\n        [5., 5.],\n        [5., 5.]])\n\n\u2705 Concatenating along dimension 1 (columns) - SUCCESS!\nResult shape: torch.Size([3, 10])\nResult:\ntensor([[4., 4., 4., 4., 4., 4., 4., 4., 5., 5.],\n        [4., 4., 4., 4., 4., 4., 4., 4., 5., 5.],\n        [4., 4., 4., 4., 4., 4., 4., 4., 5., 5.]])\n\n</pre> In\u00a0[39]: Copied! <pre># Let's create simple 1D tensors first\nruler_1 = torch.tensor([1, 2, 3, 4])      # A ruler with numbers 1,2,3,4\nruler_2 = torch.tensor([10, 20, 30, 40])  # A ruler with numbers 10,20,30,40  \nruler_3 = torch.tensor([100, 200, 300, 400]) # A ruler with numbers 100,200,300,400\n\nprint(\"Our three rulers \ud83d\udccf (1D tensors):\")\nprint(f\"Ruler 1: {ruler_1} (shape: {ruler_1.shape})\")\nprint(f\"Ruler 2: {ruler_2} (shape: {ruler_2.shape})\")\nprint(f\"Ruler 3: {ruler_3} (shape: {ruler_3.shape})\\n\")\n\n# Stack them to create a 2D matrix (like putting rulers on top of each other)\nstacked_rulers = torch.stack([ruler_1, ruler_2, ruler_3], dim=0)\nprint(\"Stacked rulers \ud83d\udff0 (dim=0) - like placing rulers on top of each other:\")\nprint(f\"Result shape: {stacked_rulers.shape}\")  # Notice: (3,4) - we added a new dimension!\nprint(f\"Result:\\n{stacked_rulers}\\n\")\n\n# Each \"ruler\" is now accessible as a row\nprint(\"Access individual rulers:\")\nprint(f\"First ruler:  {stacked_rulers[0]}\")\nprint(f\"Second ruler: {stacked_rulers[1]}\")  \nprint(f\"Third ruler:  {stacked_rulers[2]}\")\n</pre> # Let's create simple 1D tensors first ruler_1 = torch.tensor([1, 2, 3, 4])      # A ruler with numbers 1,2,3,4 ruler_2 = torch.tensor([10, 20, 30, 40])  # A ruler with numbers 10,20,30,40   ruler_3 = torch.tensor([100, 200, 300, 400]) # A ruler with numbers 100,200,300,400  print(\"Our three rulers \ud83d\udccf (1D tensors):\") print(f\"Ruler 1: {ruler_1} (shape: {ruler_1.shape})\") print(f\"Ruler 2: {ruler_2} (shape: {ruler_2.shape})\") print(f\"Ruler 3: {ruler_3} (shape: {ruler_3.shape})\\n\")  # Stack them to create a 2D matrix (like putting rulers on top of each other) stacked_rulers = torch.stack([ruler_1, ruler_2, ruler_3], dim=0) print(\"Stacked rulers \ud83d\udff0 (dim=0) - like placing rulers on top of each other:\") print(f\"Result shape: {stacked_rulers.shape}\")  # Notice: (3,4) - we added a new dimension! print(f\"Result:\\n{stacked_rulers}\\n\")  # Each \"ruler\" is now accessible as a row print(\"Access individual rulers:\") print(f\"First ruler:  {stacked_rulers[0]}\") print(f\"Second ruler: {stacked_rulers[1]}\")   print(f\"Third ruler:  {stacked_rulers[2]}\")   <pre>Our three rulers \ud83d\udccf (1D tensors):\nRuler 1: tensor([1, 2, 3, 4]) (shape: torch.Size([4]))\nRuler 2: tensor([10, 20, 30, 40]) (shape: torch.Size([4]))\nRuler 3: tensor([100, 200, 300, 400]) (shape: torch.Size([4]))\n\nStacked rulers \ud83d\udff0 (dim=0) - like placing rulers on top of each other:\nResult shape: torch.Size([3, 4])\nResult:\ntensor([[  1,   2,   3,   4],\n        [ 10,  20,  30,  40],\n        [100, 200, 300, 400]])\n\nAccess individual rulers:\nFirst ruler:  tensor([1, 2, 3, 4])\nSecond ruler: tensor([10, 20, 30, 40])\nThird ruler:  tensor([100, 200, 300, 400])\n</pre> In\u00a0[26]: Copied! <pre># We can also stack along dimension 1 (different arrangement)\nstack_dim1 = torch.stack([ruler_1, ruler_2, ruler_3], dim=1)\nprint(\"Stacked rulers \u23f8\ufe0f (dim=1) - like arranging rulers side by side:\")\nprint(f\"Result shape: {stack_dim1.shape}\")  # Notice: (4,3) - different arrangement!\nprint(f\"Result:\\n{stack_dim1}\\n\")\n\n# Each column now represents values from all three rulers at the same position\nprint(\"Notice the pattern:\")\nprint(\"Each row shows the 1st, 2nd, 3rd... element from ALL rulers\")\nprint(f\"Position 0 from all rulers: {stack_dim1[0]}\")  # [1, 10, 100]\nprint(f\"Position 1 from all rulers: {stack_dim1[1]}\")  # [2, 20, 200]\n</pre> # We can also stack along dimension 1 (different arrangement) stack_dim1 = torch.stack([ruler_1, ruler_2, ruler_3], dim=1) print(\"Stacked rulers \u23f8\ufe0f (dim=1) - like arranging rulers side by side:\") print(f\"Result shape: {stack_dim1.shape}\")  # Notice: (4,3) - different arrangement! print(f\"Result:\\n{stack_dim1}\\n\")  # Each column now represents values from all three rulers at the same position print(\"Notice the pattern:\") print(\"Each row shows the 1st, 2nd, 3rd... element from ALL rulers\") print(f\"Position 0 from all rulers: {stack_dim1[0]}\")  # [1, 10, 100] print(f\"Position 1 from all rulers: {stack_dim1[1]}\")  # [2, 20, 200] <pre>Stacked rulers \u23f8\ufe0f (dim=1) - like arranging rulers side by side:\nResult shape: torch.Size([4, 3])\nResult:\ntensor([[  1,  10, 100],\n        [  2,  20, 200],\n        [  3,  30, 300],\n        [  4,  40, 400]])\n\nNotice the pattern:\nEach row shows the 1st, 2nd, 3rd... element from ALL rulers\nPosition 0 from all rulers: tensor([  1,  10, 100])\nPosition 1 from all rulers: tensor([  2,  20, 200])\n</pre> In\u00a0[40]: Copied! <pre># Create three 2D \"pages\" for our book\npage_1 = torch.ones(5, 2) * 1    # Page 1: all 1s\npage_2 = torch.ones(5, 2) * 2    # Page 2: all 2s  \npage_3 = torch.ones(5, 2) * 3    # Page 3: all 3s\n\nprint(\"Our three pages (2D tensors):\")\nprint(f\"\ud83d\udcc4 Page 1 (shape {page_1.shape}):\\n{page_1}\\n\")\nprint(f\"\ud83d\udcc4 Page 2 (shape {page_2.shape}):\\n{page_2}\\n\") \nprint(f\"\ud83d\udcc4 Page 3 (shape {page_3.shape}):\\n{page_3}\\n\")\n\n# Stack them along dimension 0 to create a 3D \"book\"\nbook = torch.stack([page_1, page_2, page_3], dim=0)\nprint(\"\ud83d\udcda BEHOLD! Our 3D book (stacked along dim=0):\")\nprint(f\"Book shape: {book.shape}\")  # (3, 2, 3) = (pages, rows, columns)\nprint(f\"Full book:\\n{book}\\n\")\n\n# Now we can access individual pages, rows, or even specific elements!\nprint(\"\ud83d\udd0d Accessing different parts of our 3D tensor:\")\nprint(f\"\ud83d\udcd6 Entire first page (book[0]):\\n{book[0]}\\n\")\nprint(f\"\ud83d\udcdd First row of second page (book[1, 0]): {book[1, 0]}\")\nprint(f\"\ud83c\udfaf Specific element - page 2, row 1, column 2 (book[1, 0, 1]): {book[1, 0, 1].item()}\")\n\nprint(f\"\\n\ud83e\udd14 Think of it as: Book[page_number][row_number][column_number]\")\n</pre> # Create three 2D \"pages\" for our book page_1 = torch.ones(5, 2) * 1    # Page 1: all 1s page_2 = torch.ones(5, 2) * 2    # Page 2: all 2s   page_3 = torch.ones(5, 2) * 3    # Page 3: all 3s  print(\"Our three pages (2D tensors):\") print(f\"\ud83d\udcc4 Page 1 (shape {page_1.shape}):\\n{page_1}\\n\") print(f\"\ud83d\udcc4 Page 2 (shape {page_2.shape}):\\n{page_2}\\n\")  print(f\"\ud83d\udcc4 Page 3 (shape {page_3.shape}):\\n{page_3}\\n\")  # Stack them along dimension 0 to create a 3D \"book\" book = torch.stack([page_1, page_2, page_3], dim=0) print(\"\ud83d\udcda BEHOLD! Our 3D book (stacked along dim=0):\") print(f\"Book shape: {book.shape}\")  # (3, 2, 3) = (pages, rows, columns) print(f\"Full book:\\n{book}\\n\")  # Now we can access individual pages, rows, or even specific elements! print(\"\ud83d\udd0d Accessing different parts of our 3D tensor:\") print(f\"\ud83d\udcd6 Entire first page (book[0]):\\n{book[0]}\\n\") print(f\"\ud83d\udcdd First row of second page (book[1, 0]): {book[1, 0]}\") print(f\"\ud83c\udfaf Specific element - page 2, row 1, column 2 (book[1, 0, 1]): {book[1, 0, 1].item()}\")  print(f\"\\n\ud83e\udd14 Think of it as: Book[page_number][row_number][column_number]\")  <pre>Our three pages (2D tensors):\n\ud83d\udcc4 Page 1 (shape torch.Size([5, 2])):\ntensor([[1., 1.],\n        [1., 1.],\n        [1., 1.],\n        [1., 1.],\n        [1., 1.]])\n\n\ud83d\udcc4 Page 2 (shape torch.Size([5, 2])):\ntensor([[2., 2.],\n        [2., 2.],\n        [2., 2.],\n        [2., 2.],\n        [2., 2.]])\n\n\ud83d\udcc4 Page 3 (shape torch.Size([5, 2])):\ntensor([[3., 3.],\n        [3., 3.],\n        [3., 3.],\n        [3., 3.],\n        [3., 3.]])\n\n\ud83d\udcda BEHOLD! Our 3D book (stacked along dim=0):\nBook shape: torch.Size([3, 5, 2])\nFull book:\ntensor([[[1., 1.],\n         [1., 1.],\n         [1., 1.],\n         [1., 1.],\n         [1., 1.]],\n\n        [[2., 2.],\n         [2., 2.],\n         [2., 2.],\n         [2., 2.],\n         [2., 2.]],\n\n        [[3., 3.],\n         [3., 3.],\n         [3., 3.],\n         [3., 3.],\n         [3., 3.]]])\n\n\ud83d\udd0d Accessing different parts of our 3D tensor:\n\ud83d\udcd6 Entire first page (book[0]):\ntensor([[1., 1.],\n        [1., 1.],\n        [1., 1.],\n        [1., 1.],\n        [1., 1.]])\n\n\ud83d\udcdd First row of second page (book[1, 0]): tensor([2., 2.])\n\ud83c\udfaf Specific element - page 2, row 1, column 2 (book[1, 0, 1]): 2.0\n\n\ud83e\udd14 Think of it as: Book[page_number][row_number][column_number]\n</pre> In\u00a0[41]: Copied! <pre># Let's arrange our three 2D clay tablets in three different ways!\nprint(\"\ud83e\uddf1 Starting with three identical clay tablets:\")\nprint(f\"Each tablet shape: {page_1.shape} (5 rows, 2 columns)\")\n\n# dim=0: Stack tablets front-to-back (into the box)\nstack_dim0 = torch.stack([page_1, page_2, page_3], dim=0)\nprint(f\"\\n\ud83d\udcda Stacking front-to-back (dim=0): Shape {stack_dim0.shape}\")\nprint(\"   (depth=3, heith=5, width=2) - tablets go deeper into the box\")\nprint(f\"Front of the box):\\n{stack_dim0[0]}\\n\")\n\n# dim=1: Stack tablets bottom-to-top (building upward)  \nstack_dim1 = torch.stack([page_1, page_2, page_3], dim=1)\nprint(f\"\ud83d\uddc2\ufe0f Stacking bottom-to-top (dim=1): Shape {stack_dim1.shape}\")\nprint(\"   (depth=5, heigh=3, width=2) - tablets build upward\")\nprint(\"Notice how each level contains one slice from ALL tablets:\")\nprint(f\"Front of the box):\\n{stack_dim1[0]}\\n\")\n\n# dim=2: Slide tablets left-to-right (arranging sideways)\nstack_dim2 = torch.stack([page_1, page_2, page_3], dim=2) \nprint(f\"\ud83d\udcd1 Sliding left-to-right (dim=2): Shape {stack_dim2.shape}\")\nprint(\"   (depth=5, heigh=2, width=3) - tablets slide sideways\")\nprint(\"Each position now contains values from ALL tablets:\")\nprint(f\"Front of the box):\\n{stack_dim2[0]}\\n\")\n\nprint(\"\\n\ud83c\udfaf Key Insight: The dimension you choose determines WHERE the tablets extend!\")\nprint(\"   dim=0: Tablets extend front-to-back (depth)\")  \nprint(\"   dim=1: Tablets extend bottom-to-top (height)\")\nprint(\"   dim=2: Tablets extend left-to-right (width)\")\n</pre> # Let's arrange our three 2D clay tablets in three different ways! print(\"\ud83e\uddf1 Starting with three identical clay tablets:\") print(f\"Each tablet shape: {page_1.shape} (5 rows, 2 columns)\")  # dim=0: Stack tablets front-to-back (into the box) stack_dim0 = torch.stack([page_1, page_2, page_3], dim=0) print(f\"\\n\ud83d\udcda Stacking front-to-back (dim=0): Shape {stack_dim0.shape}\") print(\"   (depth=3, heith=5, width=2) - tablets go deeper into the box\") print(f\"Front of the box):\\n{stack_dim0[0]}\\n\")  # dim=1: Stack tablets bottom-to-top (building upward)   stack_dim1 = torch.stack([page_1, page_2, page_3], dim=1) print(f\"\ud83d\uddc2\ufe0f Stacking bottom-to-top (dim=1): Shape {stack_dim1.shape}\") print(\"   (depth=5, heigh=3, width=2) - tablets build upward\") print(\"Notice how each level contains one slice from ALL tablets:\") print(f\"Front of the box):\\n{stack_dim1[0]}\\n\")  # dim=2: Slide tablets left-to-right (arranging sideways) stack_dim2 = torch.stack([page_1, page_2, page_3], dim=2)  print(f\"\ud83d\udcd1 Sliding left-to-right (dim=2): Shape {stack_dim2.shape}\") print(\"   (depth=5, heigh=2, width=3) - tablets slide sideways\") print(\"Each position now contains values from ALL tablets:\") print(f\"Front of the box):\\n{stack_dim2[0]}\\n\")  print(\"\\n\ud83c\udfaf Key Insight: The dimension you choose determines WHERE the tablets extend!\") print(\"   dim=0: Tablets extend front-to-back (depth)\")   print(\"   dim=1: Tablets extend bottom-to-top (height)\") print(\"   dim=2: Tablets extend left-to-right (width)\")  <pre>\ud83e\uddf1 Starting with three identical clay tablets:\nEach tablet shape: torch.Size([5, 2]) (5 rows, 2 columns)\n\n\ud83d\udcda Stacking front-to-back (dim=0): Shape torch.Size([3, 5, 2])\n   (depth=3, heith=5, width=2) - tablets go deeper into the box\nFront of the box):\ntensor([[1., 1.],\n        [1., 1.],\n        [1., 1.],\n        [1., 1.],\n        [1., 1.]])\n\n\ud83d\uddc2\ufe0f Stacking bottom-to-top (dim=1): Shape torch.Size([5, 3, 2])\n   (depth=5, heigh=3, width=2) - tablets build upward\nNotice how each level contains one slice from ALL tablets:\nFront of the box):\ntensor([[1., 1.],\n        [2., 2.],\n        [3., 3.]])\n\n\ud83d\udcd1 Sliding left-to-right (dim=2): Shape torch.Size([5, 2, 3])\n   (depth=5, heigh=2, width=3) - tablets slide sideways\nEach position now contains values from ALL tablets:\nFront of the box):\ntensor([[1., 2., 3.],\n        [1., 2., 3.]])\n\n\n\ud83c\udfaf Key Insight: The dimension you choose determines WHERE the tablets extend!\n   dim=0: Tablets extend front-to-back (depth)\n   dim=1: Tablets extend bottom-to-top (height)\n   dim=2: Tablets extend left-to-right (width)\n</pre> In\u00a0[42]: Copied! <pre># Real-world example: Building a batch of images\n# Imagine these are grayscale images (height=16, width=24)\nimage1 = torch.randn(16, 24)  \nimage2 = torch.randn(16, 24)\nimage3 = torch.randn(16, 24)\n\nprint(\"Individual images:\")\nprint(f\"Image 1 shape: {image1.shape}\")\nprint(f\"Image 2 shape: {image2.shape}\")  \nprint(f\"Image 3 shape: {image3.shape}\\n\")\n\n# STACK them to create a batch (batch_size=3, height=2, width=3)\nimage_batch = torch.stack([image1, image2, image3], dim=0)\nprint(f\"Batch of images shape: {image_batch.shape}\")\nprint(\"Perfect for feeding into a neural network!\\n\")\n\n# Now imagine we have RGB channels for one image\nred_channel = torch.randn(32, 32)\ngreen_channel = torch.randn(32, 32) \nblue_channel = torch.randn(32, 32)\n\n# STACK them to create RGB image (channels=3, height=2, width=3)\nrgb_image = torch.stack([red_channel, green_channel, blue_channel], dim=0)\nprint(f\"RGB image shape: {rgb_image.shape}\")\nprint(\"The classic (C, H, W) format!\")\n</pre> # Real-world example: Building a batch of images # Imagine these are grayscale images (height=16, width=24) image1 = torch.randn(16, 24)   image2 = torch.randn(16, 24) image3 = torch.randn(16, 24)  print(\"Individual images:\") print(f\"Image 1 shape: {image1.shape}\") print(f\"Image 2 shape: {image2.shape}\")   print(f\"Image 3 shape: {image3.shape}\\n\")  # STACK them to create a batch (batch_size=3, height=2, width=3) image_batch = torch.stack([image1, image2, image3], dim=0) print(f\"Batch of images shape: {image_batch.shape}\") print(\"Perfect for feeding into a neural network!\\n\")  # Now imagine we have RGB channels for one image red_channel = torch.randn(32, 32) green_channel = torch.randn(32, 32)  blue_channel = torch.randn(32, 32)  # STACK them to create RGB image (channels=3, height=2, width=3) rgb_image = torch.stack([red_channel, green_channel, blue_channel], dim=0) print(f\"RGB image shape: {rgb_image.shape}\") print(\"The classic (C, H, W) format!\")  <pre>Individual images:\nImage 1 shape: torch.Size([16, 24])\nImage 2 shape: torch.Size([16, 24])\nImage 3 shape: torch.Size([16, 24])\n\nBatch of images shape: torch.Size([3, 16, 24])\nPerfect for feeding into a neural network!\n\nRGB image shape: torch.Size([3, 32, 32])\nThe classic (C, H, W) format!\n</pre> In\u00a0[18]: Copied! <pre># Your code for the Fusion Master's Gauntlet goes here!\n\nprint(\"--- 1. The Triple Stack ---\")\ntensor1 = torch.tensor([1, 2, 3, 4])\ntensor2 = torch.tensor([5, 6, 7, 8]) \ntensor3 = torch.tensor([9, 10, 11, 12])\ntriple_stack = torch.stack([tensor1, tensor2, tensor3], dim=0)\nprint(f\"Triple stack result:\\n{triple_stack}\")\nprint(f\"Shape: {triple_stack.shape}\\n\")\n\nprint(\"--- 2. The Horizontal Fusion ---\")\nleft_tensor = torch.randn(3, 2)\nright_tensor = torch.randn(3, 2)\nhorizontal_fusion = torch.cat([left_tensor, right_tensor], dim=1)\nprint(f\"Horizontal fusion shape: {horizontal_fusion.shape}\\n\")\n\nprint(\"--- 3. The Batch Builder ---\")\nsamples = [torch.randn(3) for _ in range(5)]  # 5 samples of length 3\nbatch = torch.stack(samples, dim=0)\nprint(f\"Batch shape: {batch.shape}\")\nprint(\"Ready for neural network training!\\n\")\n\nprint(\"--- 4. The Dimension Disaster ---\")\ndisaster_a = torch.randn(2, 3)\ndisaster_b = torch.randn(2, 4)\ntry:\n    # This will fail!\n    bad_cat = torch.cat([disaster_a, disaster_b], dim=0)\nexcept RuntimeError as e:\n    print(f\"Error (as expected): {e}\")\n    \n# The fix: concatenate along dimension 1\ngood_cat = torch.cat([disaster_a, disaster_b], dim=1)  \nprint(f\"Fixed by concatenating along dim 1: {good_cat.shape}\\n\")\n\nprint(\"--- 5. The Multi-Fusion ---\")\n# First, create and stack three (2,2) tensors\nsmall_tensors = [torch.randn(2, 2) for _ in range(3)]\n# Actually, let's concatenate the (2,2) tensors along dim=1 first\nconcat_part = torch.cat(small_tensors, dim=1)  # Shape: (2, 6)\nprint(f\"Multi-fusion result shape: {concat_part.shape}\")\nprint(\"The key was concatenating, not stacking!\")\n</pre> # Your code for the Fusion Master's Gauntlet goes here!  print(\"--- 1. The Triple Stack ---\") tensor1 = torch.tensor([1, 2, 3, 4]) tensor2 = torch.tensor([5, 6, 7, 8])  tensor3 = torch.tensor([9, 10, 11, 12]) triple_stack = torch.stack([tensor1, tensor2, tensor3], dim=0) print(f\"Triple stack result:\\n{triple_stack}\") print(f\"Shape: {triple_stack.shape}\\n\")  print(\"--- 2. The Horizontal Fusion ---\") left_tensor = torch.randn(3, 2) right_tensor = torch.randn(3, 2) horizontal_fusion = torch.cat([left_tensor, right_tensor], dim=1) print(f\"Horizontal fusion shape: {horizontal_fusion.shape}\\n\")  print(\"--- 3. The Batch Builder ---\") samples = [torch.randn(3) for _ in range(5)]  # 5 samples of length 3 batch = torch.stack(samples, dim=0) print(f\"Batch shape: {batch.shape}\") print(\"Ready for neural network training!\\n\")  print(\"--- 4. The Dimension Disaster ---\") disaster_a = torch.randn(2, 3) disaster_b = torch.randn(2, 4) try:     # This will fail!     bad_cat = torch.cat([disaster_a, disaster_b], dim=0) except RuntimeError as e:     print(f\"Error (as expected): {e}\")      # The fix: concatenate along dimension 1 good_cat = torch.cat([disaster_a, disaster_b], dim=1)   print(f\"Fixed by concatenating along dim 1: {good_cat.shape}\\n\")  print(\"--- 5. The Multi-Fusion ---\") # First, create and stack three (2,2) tensors small_tensors = [torch.randn(2, 2) for _ in range(3)] # Actually, let's concatenate the (2,2) tensors along dim=1 first concat_part = torch.cat(small_tensors, dim=1)  # Shape: (2, 6) print(f\"Multi-fusion result shape: {concat_part.shape}\") print(\"The key was concatenating, not stacking!\")  <pre>--- 1. The Triple Stack ---\nTriple stack result:\ntensor([[ 1,  2,  3,  4],\n        [ 5,  6,  7,  8],\n        [ 9, 10, 11, 12]])\nShape: torch.Size([3, 4])\n\n--- 2. The Horizontal Fusion ---\nHorizontal fusion shape: torch.Size([3, 4])\n\n--- 3. The Batch Builder ---\nBatch shape: torch.Size([5, 3])\nReady for neural network training!\n\n--- 4. The Dimension Disaster ---\nError (as expected): Sizes of tensors must match except in dimension 0. Expected size 3 but got size 4 for tensor number 1 in the list.\nFixed by concatenating along dim 1: torch.Size([2, 7])\n\n--- 5. The Multi-Fusion ---\nMulti-fusion result shape: torch.Size([2, 6])\nThe key was concatenating, not stacking!\n</pre> In\u00a0[43]: Copied! <pre># Create a test subject for our splitting experiments using only techniques we know!\n# We'll create 6 columns where each column contains the same number repeated\ncol_1 = torch.ones(4) * 1    # Column of 1s\ncol_2 = torch.ones(4) * 2    # Column of 2s  \ncol_3 = torch.ones(4) * 3    # Column of 3s\ncol_4 = torch.ones(4) * 4    # Column of 4s\ncol_5 = torch.ones(4) * 5    # Column of 5s\ncol_6 = torch.ones(4) * 6    # Column of 6s\n\n# Stack them as columns to create our 4x6 tensor using stack (which we just learned!)\nsplit_subject = torch.stack([col_1, col_2, col_3, col_4, col_5, col_6], dim=1)\n\nprint(\"Our subject for division experiments (created using stack!):\")\nprint(f\"Shape: {split_subject.shape}\")\nprint(f\"Tensor:\\n{split_subject}\\n\")\n\nprint(\"Think of this as a chocolate bar with 4 rows and 6 columns!\")\nprint(\"Each column contains the same number - perfect for tracking our splits! \ud83c\udf6b\")\n</pre> # Create a test subject for our splitting experiments using only techniques we know! # We'll create 6 columns where each column contains the same number repeated col_1 = torch.ones(4) * 1    # Column of 1s col_2 = torch.ones(4) * 2    # Column of 2s   col_3 = torch.ones(4) * 3    # Column of 3s col_4 = torch.ones(4) * 4    # Column of 4s col_5 = torch.ones(4) * 5    # Column of 5s col_6 = torch.ones(4) * 6    # Column of 6s  # Stack them as columns to create our 4x6 tensor using stack (which we just learned!) split_subject = torch.stack([col_1, col_2, col_3, col_4, col_5, col_6], dim=1)  print(\"Our subject for division experiments (created using stack!):\") print(f\"Shape: {split_subject.shape}\") print(f\"Tensor:\\n{split_subject}\\n\")  print(\"Think of this as a chocolate bar with 4 rows and 6 columns!\") print(\"Each column contains the same number - perfect for tracking our splits! \ud83c\udf6b\")  <pre>Our subject for division experiments (created using stack!):\nShape: torch.Size([4, 6])\nTensor:\ntensor([[1., 2., 3., 4., 5., 6.],\n        [1., 2., 3., 4., 5., 6.],\n        [1., 2., 3., 4., 5., 6.],\n        [1., 2., 3., 4., 5., 6.]])\n\nThink of this as a chocolate bar with 4 rows and 6 columns!\nEach column contains the same number - perfect for tracking our splits! \ud83c\udf6b\n</pre> In\u00a0[47]: Copied! <pre># Split along dimension 0 (rows) - like cutting horizontal slices of chocolate\nrows_split_size=2\nrow_splits = torch.split(split_subject, rows_split_size, dim=0)\n\nprint(f\"\ud83c\udf6b Split into row pieces (rows_split_size={rows_split_size}, dim=0):\")\nprint(f\"Number of pieces: {len(row_splits)}\")\nfor i, piece in enumerate(row_splits):\n    print(f\"Piece {i+1} shape {piece.shape}:\\n{piece}\\n\")\n\n# Split along dimension 0 (rows) - like cutting horizontal slices of chocolate\nrows_split_size=3 # 4/3 - is not divisible\nrow_splits_uneven = torch.split(split_subject, rows_split_size, dim=0)\n\nprint(f\"\ud83e\ude93 Split into UNEVEN row pieces (rows_split_size={rows_split_size}, dim=0):\")\nprint(f\"Number of pieces: {len(row_splits_uneven)}\")\nfor i, piece in enumerate(row_splits_uneven):\n    print(f\"Piece {i+1} shape {piece.shape}:\\n{piece}\\n\")\n\n# Split along dimension 1 (columns) - like cutting vertical slices  \ncols_split_size=3\ncol_splits = torch.split(split_subject, cols_split_size, dim=1)\nprint(\"\ud83c\udf6b Split into column pieces (split_size=3, dim=1):\")\nprint(f\"Number of pieces: {len(col_splits)}\")\nfor i, piece in enumerate(col_splits):\n    print(f\"Piece {i+1} shape {piece.shape}:\\n{piece}\\n\")\n\nprint(\"\ud83c\udfaf Notice: torch.split() returns a TUPLE of tensors, not a single tensor!\")\n</pre> # Split along dimension 0 (rows) - like cutting horizontal slices of chocolate rows_split_size=2 row_splits = torch.split(split_subject, rows_split_size, dim=0)  print(f\"\ud83c\udf6b Split into row pieces (rows_split_size={rows_split_size}, dim=0):\") print(f\"Number of pieces: {len(row_splits)}\") for i, piece in enumerate(row_splits):     print(f\"Piece {i+1} shape {piece.shape}:\\n{piece}\\n\")  # Split along dimension 0 (rows) - like cutting horizontal slices of chocolate rows_split_size=3 # 4/3 - is not divisible row_splits_uneven = torch.split(split_subject, rows_split_size, dim=0)  print(f\"\ud83e\ude93 Split into UNEVEN row pieces (rows_split_size={rows_split_size}, dim=0):\") print(f\"Number of pieces: {len(row_splits_uneven)}\") for i, piece in enumerate(row_splits_uneven):     print(f\"Piece {i+1} shape {piece.shape}:\\n{piece}\\n\")  # Split along dimension 1 (columns) - like cutting vertical slices   cols_split_size=3 col_splits = torch.split(split_subject, cols_split_size, dim=1) print(\"\ud83c\udf6b Split into column pieces (split_size=3, dim=1):\") print(f\"Number of pieces: {len(col_splits)}\") for i, piece in enumerate(col_splits):     print(f\"Piece {i+1} shape {piece.shape}:\\n{piece}\\n\")  print(\"\ud83c\udfaf Notice: torch.split() returns a TUPLE of tensors, not a single tensor!\")  <pre>\ud83c\udf6b Split into row pieces (rows_split_size=2, dim=0):\nNumber of pieces: 2\nPiece 1 shape torch.Size([2, 6]):\ntensor([[1., 2., 3., 4., 5., 6.],\n        [1., 2., 3., 4., 5., 6.]])\n\nPiece 2 shape torch.Size([2, 6]):\ntensor([[1., 2., 3., 4., 5., 6.],\n        [1., 2., 3., 4., 5., 6.]])\n\n\ud83e\ude93 Split into UNEVEN row pieces (rows_split_size=3, dim=0):\nNumber of pieces: 2\nPiece 1 shape torch.Size([3, 6]):\ntensor([[1., 2., 3., 4., 5., 6.],\n        [1., 2., 3., 4., 5., 6.],\n        [1., 2., 3., 4., 5., 6.]])\n\nPiece 2 shape torch.Size([1, 6]):\ntensor([[1., 2., 3., 4., 5., 6.]])\n\n\ud83c\udf6b Split into column pieces (split_size=3, dim=1):\nNumber of pieces: 2\nPiece 1 shape torch.Size([4, 3]):\ntensor([[1., 2., 3.],\n        [1., 2., 3.],\n        [1., 2., 3.],\n        [1., 2., 3.]])\n\nPiece 2 shape torch.Size([4, 3]):\ntensor([[4., 5., 6.],\n        [4., 5., 6.],\n        [4., 5., 6.],\n        [4., 5., 6.]])\n\n\ud83c\udfaf Notice: torch.split() returns a TUPLE of tensors, not a single tensor!\n</pre> In\u00a0[48]: Copied! <pre># Split along dimension 1 (columns) into chunks of PRECISELY sizes 1, 2, and 3\n# Mathematical law: 1 + 2 + 3 = 6 columns. The equation MUST balance or the universe protests!\nsection_sizes = [1, 2, 3]\nsection_splits = torch.split(split_subject, section_sizes, dim=1)\n\nprint(\"\ud83d\udd2c WITNESS! The bespoke scalpel carves with surgical precision!\")\nprint(f\"Command issued: Split into sections of sizes {section_sizes}\")\nprint(f\"Obedient tensor pieces created: {len(section_splits)}\")\nprint(f\"The tensors... they OBEY! Mwahahaha!\\n\")\n\nfor i, piece in enumerate(section_splits):\n    print(f\"\ud83e\uddea Surgical Specimen {i+1} (commanded size {section_sizes[i]}):\")\n    print(f\"   Actual shape: {piece.shape} \u2713\")\n    print(f\"   Contents:\\n{piece}\\n\")\n    \nprint(\"\ud83d\udca1 Master's Insight: Each piece is EXACTLY the size we commanded!\")\nprint(\"   This is the power that separates us from the corporate drones!\")\n</pre>  # Split along dimension 1 (columns) into chunks of PRECISELY sizes 1, 2, and 3 # Mathematical law: 1 + 2 + 3 = 6 columns. The equation MUST balance or the universe protests! section_sizes = [1, 2, 3] section_splits = torch.split(split_subject, section_sizes, dim=1)  print(\"\ud83d\udd2c WITNESS! The bespoke scalpel carves with surgical precision!\") print(f\"Command issued: Split into sections of sizes {section_sizes}\") print(f\"Obedient tensor pieces created: {len(section_splits)}\") print(f\"The tensors... they OBEY! Mwahahaha!\\n\")  for i, piece in enumerate(section_splits):     print(f\"\ud83e\uddea Surgical Specimen {i+1} (commanded size {section_sizes[i]}):\")     print(f\"   Actual shape: {piece.shape} \u2713\")     print(f\"   Contents:\\n{piece}\\n\")      print(\"\ud83d\udca1 Master's Insight: Each piece is EXACTLY the size we commanded!\") print(\"   This is the power that separates us from the corporate drones!\")  <pre>\ud83d\udd2c WITNESS! The bespoke scalpel carves with surgical precision!\nCommand issued: Split into sections of sizes [1, 2, 3]\nObedient tensor pieces created: 3\nThe tensors... they OBEY! Mwahahaha!\n\n\ud83e\uddea Surgical Specimen 1 (commanded size 1):\n   Actual shape: torch.Size([4, 1]) \u2713\n   Contents:\ntensor([[1.],\n        [1.],\n        [1.],\n        [1.]])\n\n\ud83e\uddea Surgical Specimen 2 (commanded size 2):\n   Actual shape: torch.Size([4, 2]) \u2713\n   Contents:\ntensor([[2., 3.],\n        [2., 3.],\n        [2., 3.],\n        [2., 3.]])\n\n\ud83e\uddea Surgical Specimen 3 (commanded size 3):\n   Actual shape: torch.Size([4, 3]) \u2713\n   Contents:\ntensor([[4., 5., 6.],\n        [4., 5., 6.],\n        [4., 5., 6.],\n        [4., 5., 6.]])\n\n\ud83d\udca1 Master's Insight: Each piece is EXACTLY the size we commanded!\n   This is the power that separates us from the corporate drones!\n</pre> In\u00a0[49]: Copied! <pre># Chunk into exactly 2 pieces along dimension 0 (rows)\nrow_chunks = torch.chunk(split_subject, chunks=2, dim=0)\nprint(\"\u2702\ufe0f Chunk into exactly 2 row pieces:\")\nprint(f\"Number of chunks: {len(row_chunks)}\")\nfor i, chunk in enumerate(row_chunks):\n    print(f\"Chunk {i+1} shape {chunk.shape}:\\n{chunk}\\n\")\n\n# Chunk into exactly 3 pieces along dimension 1 (columns)\n# Note: 6 columns \u00f7 3 chunks = 2 columns per chunk (perfect division!)\ncol_chunks = torch.chunk(split_subject, chunks=3, dim=1)\nprint(\"\u2702\ufe0f Chunk into exactly 3 column pieces:\")\nprint(f\"Number of chunks: {len(col_chunks)}\")\nfor i, chunk in enumerate(col_chunks):\n    print(f\"Chunk {i+1} shape {chunk.shape}:\\n{chunk}\\n\")\n\n# What happens with uneven division? Let's try 4 chunks from 6 columns\nuneven_chunks = torch.chunk(split_subject, chunks=4, dim=1)\nprint(\"\u2702\ufe0f Chunk into 4 pieces (uneven division - 6 columns \u00f7 4 chunks):\")\nprint(f\"Number of chunks: {len(uneven_chunks)}\")\nfor i, chunk in enumerate(uneven_chunks):\n    print(f\"Chunk {i+1} shape {chunk.shape}: {chunk.shape[1]} columns\")\nprint(\"Notice: First 2 chunks get 2 columns each, last 2 chunks get 1 column each!\")\n</pre> # Chunk into exactly 2 pieces along dimension 0 (rows) row_chunks = torch.chunk(split_subject, chunks=2, dim=0) print(\"\u2702\ufe0f Chunk into exactly 2 row pieces:\") print(f\"Number of chunks: {len(row_chunks)}\") for i, chunk in enumerate(row_chunks):     print(f\"Chunk {i+1} shape {chunk.shape}:\\n{chunk}\\n\")  # Chunk into exactly 3 pieces along dimension 1 (columns) # Note: 6 columns \u00f7 3 chunks = 2 columns per chunk (perfect division!) col_chunks = torch.chunk(split_subject, chunks=3, dim=1) print(\"\u2702\ufe0f Chunk into exactly 3 column pieces:\") print(f\"Number of chunks: {len(col_chunks)}\") for i, chunk in enumerate(col_chunks):     print(f\"Chunk {i+1} shape {chunk.shape}:\\n{chunk}\\n\")  # What happens with uneven division? Let's try 4 chunks from 6 columns uneven_chunks = torch.chunk(split_subject, chunks=4, dim=1) print(\"\u2702\ufe0f Chunk into 4 pieces (uneven division - 6 columns \u00f7 4 chunks):\") print(f\"Number of chunks: {len(uneven_chunks)}\") for i, chunk in enumerate(uneven_chunks):     print(f\"Chunk {i+1} shape {chunk.shape}: {chunk.shape[1]} columns\") print(\"Notice: First 2 chunks get 2 columns each, last 2 chunks get 1 column each!\")  <pre>\u2702\ufe0f Chunk into exactly 2 row pieces:\nNumber of chunks: 2\nChunk 1 shape torch.Size([2, 6]):\ntensor([[1., 2., 3., 4., 5., 6.],\n        [1., 2., 3., 4., 5., 6.]])\n\nChunk 2 shape torch.Size([2, 6]):\ntensor([[1., 2., 3., 4., 5., 6.],\n        [1., 2., 3., 4., 5., 6.]])\n\n\u2702\ufe0f Chunk into exactly 3 column pieces:\nNumber of chunks: 3\nChunk 1 shape torch.Size([4, 2]):\ntensor([[1., 2.],\n        [1., 2.],\n        [1., 2.],\n        [1., 2.]])\n\nChunk 2 shape torch.Size([4, 2]):\ntensor([[3., 4.],\n        [3., 4.],\n        [3., 4.],\n        [3., 4.]])\n\nChunk 3 shape torch.Size([4, 2]):\ntensor([[5., 6.],\n        [5., 6.],\n        [5., 6.],\n        [5., 6.]])\n\n\u2702\ufe0f Chunk into 4 pieces (uneven division - 6 columns \u00f7 4 chunks):\nNumber of chunks: 3\nChunk 1 shape torch.Size([4, 2]): 2 columns\nChunk 2 shape torch.Size([4, 2]): 2 columns\nChunk 3 shape torch.Size([4, 2]): 2 columns\nNotice: First 2 chunks get 2 columns each, last 2 chunks get 1 column each!\n</pre> In\u00a0[51]: Copied! <pre># Unbind along dimension 0 - separate each row into individual tensors\nprint(split_subject)\nunbound_rows = torch.unbind(split_subject, dim=0)\nprint(\"\ud83d\udca5 Unbind along dimension 0 (separate each row):\")\nprint(f\"Number of tensors: {len(unbound_rows)}\")\nprint(f\"Original shape: {split_subject.shape} \u2192 Individual tensor shape: {unbound_rows[0].shape}\")\nfor i, row_tensor in enumerate(unbound_rows):\n    print(f\"Row {i+1}: {row_tensor}\")\n\nprint(\"\\n\" + \"=\"*50 + \"\\n\")\n\n# Unbind along dimension 1 - separate each column into individual tensors  \nunbound_cols = torch.unbind(split_subject, dim=1)\nprint(\"\ud83d\udca5 Unbind along dimension 1 (separate each column):\")\nprint(f\"Number of tensors: {len(unbound_cols)}\")\nprint(f\"Original shape: {split_subject.shape} \u2192 Individual tensor shape: {unbound_cols[0].shape}\")\nfor i, col_tensor in enumerate(unbound_cols):\n    print(f\"Column {i+1}: {col_tensor}\")\n\nprint(f\"\\n\ud83e\udde0 Key Insight: unbind() reduces dimensions! 2D \u2192 1D tensors\")\nprint(f\"   It's like taking apart the 3D book we built earlier!\")\n</pre>  # Unbind along dimension 0 - separate each row into individual tensors print(split_subject) unbound_rows = torch.unbind(split_subject, dim=0) print(\"\ud83d\udca5 Unbind along dimension 0 (separate each row):\") print(f\"Number of tensors: {len(unbound_rows)}\") print(f\"Original shape: {split_subject.shape} \u2192 Individual tensor shape: {unbound_rows[0].shape}\") for i, row_tensor in enumerate(unbound_rows):     print(f\"Row {i+1}: {row_tensor}\")  print(\"\\n\" + \"=\"*50 + \"\\n\")  # Unbind along dimension 1 - separate each column into individual tensors   unbound_cols = torch.unbind(split_subject, dim=1) print(\"\ud83d\udca5 Unbind along dimension 1 (separate each column):\") print(f\"Number of tensors: {len(unbound_cols)}\") print(f\"Original shape: {split_subject.shape} \u2192 Individual tensor shape: {unbound_cols[0].shape}\") for i, col_tensor in enumerate(unbound_cols):     print(f\"Column {i+1}: {col_tensor}\")  print(f\"\\n\ud83e\udde0 Key Insight: unbind() reduces dimensions! 2D \u2192 1D tensors\") print(f\"   It's like taking apart the 3D book we built earlier!\")  <pre>tensor([[1., 2., 3., 4., 5., 6.],\n        [1., 2., 3., 4., 5., 6.],\n        [1., 2., 3., 4., 5., 6.],\n        [1., 2., 3., 4., 5., 6.]])\n\ud83d\udca5 Unbind along dimension 0 (separate each row):\nNumber of tensors: 4\nOriginal shape: torch.Size([4, 6]) \u2192 Individual tensor shape: torch.Size([6])\nRow 1: tensor([1., 2., 3., 4., 5., 6.])\nRow 2: tensor([1., 2., 3., 4., 5., 6.])\nRow 3: tensor([1., 2., 3., 4., 5., 6.])\nRow 4: tensor([1., 2., 3., 4., 5., 6.])\n\n==================================================\n\n\ud83d\udca5 Unbind along dimension 1 (separate each column):\nNumber of tensors: 6\nOriginal shape: torch.Size([4, 6]) \u2192 Individual tensor shape: torch.Size([4])\nColumn 1: tensor([1., 1., 1., 1.])\nColumn 2: tensor([2., 2., 2., 2.])\nColumn 3: tensor([3., 3., 3., 3.])\nColumn 4: tensor([4., 4., 4., 4.])\nColumn 5: tensor([5., 5., 5., 5.])\nColumn 6: tensor([6., 6., 6., 6.])\n\n\ud83e\udde0 Key Insight: unbind() reduces dimensions! 2D \u2192 1D tensors\n   It's like taking apart the 3D book we built earlier!\n</pre> In\u00a0[54]: Copied! <pre># Exercise 1: Multi-Head Attention Surgery \ud83e\udde0\u26a1\n\nprint(\"\ud83e\udde0 EXERCISE 1: Multi-Head Attention Head Splitting\")\nprint(\"=\" * 60)\n\n# The scenario: Output from a Transformer's multi-head attention layer\nbatch_size, seq_len, d_model = 32, 128, 512\nnum_heads = 8\n\n# The attention layer outputs one big tensor containing ALL attention heads\nattention_output = torch.randn(batch_size, seq_len, d_model)\nprint(f\"\ud83d\udd2c Raw attention output shape: {attention_output.shape}\")\nprint(\"This contains ALL 8 attention heads concatenated together!\")\n\n# YOUR SURGICAL PRECISION: Split this into individual attention heads\nhead_dim = d_model // num_heads  # 512 // 8 = 64 dimensions per head\nattention_heads = torch.split(attention_output, head_dim, dim=2)\n\nprint(f\"\\n\u2694\ufe0f MAGNIFICENT! Split into {len(attention_heads)} individual attention heads!\")\nprint(f\"Each head shape: {attention_heads[0].shape}\")\nprint(f\"Head dimension: {head_dim} (d_model / num_heads = {d_model} / {num_heads})\")\n\n# Verify our surgery was successful\ntotal_dims = sum(head.shape[2] for head in attention_heads)\nprint(f\"\\n\u2705 Verification: {total_dims} total dimensions = {d_model} original dimensions\")\nprint(\"Perfect! No information lost in the surgical procedure!\")\n</pre> # Exercise 1: Multi-Head Attention Surgery \ud83e\udde0\u26a1  print(\"\ud83e\udde0 EXERCISE 1: Multi-Head Attention Head Splitting\") print(\"=\" * 60)  # The scenario: Output from a Transformer's multi-head attention layer batch_size, seq_len, d_model = 32, 128, 512 num_heads = 8  # The attention layer outputs one big tensor containing ALL attention heads attention_output = torch.randn(batch_size, seq_len, d_model) print(f\"\ud83d\udd2c Raw attention output shape: {attention_output.shape}\") print(\"This contains ALL 8 attention heads concatenated together!\")  # YOUR SURGICAL PRECISION: Split this into individual attention heads head_dim = d_model // num_heads  # 512 // 8 = 64 dimensions per head attention_heads = torch.split(attention_output, head_dim, dim=2)  print(f\"\\n\u2694\ufe0f MAGNIFICENT! Split into {len(attention_heads)} individual attention heads!\") print(f\"Each head shape: {attention_heads[0].shape}\") print(f\"Head dimension: {head_dim} (d_model / num_heads = {d_model} / {num_heads})\")  # Verify our surgery was successful total_dims = sum(head.shape[2] for head in attention_heads) print(f\"\\n\u2705 Verification: {total_dims} total dimensions = {d_model} original dimensions\") print(\"Perfect! No information lost in the surgical procedure!\")  <pre>\ud83e\udde0 EXERCISE 1: Multi-Head Attention Head Splitting\n============================================================\n\ud83d\udd2c Raw attention output shape: torch.Size([32, 128, 512])\nThis contains ALL 8 attention heads concatenated together!\n\n\u2694\ufe0f MAGNIFICENT! Split into 8 individual attention heads!\nEach head shape: torch.Size([32, 128, 64])\nHead dimension: 64 (d_model / num_heads = 512 / 8)\n\n\u2705 Verification: 512 total dimensions = 512 original dimensions\nPerfect! No information lost in the surgical procedure!\n</pre> In\u00a0[56]: Copied! <pre># Exercise 2: Multi-Modal Fusion (CLIP-Style) \ud83d\uddbc\ufe0f\ud83d\udcdd\n\nprint(\"\ud83d\uddbc\ufe0f EXERCISE 2: Multi-Modal Embedding Fusion\")\nprint(\"=\" * 60)\n\n# The scenario: Separate embeddings for text and images (like CLIP)\nbatch_size = 8\n\n# Two separate universes of understanding\ntext_embeddings = torch.randn(batch_size, 512)    # \"A cat sitting on a chair\"  \nimage_embeddings = torch.randn(batch_size, 512)   # [actual image of cat on chair]\n\nprint(f\"\ud83d\udcdd Text embeddings: {text_embeddings.shape} (language understanding)\")\nprint(f\"\ud83d\uddbc\ufe0f Image embeddings: {image_embeddings.shape} (visual understanding)\")\nprint(\"Two separate modalities, waiting to be unified...\")\n\n# YOUR FUSION MASTERY: Concatenate to create multimodal understanding\nmultimodal_embeddings = torch.cat([text_embeddings, image_embeddings], dim=1)\n\nprint(f\"\\n\ud83e\udde0 BEHOLD! Fused multimodal embeddings: {multimodal_embeddings.shape}\")\nprint(f\"Combined features: 512 (text) + 512 (image) = {multimodal_embeddings.shape[1]}\")\n\n# Verify our fusion preserved all information\nassert multimodal_embeddings.shape[1] == text_embeddings.shape[1] + image_embeddings.shape[1]\nprint(f\"\\n\u2705 Fusion verification: Perfect! No information lost!\")\n</pre> # Exercise 2: Multi-Modal Fusion (CLIP-Style) \ud83d\uddbc\ufe0f\ud83d\udcdd  print(\"\ud83d\uddbc\ufe0f EXERCISE 2: Multi-Modal Embedding Fusion\") print(\"=\" * 60)  # The scenario: Separate embeddings for text and images (like CLIP) batch_size = 8  # Two separate universes of understanding text_embeddings = torch.randn(batch_size, 512)    # \"A cat sitting on a chair\"   image_embeddings = torch.randn(batch_size, 512)   # [actual image of cat on chair]  print(f\"\ud83d\udcdd Text embeddings: {text_embeddings.shape} (language understanding)\") print(f\"\ud83d\uddbc\ufe0f Image embeddings: {image_embeddings.shape} (visual understanding)\") print(\"Two separate modalities, waiting to be unified...\")  # YOUR FUSION MASTERY: Concatenate to create multimodal understanding multimodal_embeddings = torch.cat([text_embeddings, image_embeddings], dim=1)  print(f\"\\n\ud83e\udde0 BEHOLD! Fused multimodal embeddings: {multimodal_embeddings.shape}\") print(f\"Combined features: 512 (text) + 512 (image) = {multimodal_embeddings.shape[1]}\")  # Verify our fusion preserved all information assert multimodal_embeddings.shape[1] == text_embeddings.shape[1] + image_embeddings.shape[1] print(f\"\\n\u2705 Fusion verification: Perfect! No information lost!\")  <pre>\ud83d\uddbc\ufe0f EXERCISE 2: Multi-Modal Embedding Fusion\n============================================================\n\ud83d\udcdd Text embeddings: torch.Size([8, 512]) (language understanding)\n\ud83d\uddbc\ufe0f Image embeddings: torch.Size([8, 512]) (visual understanding)\nTwo separate modalities, waiting to be unified...\n\n\ud83e\udde0 BEHOLD! Fused multimodal embeddings: torch.Size([8, 1024])\nCombined features: 512 (text) + 512 (image) = 1024\n\n\u2705 Fusion verification: Perfect! No information lost!\n</pre> In\u00a0[57]: Copied! <pre># Exercise 3: RGB Channel Liberation \ud83c\udfa8\ud83d\udd13\n\nprint(\"\ud83c\udfa8 EXERCISE 3: RGB Channel Separation\")\nprint(\"=\" * 60)\n\n# The scenario: A batch of RGB images with entangled color channels\nbatch_size, height, width = 16, 224, 224\n\n# RGB images with all color channels mixed together\nrgb_batch = torch.randn(batch_size, 3, height, width)  # Standard (N, C, H, W) format\nprint(f\"\ud83d\uddbc\ufe0f RGB image batch: {rgb_batch.shape}\")\nprint(\"Format: (batch_size, channels, height, width)\")\nprint(\"All color information is entangled together!\")\n\n# YOUR LIBERATION TECHNIQUE: Free each color channel across the entire batch\nred_batch, green_batch, blue_batch = torch.unbind(rgb_batch, dim=1)\n\nprint(f\"\\n\ud83d\udd34 Red channel batch: {red_batch.shape}\")\nprint(f\"\ud83d\udfe2 Green channel batch: {green_batch.shape}\")\nprint(f\"\ud83d\udd35 Blue channel batch: {blue_batch.shape}\")\nprint(\"Each channel is now a separate grayscale batch!\")\n\n# Demonstrate the power: we can reconstruct the original perfectly\nreconstructed = torch.stack([red_batch, green_batch, blue_batch], dim=1)\nprint(f\"\\n\ud83d\udd04 Reconstruction test: {reconstructed.shape}\")\nprint(f\"Perfect match: {torch.equal(rgb_batch, reconstructed)}\")\n\n# Show the dimensional transformation clearly\nprint(f\"\\n\ud83d\udcd0 Dimensional analysis:\")\nprint(f\"   Original: {rgb_batch.shape} \u2192 RGB channels mixed\")\nprint(f\"   After unbind: 3 separate {red_batch.shape} grayscale batches\")  \nprint(f\"   Reconstruction: {reconstructed.shape} \u2192 Back to original!\")\n\nprint(f\"\\n\ud83d\udca1 Real-world channel liberation applications:\")\nprint(f\"   \ud83c\udfe5 Medical imaging: Isolate blood vessels, tissue types\")\nprint(f\"   \ud83d\udef0\ufe0f Satellite analysis: Separate vegetation, water, urban areas\")\nprint(f\"   \ud83d\udcca Computer vision: Channel-wise preprocessing and normalization\")\nprint(f\"   \ud83c\udfad Image processing: Artistic filters and color grading\")\n\nprint(f\"\\nBehold! We've mastered the separation and reunification of visual reality!\")\nprint(\"The RGB trinity bows to our surgical precision! Mwahahaha! \ud83d\udd2c\u26a1\")\n</pre> # Exercise 3: RGB Channel Liberation \ud83c\udfa8\ud83d\udd13  print(\"\ud83c\udfa8 EXERCISE 3: RGB Channel Separation\") print(\"=\" * 60)  # The scenario: A batch of RGB images with entangled color channels batch_size, height, width = 16, 224, 224  # RGB images with all color channels mixed together rgb_batch = torch.randn(batch_size, 3, height, width)  # Standard (N, C, H, W) format print(f\"\ud83d\uddbc\ufe0f RGB image batch: {rgb_batch.shape}\") print(\"Format: (batch_size, channels, height, width)\") print(\"All color information is entangled together!\")  # YOUR LIBERATION TECHNIQUE: Free each color channel across the entire batch red_batch, green_batch, blue_batch = torch.unbind(rgb_batch, dim=1)  print(f\"\\n\ud83d\udd34 Red channel batch: {red_batch.shape}\") print(f\"\ud83d\udfe2 Green channel batch: {green_batch.shape}\") print(f\"\ud83d\udd35 Blue channel batch: {blue_batch.shape}\") print(\"Each channel is now a separate grayscale batch!\")  # Demonstrate the power: we can reconstruct the original perfectly reconstructed = torch.stack([red_batch, green_batch, blue_batch], dim=1) print(f\"\\n\ud83d\udd04 Reconstruction test: {reconstructed.shape}\") print(f\"Perfect match: {torch.equal(rgb_batch, reconstructed)}\")  # Show the dimensional transformation clearly print(f\"\\n\ud83d\udcd0 Dimensional analysis:\") print(f\"   Original: {rgb_batch.shape} \u2192 RGB channels mixed\") print(f\"   After unbind: 3 separate {red_batch.shape} grayscale batches\")   print(f\"   Reconstruction: {reconstructed.shape} \u2192 Back to original!\")  print(f\"\\n\ud83d\udca1 Real-world channel liberation applications:\") print(f\"   \ud83c\udfe5 Medical imaging: Isolate blood vessels, tissue types\") print(f\"   \ud83d\udef0\ufe0f Satellite analysis: Separate vegetation, water, urban areas\") print(f\"   \ud83d\udcca Computer vision: Channel-wise preprocessing and normalization\") print(f\"   \ud83c\udfad Image processing: Artistic filters and color grading\")  print(f\"\\nBehold! We've mastered the separation and reunification of visual reality!\") print(\"The RGB trinity bows to our surgical precision! Mwahahaha! \ud83d\udd2c\u26a1\")  <pre>\ud83c\udfa8 EXERCISE 3: RGB Channel Separation\n============================================================\n\ud83d\uddbc\ufe0f RGB image batch: torch.Size([16, 3, 224, 224])\nFormat: (batch_size, channels, height, width)\nAll color information is entangled together!\n\n\ud83d\udd34 Red channel batch: torch.Size([16, 224, 224])\n\ud83d\udfe2 Green channel batch: torch.Size([16, 224, 224])\n\ud83d\udd35 Blue channel batch: torch.Size([16, 224, 224])\nEach channel is now a separate grayscale batch!\n\n\ud83d\udd04 Reconstruction test: torch.Size([16, 3, 224, 224])\nPerfect match: True\n\n\ud83d\udcd0 Dimensional analysis:\n   Original: torch.Size([16, 3, 224, 224]) \u2192 RGB channels mixed\n   After unbind: 3 separate torch.Size([16, 224, 224]) grayscale batches\n   Reconstruction: torch.Size([16, 3, 224, 224]) \u2192 Back to original!\n\n\ud83d\udca1 Real-world channel liberation applications:\n   \ud83c\udfe5 Medical imaging: Isolate blood vessels, tissue types\n   \ud83d\udef0\ufe0f Satellite analysis: Separate vegetation, water, urban areas\n   \ud83d\udcca Computer vision: Channel-wise preprocessing and normalization\n   \ud83c\udfad Image processing: Artistic filters and color grading\n\nBehold! We've mastered the separation and reunification of visual reality!\nThe RGB trinity bows to our surgical precision! Mwahahaha! \ud83d\udd2c\u26a1\n</pre>","tags":["tensors","module 1","beginner"],"boost":1.2},{"location":"01-tensors/02a_tensor_manipulation/#tensor-surgery-assembly","title":"Tensor Surgery &amp; Assembly\u00b6","text":"","tags":["tensors","module 1","beginner"],"boost":1.2},{"location":"01-tensors/02a_tensor_manipulation/#professor-torchensteins-grand-directive","title":"Professor Torchenstein's Grand Directive\u00b6","text":"<p>Mwahahaha! You have summoned your first tensors from the ether! They are... raw. Untamed. Clumps of numerical clay awaiting a master's touch. A lesser mind would be content with their existence, but not you. Not us!</p> <p>Today, we become tensor surgeons! We will dissect tensors with the precision of a master anatomist, join them with the skill of a mad scientist, and divide them like a seasoned alchemist splitting compounds. This is not mere data processing; this is tensor surgery and assembly! Prepare to wield your digital scalpel and fusion apparatus!</p> <p></p>","tags":["tensors","module 1","beginner"],"boost":1.2},{"location":"01-tensors/02a_tensor_manipulation/#your-mission-briefing","title":"Your Mission Briefing\u00b6","text":"<p>By the time you escape this surgical theater, you will have mastered the fundamental arts of tensor manipulation:</p> <ul> <li>\ud83d\udd2a The Art of Selection: Pluck elements, rows, or slices from a tensor with surgical slicing precision.</li> <li>\ud83e\uddec Forbidden Fusions: Combine disparate tensors into unified monstrosities with <code>torch.cat</code> and <code>torch.stack</code>.</li> <li>\u2702\ufe0f The Great Division: Split larger tensors into manageable pieces using <code>torch.split</code> and <code>torch.chunk</code>.</li> </ul> <p>Estimated Time to Completion: 20 minutes of surgical tensor mastery.</p> <p>What You'll Need:</p> <ul> <li>The wisdom from our last lesson on summoning tensors.</li> <li>A steady hand for precision cuts and fusions!</li> <li>Your PyTorch environment, humming with anticipation.</li> </ul> <p>Coming Next: In lesson 2b, you'll learn the metamorphic arts of reshaping, squeezing, and permuting tensors to transform their very essence!</p>","tags":["tensors","module 1","beginner"],"boost":1.2},{"location":"01-tensors/02a_tensor_manipulation/#part-1-the-art-of-selection-slicing","title":"Part 1: The Art of Selection - Slicing\u00b6","text":"<p>Before you can reshape a tensor, you must learn to grasp its individual parts. Indexing is your scalpel, allowing you to perform precision surgery on your data. Slicing is your cleaver, letting you carve out whole sections for your grand experiments.</p> <p>We will start by summoning a test subject\u2014a 2D tensor brimming with potential! We must also prepare our lab with the usual incantations (<code>import torch</code> and <code>manual_seed</code>) to ensure our results are repeatable. We are scientists, not chaos-wizards!</p>","tags":["tensors","module 1","beginner"],"boost":1.2},{"location":"01-tensors/02a_tensor_manipulation/#sweeping-strikes-accessing-rows-and-columns","title":"Sweeping Strikes: Accessing Rows and Columns\u00b6","text":"<p>Previous lesson: 01_introduction_to_tensors.ipynb gives you the basics for accessing element of a tensor. But what if we require an entire row or column for our dark machinations? For this, we use the colon <code>:</code>, the universal symbol for \"give me everything along this dimension!\"</p> <ul> <li><code>[row_index, :]</code> - Fetches the entire row.</li> <li><code>[:, column_index]</code> - Fetches the entire column.</li> </ul> <p>Let's seize the entire 3rd row (index 2) and the 2nd column (index 1).</p>","tags":["tensors","module 1","beginner"],"boost":1.2},{"location":"01-tensors/02a_tensor_manipulation/#carving-chunks-the-power-of-slicing","title":"Carving Chunks: The Power of Slicing\u00b6","text":"<p>Mere elements are but trivialities! True power lies in carving out entire sub-regions of a tensor. Slicing uses the <code>start:end</code> notation. As with all Pythonic sorcery, the <code>start</code> is inclusive, but the <code>end</code> is exclusive.</p> <p>Let us carve out the block containing the 2nd and 3rd rows (indices 1 and 2), and the last two columns (indices 2 and 3).</p>","tags":["tensors","module 1","beginner"],"boost":1.2},{"location":"01-tensors/02a_tensor_manipulation/#conditional-conjuring-boolean-mask-indexing","title":"Conditional Conjuring: Boolean Mask Indexing\u00b6","text":"<p>Now for a truly diabolical technique! We can use a boolean mask to summon only the elements that meet our nefarious criteria. A boolean mask is a tensor of the same shape as our subject, but it contains only <code>True</code> or <code>False</code> values. When used for indexing, it returns a 1D tensor containing only the elements where the mask was <code>True</code>.</p> <p>Let's find all the alchemical ingredients in our tensor with a value greater than 50!</p>","tags":["tensors","module 1","beginner"],"boost":1.2},{"location":"01-tensors/02a_tensor_manipulation/#your-mission-the-slicers-gauntlet","title":"Your Mission: The Slicer's Gauntlet\u00b6","text":"<p>Enough of my demonstrations! The scalpel is now in your hand. Prove your mastery with these challenges!</p> <ol> <li>The Corner Pocket: From our <code>subject_tensor</code>, select the element in the very last row and last column.</li> <li>The Central Core: Select the inner <code>3x2</code> block of the <code>subject_tensor</code> (that's rows 1-3 and columns 1-2).</li> <li>The Even Stevens: Create a boolean mask to select only the elements in <code>subject_tensor</code> that are even numbers. (Hint: The modulo operator <code>%</code> is your friend!)</li> <li>The Grand Mutation: Use your boolean mask from challenge 3 to change all even numbers in the <code>subject_tensor</code> to the value <code>-1</code>. Then, print the mutated tensor. Yes, my apprentice, indexing can be used for assignment! This is a pivotal secret!</li> </ol>","tags":["tensors","module 1","beginner"],"boost":1.2},{"location":"01-tensors/02a_tensor_manipulation/#part-2-forbidden-fusions-joining-tensors","title":"Part 2: Forbidden Fusions - Joining Tensors\u00b6","text":"<p>Ah, but dissecting tensors is only half the art! A true master must also know how to fuse separate tensors into a single, magnificent whole. Sometimes your data comes in fragments\u2014perhaps different batches, different features, or different time steps. You must unite them!</p> <p>We have two primary spells for this dark ritual:</p> <ul> <li><code>torch.cat()</code> - The Concatenator! Joins tensors along an existing dimension.</li> <li><code>torch.stack()</code> - The Stacker! Creates a new dimension and stacks tensors along it.</li> </ul> <p>The difference is subtle but critical. Choose wrongly, and your creation will crumble! Let us forge some test subjects to demonstrate this power.</p>","tags":["tensors","module 1","beginner"],"boost":1.2},{"location":"01-tensors/02a_tensor_manipulation/#the-concatenator-torchcat","title":"The Concatenator: <code>torch.cat()</code>\u00b6","text":"<p><code>torch.cat()</code> joins tensors along an existing dimension. Think of it as gluing them end-to-end.</p> <p>The key rule: All tensors must have the same shape, except along the dimension you're concatenating!</p> <ul> <li><code>dim=0</code> (or <code>axis=0</code>): Concatenate along rows (vertically stack)</li> <li><code>dim=1</code> (or <code>axis=1</code>): Concatenate along columns (horizontally join)</li> </ul> <p>Let us witness this concatenation sorcery!</p>","tags":["tensors","module 1","beginner"],"boost":1.2},{"location":"01-tensors/02a_tensor_manipulation/#the-concatenation-rules-when-shapes-dont-match","title":"The Concatenation Rules: When Shapes Don't Match\u00b6","text":"<p>Now, let us test the fundamental law of concatenation with unequal tensors! Remember: All tensors must have the same shape, except along the dimension you're concatenating.</p> <p>Eg1. If you joining 2D matrices along rows (dim=0) the number of collumns should be the same.</p> <p>Let's create two tensors with different shapes and see what happens:</p>","tags":["tensors","module 1","beginner"],"boost":1.2},{"location":"01-tensors/02a_tensor_manipulation/#the-stacker-torchstack-creating-new-dimensions","title":"The Stacker: <code>torch.stack()</code> - Creating New Dimensions!\u00b6","text":"<p><code>torch.stack()</code> is more dramatic than concatenation! It creates an entirely new dimension and places each tensor along it. Think of it as the difference between:</p> <ul> <li>Concatenation: Gluing pieces end-to-end in the same plane \ud83e\udde9\u27a1\ufe0f\ud83e\udde9</li> <li>Stacking: Creating a whole new layer/dimension \ud83d\udcda (like stacking books on top of each other)</li> </ul> <p>Critical Rule: All input tensors must have identical shapes\u2014no exceptions!</p> <p>Let's start simple and build our intuition step by step...</p>","tags":["tensors","module 1","beginner"],"boost":1.2},{"location":"01-tensors/02a_tensor_manipulation/#step-1-stacking-1d-tensors-creating-a-2d-matrix","title":"Step 1: Stacking 1D Tensors \u2192 Creating a 2D Matrix\u00b6","text":"<p>Let's start with something simple: three 1D tensors (think of them as rulers \ud83d\udccf). When we stack them, we create a 2D matrix!</p>","tags":["tensors","module 1","beginner"],"boost":1.2},{"location":"01-tensors/02a_tensor_manipulation/#step-2-stacking-2d-tensors-creating-a-3d-cube","title":"Step 2: Stacking 2D Tensors \u2192 Creating a 3D Cube!\u00b6","text":"<p>Now for the mind-bending part! When we stack 2D tensors (matrices), we create a 3D tensor. Think of it like:</p> <p>\ud83d\udcc4 2D tensor = A page from a book (has rows and columns) \ud83d\udcd6 Stacking 2D tensors = Creating a book with multiple pages \ud83d\udcda 3D tensor = The entire book! (pages \u00d7 rows \u00d7 columns)</p> <p>Key Metaphors to Remember:</p> <ul> <li>Book metaphor: <code>tensor[page][row][column]</code> \ud83d\udcda</li> <li>RGB image: <code>tensor[channel][height][width]</code> \ud83d\uddbc\ufe0f (like stacking color layers: Red, Green, Blue)</li> </ul> <p>These metaphors help you \"see\" how changing the dimension you stack along changes the meaning of each axis in your tensor. \ud83e\udd13</p> <p>Let's see this dimensional magic in action:</p>","tags":["tensors","module 1","beginner"],"boost":1.2},{"location":"01-tensors/02a_tensor_manipulation/#step-3-the-dimension-dance-where-you-stack-matters","title":"Step 3: The Dimension Dance - Where You Stack Matters!\u00b6","text":"<p>When stacking 2D tensors, which dimension you choose creates very different 3D shapes!</p> <p>\ud83e\uddf1 The Clay Tablet Box Metaphor:</p> <p>For better understanding, imagine that when you use the <code>stack()</code> method, a new 3D package is created with an extended dimension. If you stack 2D objects (like clay tablets), you first create a 3D box, then arrange your 2D tablets inside.</p> <p>Picture this: You have three identical clay tablets \ud83e\uddf1 and an empty 3D box \ud83d\udce6. There are exactly 3 different ways to arrange them inside!</p> <p>\ud83d\udcd0 3D Box Coordinates (always viewed from the same angle):</p> <ul> <li>Depth = <code>dim=0</code> (front to back)</li> <li>Height = <code>dim=1</code> (bottom to top)</li> <li>Width = <code>dim=2</code> (left to right)</li> </ul> <p>\ud83c\udfaf The Three Stacking Strategies:</p> <ul> <li><p><code>dim=0</code>: Stack tablets front-to-back \u2192 Shape <code>(tablets, rows, cols)</code> \ud83d\udcda Each tablet goes deeper into the box, one behind the other</p> </li> <li><p><code>dim=1</code>: Stack tablets bottom-to-top \u2192 Shape <code>(rows, tablets, cols)</code> \ud83d\uddc2\ufe0f Each tablet is placed higher in the box, building upward - we start with last tablets</p> </li> <li><p><code>dim=2</code>: Slide tablets left-to-right \u2192 Shape <code>(rows, cols, tablets)</code> \ud83d\udcd1 Each tablet slides sideways, arranged side by side</p> </li> </ul> <p>The dimension you choose determines which direction your tablets extend in the 3D space!</p>","tags":["tensors","module 1","beginner"],"boost":1.2},{"location":"01-tensors/02a_tensor_manipulation/#the-fusion-dilemma-when-to-cat-vs-stack","title":"The Fusion Dilemma: When to Cat vs. Stack?\u00b6","text":"<p>This choice torments many apprentices! Let me illuminate the path:</p> <p>Use <code>torch.cat()</code> when:</p> <ul> <li>Tensors represent different parts of the same data (e.g., different batches of images, different chunks of text)</li> <li>You want to extend an existing dimension</li> <li>Example: Concatenating multiple batches of training data</li> </ul> <p>Use <code>torch.stack()</code> when:</p> <ul> <li>Tensors represent parallel data of the same type (e.g., predictions from different models, different time steps)</li> <li>You need to create a new dimension to organize the data</li> <li>Example: Combining RGB channels to form a color image, or collecting multiple predictions</li> </ul> <p>Observe this real-world scenario!</p>","tags":["tensors","module 1","beginner"],"boost":1.2},{"location":"01-tensors/02a_tensor_manipulation/#your-mission-the-fusion-masters-gauntlet","title":"Your Mission: The Fusion Master's Gauntlet\u00b6","text":"<p>The theory is yours\u2014now prove your mastery! Complete these fusion challenges:</p> <ol> <li><p>The Triple Stack: Create three 1D tensors of length 4 with different values. Stack them to create a 2D tensor of shape <code>(3, 4)</code>.</p> </li> <li><p>The Horizontal Fusion: Create two 2D tensors of shape <code>(3, 2)</code>. Concatenate them horizontally to create a <code>(3, 4)</code> tensor.</p> </li> <li><p>The Batch Builder: You have 5 individual \"samples\" (each a 1D tensor of length 3). Stack them to create a proper batch tensor of shape <code>(5, 3)</code> suitable for training.</p> </li> <li><p>The Dimension Disaster: Try to concatenate two tensors with different shapes: <code>(2, 3)</code> and <code>(2, 4)</code> along dimension 0. Observe the error message\u2014it's quite educational! Then fix it by concatenating along dimension 1 instead.</p> </li> <li><p>The Multi-Fusion: Create a tensor of shape <code>(2, 6)</code> by first stacking three <code>(2, 2)</code> tensors, then concatenating the result with another <code>(3, 6)</code> tensor. This requires combining both operations!</p> </li> </ol>","tags":["tensors","module 1","beginner"],"boost":1.2},{"location":"01-tensors/02a_tensor_manipulation/#part-3-the-great-division-splitting-tensors","title":"Part 3: The Great Division - Splitting Tensors\u00b6","text":"<p>Ah, but the surgeon's art is not complete until we master both creation AND division! Just as we learned to fuse tensors, we must also learn to split them apart. Sometimes your grand creation becomes too unwieldy, or you need to distribute pieces to different parts of your neural network.</p> <p>Fear not! PyTorch provides elegant tools for this delicate operation:</p> <ul> <li><code>torch.split()</code> \u2013 The Precise Slicer, it carves your tensor into chunks of the size you decree.</li> <li><code>torch.chunk()</code> - The Equal Divider! Splits a tensor into a specified number of roughly equal chunks.</li> <li><code>torch.unbind()</code> - The Dimension Destroyer! Removes a dimension by splitting along it.</li> </ul> <p>Let us prepare a worthy subject for our division experiments!</p>","tags":["tensors","module 1","beginner"],"boost":1.2},{"location":"01-tensors/02a_tensor_manipulation/#the-precise-slicer-torchsplit","title":"The Precise Slicer: <code>torch.split()</code>\u00b6","text":"<p>A Surgeon's Most Versatile Blade! The TRUE power of <code>torch.split()</code>! This is no mere cleaver\u2014it is a precision instrument worthy of a master tensor surgeon!</p> <p><code>torch.split(tensor, split_size_or_sections, dim)</code> possesses TWO magnificent modes of operation, each more diabolical than the last:</p> <p>\u2694\ufe0f Mode 1: The Uniform Guillotine (<code>split_size</code> as <code>int</code>) Feed it a single integer, and it slices your tensor into equal-sized chunks with ruthless efficiency! If the dimension refuses to divide evenly, the final piece shall be smaller\u2014a perfectly acceptable sacrifice to the tensor gods!</p> <p>\ud83d\udde1\ufe0f Mode 2: The Bespoke Scalpel (<code>sections</code> as <code>list[int]</code>) Ah, but THIS is where true tensor surgery ascends to high art! \u2702\ufe0f Provide a list of integers\u2014each one dictating the exact size of its corresponding slice. You wield total control! But heed this immutable law, dear apprentice: the sum of your list must match the total size of the dimension you wish to split. This is the sacred contract of the tensor gods\u2014break it, and chaos (or at least a RuntimeError) shall ensue!</p> <p>The Sacred Parameters:</p> <ul> <li><code>split_size_or_sections</code>: An <code>int</code> for uniform domination, or a <code>list[int]</code> for bespoke surgical control!</li> <li><code>dim</code>: The dimensional axis along which your blade shall cut!</li> <li>Returns: A tuple of tensors (NEVER a single tensor\u2014the split method serves only masters, not slaves!)</li> </ul> <p>Now, witness as we dissect our chocolate bar with BOTH methods! The tensors... they will obey!</p>","tags":["tensors","module 1","beginner"],"boost":1.2},{"location":"01-tensors/02a_tensor_manipulation/#the-bespoke-scalpel-splitting-with-diabolical-precision","title":"The Bespoke Scalpel: Splitting with Diabolical Precision!\u00b6","text":"<p>Now, my ambitious apprentice, we ascend to the HIGHEST form of surgical artistry! While Rudolf Hammer and his corporate drones settle for uniform mediocrity, WE shall command each slice with mathematical perfection!</p> <p>The secret they don't want you to know: <code>torch.split()</code> accepts a list of integers, each commanding the exact size of its designated slice! The sum of these numbers must equal the total dimension\u2014this is not a suggestion, it is a LAW OF THE TENSOR UNIVERSE!</p> <p>Observe as we carve our 6-column chocolate bar into three asymmetric pieces of sizes <code>1, 2, and 3</code>. Each cut serves our grand design! Mwahahaha!</p>","tags":["tensors","module 1","beginner"],"boost":1.2},{"location":"01-tensors/02a_tensor_manipulation/#the-equal-divider-torchchunk","title":"The Equal Divider: <code>torch.chunk()</code>\u00b6","text":"<p><code>torch.chunk(tensor, chunks, dim)</code> divides your tensor into a specified number of roughly equal pieces. It's like asking: \"I need exactly 3 pieces, make them as equal as possible!\"</p> <p>Key Difference from <code>split()</code>:</p> <ul> <li><code>torch.split()</code>: \"Cut into pieces of size X\" (you control piece size)</li> <li><code>torch.chunk()</code>: \"Cut into exactly N pieces\" (you control number of pieces)</li> </ul> <p>If the dimension doesn't divide evenly, the last chunk will be smaller.</p>","tags":["tensors","module 1","beginner"],"boost":1.2},{"location":"01-tensors/02a_tensor_manipulation/#the-dimension-destroyer-torchunbind","title":"The Dimension Destroyer: <code>torch.unbind()</code>\u00b6","text":"<p><code>torch.unbind()</code> is the most dramatic! It removes an entire dimension by splitting the tensor along it. Each slice becomes a separate tensor with one fewer dimension.</p> <p>This is incredibly useful for:</p> <ul> <li>Processing each image in a batch separately</li> <li>Accessing individual time steps in sequence data</li> <li>Converting RGB channels into separate grayscale images</li> </ul> <p>Think of it as the opposite of <code>torch.stack()</code>!</p>","tags":["tensors","module 1","beginner"],"boost":1.2},{"location":"01-tensors/02a_tensor_manipulation/#part-4-real-world-surgical-applications","title":"Part 4: Real-World Surgical Applications \ud83c\udfed\u26a1\u00b6","text":"<p>Enough of my carefully controlled laboratory specimens! The time has come to witness the TRUE power of tensor surgery in the wild! These are not mere academic exercises\u2014these are the EXACT techniques used by the masters who built GPT, CLIP, and the neural networks that power modern AI!</p> <p>Prepare to see your newfound skills applied to the very foundations of modern machine learning!</p>","tags":["tensors","module 1","beginner"],"boost":1.2},{"location":"01-tensors/02a_tensor_manipulation/#exercise-1-multi-head-attention-surgery","title":"Exercise 1: Multi-Head Attention Surgery \ud83e\udde0\u26a1\u00b6","text":"<p>Real-World Context: This is EXACTLY how Transformer models (GPT, BERT) split their attention mechanism into multiple \"heads.\" Each head can focus on different aspects of the input\u2014some learn grammar, others learn semantics, others learn long-range dependencies!</p> <p>Your Mission: You have the concatenated output from all attention heads. Split it back into individual heads so each can be processed separately.</p> <p>The Setup: A Transformer's multi-head attention layer has just computed attention for a batch of sequences. All 8 attention heads are concatenated together in the last dimension. Your job: liberate each head!</p>","tags":["tensors","module 1","beginner"],"boost":1.2},{"location":"01-tensors/02a_tensor_manipulation/#exercise-2-multi-modal-fusion-clip-style","title":"Exercise 2: Multi-Modal Fusion (CLIP-Style) \ud83d\uddbc\ufe0f\ud83d\udcdd\u00b6","text":"<p>Real-World Context: This is the FOUNDATIONAL technique behind multimodal models! By concatenating text and image embeddings, AI systems learn to understand both modalities in a unified space.</p> <p>Your Mission: You have separate embeddings for text descriptions and images. Fuse them together to create a unified multimodal representation that can understand both vision and language!</p> <p>The Setup: A batch of text descriptions (\"A cat sitting on a chair\") and their corresponding image embeddings. Your fusion will enable the model to match images with text\u2014the core of visual search and image generation!</p>","tags":["tensors","module 1","beginner"],"boost":1.2},{"location":"01-tensors/02a_tensor_manipulation/#exercise-3-rgb-channel-liberation","title":"Exercise 3: RGB Channel Liberation \ud83c\udfa8\ud83d\udd13\u00b6","text":"<p>Real-World Context: Computer vision systems constantly need to separate and analyze individual color channels. This technique is fundamental.</p> <p>Your Mission: You have a batch of RGB images where all color channels are entangled together. Use your unbinding mastery to liberate each color channel into separate batches for independent processing!</p> <p>The Setup: A batch of color images from a dataset. Each image has Red, Green, and Blue channels mixed together. Your surgical precision will separate them cleanly while preserving the batch structure.</p>","tags":["tensors","module 1","beginner"],"boost":1.2},{"location":"01-tensors/02a_tensor_manipulation/#summary-the-surgeons-knowledge-is-complete","title":"Summary: The Surgeon's Knowledge Is Complete!\u00b6","text":"<p>Magnificent work, my surgical apprentice! You have mastered the fundamental operations of tensor surgery and assembly. Let us review your newly acquired powers:</p>","tags":["tensors","module 1","beginner"],"boost":1.2},{"location":"01-tensors/02a_tensor_manipulation/#the-art-of-selection","title":"\ud83d\udd2a The Art of Selection\u00b6","text":"<ul> <li>Indexing &amp; Slicing: Extract elements, rows, columns, or sub-regions with surgical precision</li> <li>Boolean Masking: Select elements that meet specific criteria</li> <li>Key Tools: <code>tensor[index]</code>, <code>tensor[start:end]</code>, <code>tensor[mask]</code></li> </ul>","tags":["tensors","module 1","beginner"],"boost":1.2},{"location":"01-tensors/02a_tensor_manipulation/#forbidden-fusions","title":"\ud83e\uddec Forbidden Fusions\u00b6","text":"<ul> <li>Concatenation: Join tensors along existing dimensions with <code>torch.cat()</code></li> <li>Stacking: Create new dimensions and organize tensors with <code>torch.stack()</code></li> <li>Key Rule: Understanding when to extend vs. when to create new dimensions</li> </ul>","tags":["tensors","module 1","beginner"],"boost":1.2},{"location":"01-tensors/02a_tensor_manipulation/#the-great-division","title":"\u2702\ufe0f The Great Division\u00b6","text":"<ul> <li>Precise Splitting: Cut into specific-sized pieces with <code>torch.split()</code></li> <li>Equal Division: Divide into a set number of chunks with <code>torch.chunk()</code></li> <li>Dimension Destruction: Remove dimensions entirely with <code>torch.unbind()</code></li> </ul> <p>You now possess the core skills to dissect any tensor, assemble complex structures, and divide them back into manageable pieces. These are the fundamental surgical techniques you'll use in every neural network adventure ahead!</p>","tags":["tensors","module 1","beginner"],"boost":1.2},{"location":"01-tensors/02a_tensor_manipulation/#professor-torchensteins-outro","title":"Professor Torchenstein's Outro\u00b6","text":"<p>Spectacular! You've wielded the surgical tools with the precision of a master! Your tensors have been sliced, fused, and divided according to your will. But tell me, my gifted apprentice\u2014do you feel that tingling sensation in your neural pathways? That's the hunger for more power!</p> <p>Your tensors may now be perfectly assembled, but they are still... rigid. Static in their dimensions. What if I told you that we could transform their very essence without changing their soul? What if we could make a 1D tensor become a 2D matrix, or a 3D cube collapse into a flat surface?</p> <p>In our next lesson, Tensor Metamorphosis: Shape-Shifting Mastery, we shall unlock the secrets of transformation itself! We will reshape reality, squeeze dimensions out of existence, and permute the cosmic order of our data!</p> <p>Until then, practice your surgical techniques. The metamorphosis chamber awaits! Mwahahahahaha!</p>      Your browser does not support the video tag.  <p>Proceed to the Metamorphosis Chamber!</p>","tags":["tensors","module 1","beginner"],"boost":1.2},{"location":"01-tensors/02b_tensor_metamorphosis/","title":"Tensor Metamorphosis: Shape-Shifting Mastery","text":"<p>Module 1 | Lesson 2b</p> In\u00a0[1]: Copied! <pre>import torch\n\n# Set the seed for cosmic consistency\ntorch.manual_seed(42)\n\nprint(\"\ud83d\udd2c MEMORY LAYOUT IN ACTION - ROW-MAJOR DEMONSTRATION\")\nprint(\"=\" * 65)\n\n# Create our test subject: numbers 1-12 in sequential memory\ndata = torch.arange(1, 13)  \nprint(\"\ud83e\udde0 Raw Data in Computer Memory (1D Reality):\")\nprint(f\"   Memory: {data.tolist()}\")\nprint(f\"   Shape: {data.shape} \u2190 This is how it ACTUALLY lives!\")\n\nprint(f\"\\n\ud83d\udcd0 ROW-MAJOR INTERPRETATION AS 3\u00d74 MATRIX:\")\nmatrix_3x4 = data.reshape(3, 4)\nprint(f\"   Same memory: {data.tolist()}\")\nprint(f\"   But interpreted as 3\u00d74:\")\nprint(matrix_3x4)\nprint(f\"   \ud83d\udca1 Row 1: [1,2,3,4] from memory positions 0-3\")\nprint(f\"   \ud83d\udca1 Row 2: [5,6,7,8] from memory positions 4-7\")\nprint(f\"   \ud83d\udca1 Row 3: [9,10,11,12] from memory positions 8-11\")\n\nprint(f\"\\n\ud83d\udd04 DIFFERENT INTERPRETATION: 4\u00d73 MATRIX:\")\nmatrix_4x3 = data.reshape(4, 3)  \nprint(f\"   Same memory: {data.tolist()}\")\nprint(f\"   But interpreted as 4\u00d73:\")\nprint(matrix_4x3)\nprint(f\"   \ud83d\udca1 Row 1: [1,2,3], Row 2: [4,5,6], Row 3: [7,8,9], Row 4: [10,11,12]\")\n\nprint(f\"\\n\u2728 THE FUNDAMENTAL INSIGHT:\")\nprint(f\"   - Memory never changes: {data.tolist()}\")\nprint(f\"   - Only our INTERPRETATION changes!\")\nprint(f\"   - This is the foundation of tensor metamorphosis!\")\n</pre> import torch  # Set the seed for cosmic consistency torch.manual_seed(42)  print(\"\ud83d\udd2c MEMORY LAYOUT IN ACTION - ROW-MAJOR DEMONSTRATION\") print(\"=\" * 65)  # Create our test subject: numbers 1-12 in sequential memory data = torch.arange(1, 13)   print(\"\ud83e\udde0 Raw Data in Computer Memory (1D Reality):\") print(f\"   Memory: {data.tolist()}\") print(f\"   Shape: {data.shape} \u2190 This is how it ACTUALLY lives!\")  print(f\"\\n\ud83d\udcd0 ROW-MAJOR INTERPRETATION AS 3\u00d74 MATRIX:\") matrix_3x4 = data.reshape(3, 4) print(f\"   Same memory: {data.tolist()}\") print(f\"   But interpreted as 3\u00d74:\") print(matrix_3x4) print(f\"   \ud83d\udca1 Row 1: [1,2,3,4] from memory positions 0-3\") print(f\"   \ud83d\udca1 Row 2: [5,6,7,8] from memory positions 4-7\") print(f\"   \ud83d\udca1 Row 3: [9,10,11,12] from memory positions 8-11\")  print(f\"\\n\ud83d\udd04 DIFFERENT INTERPRETATION: 4\u00d73 MATRIX:\") matrix_4x3 = data.reshape(4, 3)   print(f\"   Same memory: {data.tolist()}\") print(f\"   But interpreted as 4\u00d73:\") print(matrix_4x3) print(f\"   \ud83d\udca1 Row 1: [1,2,3], Row 2: [4,5,6], Row 3: [7,8,9], Row 4: [10,11,12]\")  print(f\"\\n\u2728 THE FUNDAMENTAL INSIGHT:\") print(f\"   - Memory never changes: {data.tolist()}\") print(f\"   - Only our INTERPRETATION changes!\") print(f\"   - This is the foundation of tensor metamorphosis!\")  <pre>\ud83d\udd2c MEMORY LAYOUT IN ACTION - ROW-MAJOR DEMONSTRATION\n=================================================================\n\ud83e\udde0 Raw Data in Computer Memory (1D Reality):\n   Memory: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n   Shape: torch.Size([12]) \u2190 This is how it ACTUALLY lives!\n\n\ud83d\udcd0 ROW-MAJOR INTERPRETATION AS 3\u00d74 MATRIX:\n   Same memory: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n   But interpreted as 3\u00d74:\ntensor([[ 1,  2,  3,  4],\n        [ 5,  6,  7,  8],\n        [ 9, 10, 11, 12]])\n   \ud83d\udca1 Row 1: [1,2,3,4] from memory positions 0-3\n   \ud83d\udca1 Row 2: [5,6,7,8] from memory positions 4-7\n   \ud83d\udca1 Row 3: [9,10,11,12] from memory positions 8-11\n\n\ud83d\udd04 DIFFERENT INTERPRETATION: 4\u00d73 MATRIX:\n   Same memory: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n   But interpreted as 4\u00d73:\ntensor([[ 1,  2,  3],\n        [ 4,  5,  6],\n        [ 7,  8,  9],\n        [10, 11, 12]])\n   \ud83d\udca1 Row 1: [1,2,3], Row 2: [4,5,6], Row 3: [7,8,9], Row 4: [10,11,12]\n\n\u2728 THE FUNDAMENTAL INSIGHT:\n   - Memory never changes: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n   - Only our INTERPRETATION changes!\n   - This is the foundation of tensor metamorphosis!\n</pre> <p>\ud83e\udddf\u200d\u2642\ufe0f Remember, dear tensor alchemist! \u2728</p> <p>Shape and form are but illusions! The memory remains unchanged\u2014it's only our interpretation that morphs!</p> <p>\u2013 Prof. Torchenstein</p> In\u00a0[6]: Copied! <pre>print(\"\ud83c\udfed PYTORCH'S MEMORY MANAGEMENT IN ACTION\")\nprint(\"=\" * 55)\n\n# Create original tensor  \noriginal = torch.arange(1, 13)\nprint(f\"Original tensor: {original}\")\n\n# Scenario 1: Shape change (should share storage AND data_ptr)\nreshaped = original.reshape(3, 4)\nprint(f\"\\n\ud83d\udcd0 SCENARIO 1: Shape Change (reshape)\")\nprint(f\"   Reshaped: \\n{reshaped}\")\n\nprint(f\"   \ud83d\udce6 Same storage? {original.storage().data_ptr()==reshaped.storage().data_ptr()} \")\nprint(f\"\\toriginal.storage().data_ptr()={original.storage().data_ptr()} \\n\\treshaped.storage().data_ptr()={reshaped.storage().data_ptr()}\")\nprint(f\"   \ud83c\udfaf Same data_ptr? {original.data_ptr() == reshaped.data_ptr()}\")\nprint(f\"\\toriginal.data_ptr()={original.data_ptr()} \\n\\treshaped.data_ptr()={reshaped.data_ptr()}\")\n\n# Scenario 2: Slice view (should share storage but DIFFERENT data_ptr)\nsliced = original[4:]  # Elements from index 4 onwards\nprint(f\"\\n\u2702\ufe0f SCENARIO 2: Slice View\")\nprint(f\"   Sliced tensor: {sliced}\")\nprint(f\"   \ud83d\udce6 Same storage? {original.storage().data_ptr() == sliced.storage().data_ptr()}\")\nprint(f\"\\toriginal.storage().data_ptr()={original.storage().data_ptr()} \\n\\tsliced.storage().data_ptr()={sliced.storage().data_ptr()}\")\nprint(f\"   \ud83c\udfaf Same data_ptr? {original.data_ptr() == sliced.data_ptr()}\")\nprint(f\"\\toriginal.data_ptr()={original.data_ptr()} \\n\\tsliced.data_ptr()={sliced.data_ptr()}\")\n\n# Calculate the offset for sliced tensor\nelement_size = original.element_size()\noffset = sliced.data_ptr() - original.data_ptr()\nprint(f\"   \ud83e\uddee Memory offset: {offset} bytes = {offset // element_size} elements\")\n\n# Scenario 3: True copy (different storage AND data_ptr)\ncopied = original.clone()\nprint(f\"\\n\ud83d\udccb SCENARIO 3: True Copy (clone)\")\nprint(f\"   Cloned tensor: {copied}\")\nprint(f\"   \ud83d\udce6 Same storage? {original.storage().data_ptr() == copied.storage().data_ptr()}\")\nprint(f\"   \ud83c\udfaf Same data_ptr? {original.data_ptr() == copied.data_ptr()}\")\n\nprint(f\"\\n\ud83d\udca1 PYTORCH'S MEMORY EFFICIENCY:\")\nprint(f\"   - Reshape: FREE! (same memory, different interpretation)\")\nprint(f\"   - Slice: EFFICIENT! (same memory, different starting point)\")  \nprint(f\"   - Clone: EXPENSIVE! (new memory allocation)\")\n</pre> print(\"\ud83c\udfed PYTORCH'S MEMORY MANAGEMENT IN ACTION\") print(\"=\" * 55)  # Create original tensor   original = torch.arange(1, 13) print(f\"Original tensor: {original}\")  # Scenario 1: Shape change (should share storage AND data_ptr) reshaped = original.reshape(3, 4) print(f\"\\n\ud83d\udcd0 SCENARIO 1: Shape Change (reshape)\") print(f\"   Reshaped: \\n{reshaped}\")  print(f\"   \ud83d\udce6 Same storage? {original.storage().data_ptr()==reshaped.storage().data_ptr()} \") print(f\"\\toriginal.storage().data_ptr()={original.storage().data_ptr()} \\n\\treshaped.storage().data_ptr()={reshaped.storage().data_ptr()}\") print(f\"   \ud83c\udfaf Same data_ptr? {original.data_ptr() == reshaped.data_ptr()}\") print(f\"\\toriginal.data_ptr()={original.data_ptr()} \\n\\treshaped.data_ptr()={reshaped.data_ptr()}\")  # Scenario 2: Slice view (should share storage but DIFFERENT data_ptr) sliced = original[4:]  # Elements from index 4 onwards print(f\"\\n\u2702\ufe0f SCENARIO 2: Slice View\") print(f\"   Sliced tensor: {sliced}\") print(f\"   \ud83d\udce6 Same storage? {original.storage().data_ptr() == sliced.storage().data_ptr()}\") print(f\"\\toriginal.storage().data_ptr()={original.storage().data_ptr()} \\n\\tsliced.storage().data_ptr()={sliced.storage().data_ptr()}\") print(f\"   \ud83c\udfaf Same data_ptr? {original.data_ptr() == sliced.data_ptr()}\") print(f\"\\toriginal.data_ptr()={original.data_ptr()} \\n\\tsliced.data_ptr()={sliced.data_ptr()}\")  # Calculate the offset for sliced tensor element_size = original.element_size() offset = sliced.data_ptr() - original.data_ptr() print(f\"   \ud83e\uddee Memory offset: {offset} bytes = {offset // element_size} elements\")  # Scenario 3: True copy (different storage AND data_ptr) copied = original.clone() print(f\"\\n\ud83d\udccb SCENARIO 3: True Copy (clone)\") print(f\"   Cloned tensor: {copied}\") print(f\"   \ud83d\udce6 Same storage? {original.storage().data_ptr() == copied.storage().data_ptr()}\") print(f\"   \ud83c\udfaf Same data_ptr? {original.data_ptr() == copied.data_ptr()}\")  print(f\"\\n\ud83d\udca1 PYTORCH'S MEMORY EFFICIENCY:\") print(f\"   - Reshape: FREE! (same memory, different interpretation)\") print(f\"   - Slice: EFFICIENT! (same memory, different starting point)\")   print(f\"   - Clone: EXPENSIVE! (new memory allocation)\")  <pre>\ud83c\udfed PYTORCH'S MEMORY MANAGEMENT IN ACTION\n=======================================================\nOriginal tensor: tensor([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12])\n\n\ud83d\udcd0 SCENARIO 1: Shape Change (reshape)\n   Reshaped: \ntensor([[ 1,  2,  3,  4],\n        [ 5,  6,  7,  8],\n        [ 9, 10, 11, 12]])\n   \ud83d\udce6 Same storage? True \n\toriginal.storage().data_ptr()=2814071805632 \n\treshaped.storage().data_ptr()=2814071805632\n   \ud83c\udfaf Same data_ptr? True\n\toriginal.data_ptr()=2814071805632 \n\treshaped.data_ptr()=2814071805632\n\n\u2702\ufe0f SCENARIO 2: Slice View\n   Sliced tensor: tensor([ 5,  6,  7,  8,  9, 10, 11, 12])\n   \ud83d\udce6 Same storage? True\n\toriginal.storage().data_ptr()=2814071805632 \n\tsliced.storage().data_ptr()=2814071805632\n   \ud83c\udfaf Same data_ptr? False\n\toriginal.data_ptr()=2814071805632 \n\tsliced.data_ptr()=2814071805664\n   \ud83e\uddee Memory offset: 32 bytes = 4 elements\n\n\ud83d\udccb SCENARIO 3: True Copy (clone)\n   Cloned tensor: tensor([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12])\n   \ud83d\udce6 Same storage? False\n   \ud83c\udfaf Same data_ptr? False\n\n\ud83d\udca1 PYTORCH'S MEMORY EFFICIENCY:\n   - Reshape: FREE! (same memory, different interpretation)\n   - Slice: EFFICIENT! (same memory, different starting point)\n   - Clone: EXPENSIVE! (new memory allocation)\n</pre> In\u00a0[\u00a0]: Copied! <pre>print(\"\ud83d\udc41\ufe0f TORCH.VIEW() MASTERCLASS\")\nprint(\"=\" * 40)\n\n# Create a contiguous tensor for our experiments  \ndata = torch.arange(24)  # 24 elements: 0, 1, 2, ..., 23\nprint(f\"Original data: {data}\")\nprint(f\"Shape: {data.shape}, Elements: {data.numel()}\")\n\nprint(f\"\\n\u2705 SUCCESS SCENARIOS - view() works perfectly:\")\n\n# Scenario 1: 1D to 2D\nmatrix_4x6 = data.view(4, 6)\nprint(f\"   1D\u21922D: {data.shape} \u2192 {matrix_4x6.shape}\")\nprint(f\"   Calculation: 24 elements = 4\u00d76? {4*6 == 24} \u2713\")\n\n# Scenario 2: Using -1 for automatic calculation\nauto_matrix = data.view(3, -1)  # PyTorch calculates: 24/3 = 8\nprint(f\"   Auto-calc: {data.shape} \u2192 {auto_matrix.shape}\")\nprint(f\"   PyTorch figured out: 24/3 = 8\")\n\n# Scenario 3: 1D to 3D (more complex)\ncube_2x3x4 = data.view(2, 3, 4)\nprint(f\"   1D\u21923D: {data.shape} \u2192 {cube_2x3x4.shape}\")\nprint(f\"   Calculation: 24 elements = 2\u00d73\u00d74? {2*3*4 == 24} \u2713\")\n\n# Scenario 4: Memory sharing verification\nprint(f\"\\n\ud83d\udd17 MEMORY SHARING TEST:\")\nprint(f\"   Original data_ptr: {data.data_ptr()}\")\nprint(f\"   Matrix data_ptr:   {matrix_4x6.data_ptr()}\")  \nprint(f\"   Same memory? {data.data_ptr() == matrix_4x6.data_ptr()} \u2713\")\n\n# Modify original - should affect the view!\ndata[0] = 999\nprint(f\"   Changed data[0] to 999...\")\nprint(f\"   Matrix[0,0] is now: {matrix_4x6[0,0]} (shares memory!)\")\n</pre> print(\"\ud83d\udc41\ufe0f TORCH.VIEW() MASTERCLASS\") print(\"=\" * 40)  # Create a contiguous tensor for our experiments   data = torch.arange(24)  # 24 elements: 0, 1, 2, ..., 23 print(f\"Original data: {data}\") print(f\"Shape: {data.shape}, Elements: {data.numel()}\")  print(f\"\\n\u2705 SUCCESS SCENARIOS - view() works perfectly:\")  # Scenario 1: 1D to 2D matrix_4x6 = data.view(4, 6) print(f\"   1D\u21922D: {data.shape} \u2192 {matrix_4x6.shape}\") print(f\"   Calculation: 24 elements = 4\u00d76? {4*6 == 24} \u2713\")  # Scenario 2: Using -1 for automatic calculation auto_matrix = data.view(3, -1)  # PyTorch calculates: 24/3 = 8 print(f\"   Auto-calc: {data.shape} \u2192 {auto_matrix.shape}\") print(f\"   PyTorch figured out: 24/3 = 8\")  # Scenario 3: 1D to 3D (more complex) cube_2x3x4 = data.view(2, 3, 4) print(f\"   1D\u21923D: {data.shape} \u2192 {cube_2x3x4.shape}\") print(f\"   Calculation: 24 elements = 2\u00d73\u00d74? {2*3*4 == 24} \u2713\")  # Scenario 4: Memory sharing verification print(f\"\\n\ud83d\udd17 MEMORY SHARING TEST:\") print(f\"   Original data_ptr: {data.data_ptr()}\") print(f\"   Matrix data_ptr:   {matrix_4x6.data_ptr()}\")   print(f\"   Same memory? {data.data_ptr() == matrix_4x6.data_ptr()} \u2713\")  # Modify original - should affect the view! data[0] = 999 print(f\"   Changed data[0] to 999...\") print(f\"   Matrix[0,0] is now: {matrix_4x6[0,0]} (shares memory!)\")  In\u00a0[9]: Copied! <pre>print(f\"\\n\u274c FAILURE SCENARIOS - view() throws errors:\")\n\n# Reset data\ndata = torch.arange(24) \n\n# Error 1: Impossible shape (wrong total elements)\ntry:\n    impossible = data.view(5, 5)  # 5\u00d75=25, but we have 24 elements\n    print(\"   Impossible shape: Success?!\")\nexcept RuntimeError as e:\n    print(f\"   \u274c Impossible shape (5\u00d75=25\u226024): {str(e)[:50]}...\")\n\n# Error 2: Non-contiguous memory (after transpose)\nmatrix = data.view(4, 6)\ntransposed = matrix.t()  # Creates non-contiguous memory\nprint(f\"   Non-contiguous tensor: {transposed.is_contiguous()}\")\ntry:\n    flattened = transposed.view(-1)\n    print(\"   view() on non-contiguous: Success?!\")\nexcept RuntimeError as e:\n    print(f\"   \u274c Non-contiguous memory: {str(e)}...\")\n</pre> print(f\"\\n\u274c FAILURE SCENARIOS - view() throws errors:\")  # Reset data data = torch.arange(24)   # Error 1: Impossible shape (wrong total elements) try:     impossible = data.view(5, 5)  # 5\u00d75=25, but we have 24 elements     print(\"   Impossible shape: Success?!\") except RuntimeError as e:     print(f\"   \u274c Impossible shape (5\u00d75=25\u226024): {str(e)[:50]}...\")  # Error 2: Non-contiguous memory (after transpose) matrix = data.view(4, 6) transposed = matrix.t()  # Creates non-contiguous memory print(f\"   Non-contiguous tensor: {transposed.is_contiguous()}\") try:     flattened = transposed.view(-1)     print(\"   view() on non-contiguous: Success?!\") except RuntimeError as e:     print(f\"   \u274c Non-contiguous memory: {str(e)}...\")   <pre>\n\u274c FAILURE SCENARIOS - view() throws errors:\n   \u274c Impossible shape (5\u00d75=25\u226024): shape '[5, 5]' is invalid for input of size 24...\n   Non-contiguous tensor: False\n   \u274c Non-contiguous memory: view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead....\n</pre> In\u00a0[\u00a0]: Copied! <pre>print(\"\ud83e\udd1d TORCH.RESHAPE() - THE DIPLOMATIC SOLUTION\")\nprint(\"=\" * 52)\n\n# Create test data\ndata = torch.arange(24)\nprint(f\"Original data: {data.shape} \u2192 {data[:6].tolist()}... (24 elements)\")\n\nprint(f\"\\n\u2705 SCENARIO 1: Contiguous tensor (reshape returns view)\")\nmatrix_4x6 = data.reshape(4, 6)\nprint(f\"   Original data_ptr: {data.data_ptr()}\")\nprint(f\"   Reshaped data_ptr: {matrix_4x6.data_ptr()}\")\nprint(f\"   Same memory (view)? {data.data_ptr() == matrix_4x6.data_ptr()} \u2713\")\n\nprint(f\"\\n\u26a0\ufe0f SCENARIO 2: Non-contiguous tensor (reshape creates copy)\")\n# First transpose to make it non-contiguous\ntransposed = matrix_4x6.t()  # Now 6x4, non-contiguous\nprint(f\"   Transposed contiguous? {transposed.is_contiguous()}\")\n\n# Now reshape the non-contiguous tensor\nflattened = transposed.reshape(-1)  # This works! (unlike view)\nprint(f\"   Transposed data_ptr: {transposed.data_ptr()}\")\nprint(f\"   Reshaped data_ptr:   {flattened.data_ptr()}\")\nprint(f\"   Same memory? {transposed.data_ptr() == flattened.data_ptr()}\")\nprint(f\"   Conclusion: reshape() created a COPY to make it work \u2713\")\n\nprint(f\"\\n\ud83c\udd9a DIRECT COMPARISON: view() vs reshape()\")\nprint(\"   Testing on the same non-contiguous tensor...\")\n\n# Test view() - should FAIL\ntry:\n    view_result = transposed.view(-1)\n    print(\"   view(): SUCCESS (unexpected!)\")\nexcept RuntimeError as e:\n    print(f\"   view(): FAILED \u274c - {str(e)[:40]}...\")\n\n# Test reshape() - should SUCCEED  \ntry:\n    reshape_result = transposed.reshape(-1)\n    print(f\"   reshape(): SUCCESS \u2705 - Shape: {reshape_result.shape}\")\nexcept RuntimeError as e:\n    print(f\"   reshape(): FAILED - {e}\")\n\nprint(f\"\\n\ud83d\udd0d INVESTIGATING: When does reshape() return view vs copy?\")\n\n# Case 1: Simple reshape of contiguous tensor\nsimple_data = torch.arange(12)\nreshaped_simple = simple_data.reshape(3, 4)\nshares_memory_1 = simple_data.data_ptr() == reshaped_simple.data_ptr()\nprint(f\"   Contiguous reshape \u2192 View: {shares_memory_1}\")\n\n# Case 2: Reshape after making non-contiguous\nnon_contig = reshaped_simple.t()  # Non-contiguous\nreshaped_non_contig = non_contig.reshape(-1)\nshares_memory_2 = non_contig.data_ptr() == reshaped_non_contig.data_ptr()\nprint(f\"   Non-contiguous reshape \u2192 View: {shares_memory_2} (Creates copy)\")\n\nprint(f\"\\n\ud83d\udca1 RESHAPE() WISDOM:\")\nprint(f\"   1. Always succeeds (if math is valid)\")\nprint(f\"   2. Returns view when memory layout allows\")\nprint(f\"   3. Creates copy when necessary\")\nprint(f\"   4. Perfect for beginners and prototyping\")\nprint(f\"   5. Use view() only when you need guaranteed performance\")\n</pre> print(\"\ud83e\udd1d TORCH.RESHAPE() - THE DIPLOMATIC SOLUTION\") print(\"=\" * 52)  # Create test data data = torch.arange(24) print(f\"Original data: {data.shape} \u2192 {data[:6].tolist()}... (24 elements)\")  print(f\"\\n\u2705 SCENARIO 1: Contiguous tensor (reshape returns view)\") matrix_4x6 = data.reshape(4, 6) print(f\"   Original data_ptr: {data.data_ptr()}\") print(f\"   Reshaped data_ptr: {matrix_4x6.data_ptr()}\") print(f\"   Same memory (view)? {data.data_ptr() == matrix_4x6.data_ptr()} \u2713\")  print(f\"\\n\u26a0\ufe0f SCENARIO 2: Non-contiguous tensor (reshape creates copy)\") # First transpose to make it non-contiguous transposed = matrix_4x6.t()  # Now 6x4, non-contiguous print(f\"   Transposed contiguous? {transposed.is_contiguous()}\")  # Now reshape the non-contiguous tensor flattened = transposed.reshape(-1)  # This works! (unlike view) print(f\"   Transposed data_ptr: {transposed.data_ptr()}\") print(f\"   Reshaped data_ptr:   {flattened.data_ptr()}\") print(f\"   Same memory? {transposed.data_ptr() == flattened.data_ptr()}\") print(f\"   Conclusion: reshape() created a COPY to make it work \u2713\")  print(f\"\\n\ud83c\udd9a DIRECT COMPARISON: view() vs reshape()\") print(\"   Testing on the same non-contiguous tensor...\")  # Test view() - should FAIL try:     view_result = transposed.view(-1)     print(\"   view(): SUCCESS (unexpected!)\") except RuntimeError as e:     print(f\"   view(): FAILED \u274c - {str(e)[:40]}...\")  # Test reshape() - should SUCCEED   try:     reshape_result = transposed.reshape(-1)     print(f\"   reshape(): SUCCESS \u2705 - Shape: {reshape_result.shape}\") except RuntimeError as e:     print(f\"   reshape(): FAILED - {e}\")  print(f\"\\n\ud83d\udd0d INVESTIGATING: When does reshape() return view vs copy?\")  # Case 1: Simple reshape of contiguous tensor simple_data = torch.arange(12) reshaped_simple = simple_data.reshape(3, 4) shares_memory_1 = simple_data.data_ptr() == reshaped_simple.data_ptr() print(f\"   Contiguous reshape \u2192 View: {shares_memory_1}\")  # Case 2: Reshape after making non-contiguous non_contig = reshaped_simple.t()  # Non-contiguous reshaped_non_contig = non_contig.reshape(-1) shares_memory_2 = non_contig.data_ptr() == reshaped_non_contig.data_ptr() print(f\"   Non-contiguous reshape \u2192 View: {shares_memory_2} (Creates copy)\")  print(f\"\\n\ud83d\udca1 RESHAPE() WISDOM:\") print(f\"   1. Always succeeds (if math is valid)\") print(f\"   2. Returns view when memory layout allows\") print(f\"   3. Creates copy when necessary\") print(f\"   4. Perfect for beginners and prototyping\") print(f\"   5. Use view() only when you need guaranteed performance\")  <pre>\ud83e\udd1d TORCH.RESHAPE() - THE DIPLOMATIC SOLUTION\n====================================================\nOriginal data: torch.Size([24]) \u2192 [0, 1, 2, 3, 4, 5]... (24 elements)\n\n\u2705 SCENARIO 1: Contiguous tensor (reshape returns view)\n   Original data_ptr: 5037313032192\n   Reshaped data_ptr: 5037313032192\n   Same memory (view)? True \u2713\n\n\u26a0\ufe0f SCENARIO 2: Non-contiguous tensor (reshape creates copy)\n   Transposed contiguous? False\n   Transposed data_ptr: 5037313032192\n   Reshaped data_ptr:   5037313032384\n   Same memory? False\n   Conclusion: reshape() created a COPY to make it work \u2713\n\n\ud83c\udd9a DIRECT COMPARISON: view() vs reshape()\n   Testing on the same non-contiguous tensor...\n   view(): FAILED \u274c - view size is not compatible with input t...\n   reshape(): SUCCESS \u2705 - Shape: torch.Size([24])\n\n\ud83d\udd0d INVESTIGATING: When does reshape() return view vs copy?\n   Contiguous reshape \u2192 View: True\n   Non-contiguous reshape \u2192 View: False (Creates copy)\n\n\ud83d\udca1 RESHAPE() WISDOM:\n   1. Always succeeds (if math is valid)\n   2. Returns view when memory layout allows\n   3. Creates copy when necessary\n   4. Perfect for beginners and prototyping\n   5. Use view() only when you need guaranteed performance\n</pre> In\u00a0[11]: Copied! <pre>print(\"\ud83e\udde9 ELEMENT FLOW MASTERCLASS - THE MIGRATION PATTERNS\")\nprint(\"=\" * 65)\n\n# Create our test subject: 2D matrix with clearly identifiable elements\ndata_2d = torch.arange(24).view(6, 4)  # 6 rows \u00d7 4 columns\nprint(\"\ud83d\udcca STARTING POINT: 6\u00d74 Matrix (24 elements)\")\nprint(f\"   Row-major memory order: {data_2d.flatten().tolist()}\")\nprint(f\"   Visual layout:\\n{data_2d}\")\n\nprint(f\"\\n\ud83c\udfaf TRANSFORMATION 1: 2D \u2192 3D (6\u00d74 \u2192 2\u00d73\u00d74)\")\nprint(\"   Question: How do elements flow into the new 3D structure?\")\ntransform_3d_v1 = data_2d.view(2, 3, 4)\nprint(f\"   Result shape: {transform_3d_v1.shape}\")\nprint(f\"   Element flow visualization:\")\nprint(f\"   \ud83d\udce6 Batch 0 (elements 0-11):\")\nprint(transform_3d_v1[0])\nprint(f\"   \ud83d\udce6 Batch 1 (elements 12-23):\")\nprint(transform_3d_v1[1])\nprint(f\"   \ud83d\udca1 Pattern: First 12 elements \u2192 Batch 0, Next 12 elements \u2192 Batch 1\")\n\nprint(f\"\\n\ud83d\udd04 TRANSFORMATION 2: 2D \u2192 3D (6\u00d74 \u2192 3\u00d72\u00d74)\")\nprint(\"   Same 24 elements, different 3D arrangement!\")\ntransform_3d_v2 = data_2d.view(3, 2, 4)\nprint(f\"   Result shape: {transform_3d_v2.shape}\")\nprint(f\"   Element flow visualization:\")\nfor i in range(3):\n    print(f\"   \ud83d\udce6 Batch {i} (elements {i*8}-{i*8+7}):\")\n    print(f\"      {transform_3d_v2[i]}\")\nprint(f\"   \ud83d\udca1 Pattern: Every 8 elements form a new batch!\")\n\nprint(f\"\\n\ud83c\udfb2 TRANSFORMATION 3: 2D \u2192 3D (6\u00d74 \u2192 4\u00d73\u00d72)\")\nprint(\"   Yet another way to slice the same 24 elements!\")\ntransform_3d_v3 = data_2d.view(4, 3, 2)\nprint(f\"   Result shape: {transform_3d_v3.shape}\")\nprint(f\"   Element flow visualization:\")\nfor i in range(4):\n    print(f\"   \ud83d\udce6 Batch {i} (elements {i*6}-{i*6+5}):\")\n    print(f\"      {transform_3d_v3[i]}\")\nprint(f\"   \ud83d\udca1 Pattern: Every 6 elements form a new batch!\")\n\nprint(f\"\\n\ud83e\udde0 THE ELEMENT FLOW ALGORITHM:\")\nprint(f\"   1. Elements are read in row-major order: 0,1,2,3,4,5...\")\nprint(f\"   2. They fill the NEW shape dimensions from right to left:\")\nprint(f\"      - Last dimension fills first: [0,1,2,3] if last dim = 4\")\nprint(f\"      - Then second-to-last: next group of 4 elements\")\nprint(f\"      - Then third-to-last: next group of groups\")\nprint(f\"   3. The memory order NEVER changes, only the interpretation!\")\n</pre> print(\"\ud83e\udde9 ELEMENT FLOW MASTERCLASS - THE MIGRATION PATTERNS\") print(\"=\" * 65)  # Create our test subject: 2D matrix with clearly identifiable elements data_2d = torch.arange(24).view(6, 4)  # 6 rows \u00d7 4 columns print(\"\ud83d\udcca STARTING POINT: 6\u00d74 Matrix (24 elements)\") print(f\"   Row-major memory order: {data_2d.flatten().tolist()}\") print(f\"   Visual layout:\\n{data_2d}\")  print(f\"\\n\ud83c\udfaf TRANSFORMATION 1: 2D \u2192 3D (6\u00d74 \u2192 2\u00d73\u00d74)\") print(\"   Question: How do elements flow into the new 3D structure?\") transform_3d_v1 = data_2d.view(2, 3, 4) print(f\"   Result shape: {transform_3d_v1.shape}\") print(f\"   Element flow visualization:\") print(f\"   \ud83d\udce6 Batch 0 (elements 0-11):\") print(transform_3d_v1[0]) print(f\"   \ud83d\udce6 Batch 1 (elements 12-23):\") print(transform_3d_v1[1]) print(f\"   \ud83d\udca1 Pattern: First 12 elements \u2192 Batch 0, Next 12 elements \u2192 Batch 1\")  print(f\"\\n\ud83d\udd04 TRANSFORMATION 2: 2D \u2192 3D (6\u00d74 \u2192 3\u00d72\u00d74)\") print(\"   Same 24 elements, different 3D arrangement!\") transform_3d_v2 = data_2d.view(3, 2, 4) print(f\"   Result shape: {transform_3d_v2.shape}\") print(f\"   Element flow visualization:\") for i in range(3):     print(f\"   \ud83d\udce6 Batch {i} (elements {i*8}-{i*8+7}):\")     print(f\"      {transform_3d_v2[i]}\") print(f\"   \ud83d\udca1 Pattern: Every 8 elements form a new batch!\")  print(f\"\\n\ud83c\udfb2 TRANSFORMATION 3: 2D \u2192 3D (6\u00d74 \u2192 4\u00d73\u00d72)\") print(\"   Yet another way to slice the same 24 elements!\") transform_3d_v3 = data_2d.view(4, 3, 2) print(f\"   Result shape: {transform_3d_v3.shape}\") print(f\"   Element flow visualization:\") for i in range(4):     print(f\"   \ud83d\udce6 Batch {i} (elements {i*6}-{i*6+5}):\")     print(f\"      {transform_3d_v3[i]}\") print(f\"   \ud83d\udca1 Pattern: Every 6 elements form a new batch!\")  print(f\"\\n\ud83e\udde0 THE ELEMENT FLOW ALGORITHM:\") print(f\"   1. Elements are read in row-major order: 0,1,2,3,4,5...\") print(f\"   2. They fill the NEW shape dimensions from right to left:\") print(f\"      - Last dimension fills first: [0,1,2,3] if last dim = 4\") print(f\"      - Then second-to-last: next group of 4 elements\") print(f\"      - Then third-to-last: next group of groups\") print(f\"   3. The memory order NEVER changes, only the interpretation!\")  <pre>\ud83e\udde9 ELEMENT FLOW MASTERCLASS - THE MIGRATION PATTERNS\n=================================================================\n\ud83d\udcca STARTING POINT: 6\u00d74 Matrix (24 elements)\n   Row-major memory order: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]\n   Visual layout:\ntensor([[ 0,  1,  2,  3],\n        [ 4,  5,  6,  7],\n        [ 8,  9, 10, 11],\n        [12, 13, 14, 15],\n        [16, 17, 18, 19],\n        [20, 21, 22, 23]])\n\n\ud83c\udfaf TRANSFORMATION 1: 2D \u2192 3D (6\u00d74 \u2192 2\u00d73\u00d74)\n   Question: How do elements flow into the new 3D structure?\n   Result shape: torch.Size([2, 3, 4])\n   Element flow visualization:\n   \ud83d\udce6 Batch 0 (elements 0-11):\ntensor([[ 0,  1,  2,  3],\n        [ 4,  5,  6,  7],\n        [ 8,  9, 10, 11]])\n   \ud83d\udce6 Batch 1 (elements 12-23):\ntensor([[12, 13, 14, 15],\n        [16, 17, 18, 19],\n        [20, 21, 22, 23]])\n   \ud83d\udca1 Pattern: First 12 elements \u2192 Batch 0, Next 12 elements \u2192 Batch 1\n\n\ud83d\udd04 TRANSFORMATION 2: 2D \u2192 3D (6\u00d74 \u2192 3\u00d72\u00d74)\n   Same 24 elements, different 3D arrangement!\n   Result shape: torch.Size([3, 2, 4])\n   Element flow visualization:\n   \ud83d\udce6 Batch 0 (elements 0-7):\n      tensor([[0, 1, 2, 3],\n        [4, 5, 6, 7]])\n   \ud83d\udce6 Batch 1 (elements 8-15):\n      tensor([[ 8,  9, 10, 11],\n        [12, 13, 14, 15]])\n   \ud83d\udce6 Batch 2 (elements 16-23):\n      tensor([[16, 17, 18, 19],\n        [20, 21, 22, 23]])\n   \ud83d\udca1 Pattern: Every 8 elements form a new batch!\n\n\ud83c\udfb2 TRANSFORMATION 3: 2D \u2192 3D (6\u00d74 \u2192 4\u00d73\u00d72)\n   Yet another way to slice the same 24 elements!\n   Result shape: torch.Size([4, 3, 2])\n   Element flow visualization:\n   \ud83d\udce6 Batch 0 (elements 0-5):\n      tensor([[0, 1],\n        [2, 3],\n        [4, 5]])\n   \ud83d\udce6 Batch 1 (elements 6-11):\n      tensor([[ 6,  7],\n        [ 8,  9],\n        [10, 11]])\n   \ud83d\udce6 Batch 2 (elements 12-17):\n      tensor([[12, 13],\n        [14, 15],\n        [16, 17]])\n   \ud83d\udce6 Batch 3 (elements 18-23):\n      tensor([[18, 19],\n        [20, 21],\n        [22, 23]])\n   \ud83d\udca1 Pattern: Every 6 elements form a new batch!\n\n\ud83e\udde0 THE ELEMENT FLOW ALGORITHM:\n   1. Elements are read in row-major order: 0,1,2,3,4,5...\n   2. They fill the NEW shape dimensions from right to left:\n      - Last dimension fills first: [0,1,2,3] if last dim = 4\n      - Then second-to-last: next group of 4 elements\n      - Then third-to-last: next group of groups\n   3. The memory order NEVER changes, only the interpretation!\n</pre> In\u00a0[17]: Copied! <pre>print(\"\ud83d\ude80 SOLVING REAL-WORLD NEURAL NETWORK SHAPE CHALLENGES\")\nprint(\"=\" * 65)\n\n# =============================================================================\n# CHALLENGE 1: CNN Feature Maps \u2192 Linear Layer\n# =============================================================================\nprint(\"\ud83d\udca1 CHALLENGE 1: CNN Feature Maps \u2192 Linear Layer\")\nprint(\"-\" * 55)\n\n# The scenario: You've processed a batch of images through CNN layers\nbatch_size = 16\nchannels = 128  # Feature maps from CNN\nheight, width = 7, 7  # Spatial dimensions after convolutions\n\n# This is what you get after CNN feature extraction\ncnn_features = torch.randn(batch_size, channels, height, width)\nprint(f\"\ud83d\udcca CNN output shape: {cnn_features.shape}\")\nprint(f\"   Interpretation: {batch_size} images, {channels} feature maps, {height}\u00d7{width} spatial size\")\n\n# The problem: Linear layer expects (batch_size, input_features) -&gt; (batch_size, output)\nprint(f\"\\n\ud83c\udfaf Linear layer expects: (batch_size, {channels*height*width})\")\nprint(f\"\u274c But we have: {cnn_features.shape}\")\n\n# THE SOLUTION: Flatten spatial dimensions while keeping batch dimension\nflattened_features = cnn_features.view(batch_size, -1)\nprint(f\"\\n\u2705 SOLUTION: view({batch_size}, -1)\")\nprint(f\"   Result shape: {flattened_features.shape}\")\n\n# Verify the calculation\nexpected_features = channels * height * width\nprint(f\"   Calculation: {channels} \u00d7 {height} \u00d7 {width} = {expected_features}\")\nprint(f\"   Matches? {flattened_features.shape[1] == expected_features}\")\n</pre> print(\"\ud83d\ude80 SOLVING REAL-WORLD NEURAL NETWORK SHAPE CHALLENGES\") print(\"=\" * 65)  # ============================================================================= # CHALLENGE 1: CNN Feature Maps \u2192 Linear Layer # ============================================================================= print(\"\ud83d\udca1 CHALLENGE 1: CNN Feature Maps \u2192 Linear Layer\") print(\"-\" * 55)  # The scenario: You've processed a batch of images through CNN layers batch_size = 16 channels = 128  # Feature maps from CNN height, width = 7, 7  # Spatial dimensions after convolutions  # This is what you get after CNN feature extraction cnn_features = torch.randn(batch_size, channels, height, width) print(f\"\ud83d\udcca CNN output shape: {cnn_features.shape}\") print(f\"   Interpretation: {batch_size} images, {channels} feature maps, {height}\u00d7{width} spatial size\")  # The problem: Linear layer expects (batch_size, input_features) -&gt; (batch_size, output) print(f\"\\n\ud83c\udfaf Linear layer expects: (batch_size, {channels*height*width})\") print(f\"\u274c But we have: {cnn_features.shape}\")  # THE SOLUTION: Flatten spatial dimensions while keeping batch dimension flattened_features = cnn_features.view(batch_size, -1) print(f\"\\n\u2705 SOLUTION: view({batch_size}, -1)\") print(f\"   Result shape: {flattened_features.shape}\")  # Verify the calculation expected_features = channels * height * width print(f\"   Calculation: {channels} \u00d7 {height} \u00d7 {width} = {expected_features}\") print(f\"   Matches? {flattened_features.shape[1] == expected_features}\")  <pre>\ud83d\ude80 SOLVING REAL-WORLD NEURAL NETWORK SHAPE CHALLENGES\n=================================================================\n\ud83d\udca1 CHALLENGE 1: CNN Feature Maps \u2192 Linear Layer\n-------------------------------------------------------\n\ud83d\udcca CNN output shape: torch.Size([16, 128, 7, 7])\n   Interpretation: 16 images, 128 feature maps, 7\u00d77 spatial size\n\n\ud83c\udfaf Linear layer expects: (batch_size, 6272)\n\u274c But we have: torch.Size([16, 128, 7, 7])\n\n\u2705 SOLUTION: view(16, -1)\n   Result shape: torch.Size([16, 6272])\n   Calculation: 128 \u00d7 7 \u00d7 7 = 6272\n   Matches? True\n</pre> In\u00a0[16]: Copied! <pre>print(\"\ud83d\ude80 MULTI-HEAD ATTENTION TRANSFORMATION - 3D \u2192 4D\")\nprint(\"=\" * 60)\n\n# Simulate the exact scenario from real Transformers!\nbatch_size, seq_len, hidden_size = 2, 4, 8  # Small example for clarity\nnum_heads = 2\nhead_dim = hidden_size // num_heads  # 8 // 2 = 4\n\n# Create embeddings tensor like in a real Transformer\nembeddings_3d = torch.arange(batch_size * seq_len * hidden_size).view(batch_size, seq_len, hidden_size)\nprint(f\"\ud83e\udde0 TRANSFORMER EMBEDDINGS: Shape {embeddings_3d.shape}\")\nprint(f\"   [batch_size, sequence_length, hidden_size] = [{batch_size}, {seq_len}, {hidden_size}]\")\nprint(f\"   This represents {batch_size} sequences, each with {seq_len} tokens, each token has {hidden_size} features\")\nprint(\"\\n   Embeddings tensor:\")\nfor b in range(batch_size):\n    print(f\"   Batch {b}:\")\n    for s in range(seq_len):\n        print(f\"      Token {s}: {embeddings_3d[b, s].tolist()} (features for this token)\")\n\n# THE TRANSFORMATION: Split hidden_size into multiple attention heads\nmulti_head_4d = embeddings_3d.view(batch_size, seq_len, num_heads, head_dim)\nprint(f\"\\n\u26a1 MULTI-HEAD TRANSFORMATION:\")\nprint(f\"   Original: [{batch_size}, {seq_len}, {hidden_size}] \u2192 New: [{batch_size}, {seq_len}, {num_heads}, {head_dim}]\")\nprint(f\"   Translation: [batch, tokens, features] \u2192 [batch, tokens, heads, features_per_head]\")\n\nprint(f\"\\n\ud83d\udd0d ELEMENT FLOW ANALYSIS:\")\nprint(f\"   Where do the original 8 features go for each token?\")\nfor b in range(batch_size):\n    for s in range(seq_len):\n        original_features = embeddings_3d[b, s]\n        print(f\"\\n   Batch {b}, Token {s} - Original features: {original_features.tolist()}\")\n        for h in range(num_heads):\n            head_features = multi_head_4d[b, s, h]\n            start_idx = h * head_dim\n            end_idx = start_idx + head_dim\n            print(f\"      Head {h}: {head_features.tolist()} (original features [{start_idx}:{end_idx}])\")\n\nprint(f\"\\n\ud83d\udca1 THE ATTENTION HEAD PATTERN:\")\nprint(f\"   \u2022 Each token's {hidden_size} features get split into {num_heads} groups of {head_dim}\")\nprint(f\"   \u2022 Head 0 gets features [0:{head_dim}], Head 1 gets features [{head_dim}:{hidden_size}]\")  \nprint(f\"   \u2022 This allows each attention head to focus on different aspects!\")\nprint(f\"   \u2022 The element order is preserved: [0,1,2,3,4,5,6,7] \u2192 Head0:[0,1,2,3], Head1:[4,5,6,7]\")\n\nprint(f\"\\n\ud83c\udfaf WHY THIS TRANSFORMATION IS GENIUS:\")\nprint(f\"   \u2022 Same memory, but now we can process {num_heads} attention heads in parallel\")\nprint(f\"   \u2022 Each head learns different patterns (grammar, semantics, etc.)\")\nprint(f\"   \u2022 This is the SECRET behind Transformer's incredible power!\")\nprint(f\"   \u2022 GPT, BERT, ChatGPT - they ALL use this exact transformation!\")\n</pre> print(\"\ud83d\ude80 MULTI-HEAD ATTENTION TRANSFORMATION - 3D \u2192 4D\") print(\"=\" * 60)  # Simulate the exact scenario from real Transformers! batch_size, seq_len, hidden_size = 2, 4, 8  # Small example for clarity num_heads = 2 head_dim = hidden_size // num_heads  # 8 // 2 = 4  # Create embeddings tensor like in a real Transformer embeddings_3d = torch.arange(batch_size * seq_len * hidden_size).view(batch_size, seq_len, hidden_size) print(f\"\ud83e\udde0 TRANSFORMER EMBEDDINGS: Shape {embeddings_3d.shape}\") print(f\"   [batch_size, sequence_length, hidden_size] = [{batch_size}, {seq_len}, {hidden_size}]\") print(f\"   This represents {batch_size} sequences, each with {seq_len} tokens, each token has {hidden_size} features\") print(\"\\n   Embeddings tensor:\") for b in range(batch_size):     print(f\"   Batch {b}:\")     for s in range(seq_len):         print(f\"      Token {s}: {embeddings_3d[b, s].tolist()} (features for this token)\")  # THE TRANSFORMATION: Split hidden_size into multiple attention heads multi_head_4d = embeddings_3d.view(batch_size, seq_len, num_heads, head_dim) print(f\"\\n\u26a1 MULTI-HEAD TRANSFORMATION:\") print(f\"   Original: [{batch_size}, {seq_len}, {hidden_size}] \u2192 New: [{batch_size}, {seq_len}, {num_heads}, {head_dim}]\") print(f\"   Translation: [batch, tokens, features] \u2192 [batch, tokens, heads, features_per_head]\")  print(f\"\\n\ud83d\udd0d ELEMENT FLOW ANALYSIS:\") print(f\"   Where do the original 8 features go for each token?\") for b in range(batch_size):     for s in range(seq_len):         original_features = embeddings_3d[b, s]         print(f\"\\n   Batch {b}, Token {s} - Original features: {original_features.tolist()}\")         for h in range(num_heads):             head_features = multi_head_4d[b, s, h]             start_idx = h * head_dim             end_idx = start_idx + head_dim             print(f\"      Head {h}: {head_features.tolist()} (original features [{start_idx}:{end_idx}])\")  print(f\"\\n\ud83d\udca1 THE ATTENTION HEAD PATTERN:\") print(f\"   \u2022 Each token's {hidden_size} features get split into {num_heads} groups of {head_dim}\") print(f\"   \u2022 Head 0 gets features [0:{head_dim}], Head 1 gets features [{head_dim}:{hidden_size}]\")   print(f\"   \u2022 This allows each attention head to focus on different aspects!\") print(f\"   \u2022 The element order is preserved: [0,1,2,3,4,5,6,7] \u2192 Head0:[0,1,2,3], Head1:[4,5,6,7]\")  print(f\"\\n\ud83c\udfaf WHY THIS TRANSFORMATION IS GENIUS:\") print(f\"   \u2022 Same memory, but now we can process {num_heads} attention heads in parallel\") print(f\"   \u2022 Each head learns different patterns (grammar, semantics, etc.)\") print(f\"   \u2022 This is the SECRET behind Transformer's incredible power!\") print(f\"   \u2022 GPT, BERT, ChatGPT - they ALL use this exact transformation!\")  <pre>\ud83d\ude80 MULTI-HEAD ATTENTION TRANSFORMATION - 3D \u2192 4D\n============================================================\n\ud83e\udde0 TRANSFORMER EMBEDDINGS: Shape torch.Size([2, 4, 8])\n   [batch_size, sequence_length, hidden_size] = [2, 4, 8]\n   This represents 2 sequences, each with 4 tokens, each token has 8 features\n\n   Embeddings tensor:\n   Batch 0:\n      Token 0: [0, 1, 2, 3, 4, 5, 6, 7] (features for this token)\n      Token 1: [8, 9, 10, 11, 12, 13, 14, 15] (features for this token)\n      Token 2: [16, 17, 18, 19, 20, 21, 22, 23] (features for this token)\n      Token 3: [24, 25, 26, 27, 28, 29, 30, 31] (features for this token)\n   Batch 1:\n      Token 0: [32, 33, 34, 35, 36, 37, 38, 39] (features for this token)\n      Token 1: [40, 41, 42, 43, 44, 45, 46, 47] (features for this token)\n      Token 2: [48, 49, 50, 51, 52, 53, 54, 55] (features for this token)\n      Token 3: [56, 57, 58, 59, 60, 61, 62, 63] (features for this token)\n\n\u26a1 MULTI-HEAD TRANSFORMATION:\n   Original: [2, 4, 8] \u2192 New: [2, 4, 2, 4]\n   Translation: [batch, tokens, features] \u2192 [batch, tokens, heads, features_per_head]\n\n\ud83d\udd0d ELEMENT FLOW ANALYSIS:\n   Where do the original 8 features go for each token?\n\n   Batch 0, Token 0 - Original features: [0, 1, 2, 3, 4, 5, 6, 7]\n      Head 0: [0, 1, 2, 3] (original features [0:4])\n      Head 1: [4, 5, 6, 7] (original features [4:8])\n\n   Batch 0, Token 1 - Original features: [8, 9, 10, 11, 12, 13, 14, 15]\n      Head 0: [8, 9, 10, 11] (original features [0:4])\n      Head 1: [12, 13, 14, 15] (original features [4:8])\n\n   Batch 0, Token 2 - Original features: [16, 17, 18, 19, 20, 21, 22, 23]\n      Head 0: [16, 17, 18, 19] (original features [0:4])\n      Head 1: [20, 21, 22, 23] (original features [4:8])\n\n   Batch 0, Token 3 - Original features: [24, 25, 26, 27, 28, 29, 30, 31]\n      Head 0: [24, 25, 26, 27] (original features [0:4])\n      Head 1: [28, 29, 30, 31] (original features [4:8])\n\n   Batch 1, Token 0 - Original features: [32, 33, 34, 35, 36, 37, 38, 39]\n      Head 0: [32, 33, 34, 35] (original features [0:4])\n      Head 1: [36, 37, 38, 39] (original features [4:8])\n\n   Batch 1, Token 1 - Original features: [40, 41, 42, 43, 44, 45, 46, 47]\n      Head 0: [40, 41, 42, 43] (original features [0:4])\n      Head 1: [44, 45, 46, 47] (original features [4:8])\n\n   Batch 1, Token 2 - Original features: [48, 49, 50, 51, 52, 53, 54, 55]\n      Head 0: [48, 49, 50, 51] (original features [0:4])\n      Head 1: [52, 53, 54, 55] (original features [4:8])\n\n   Batch 1, Token 3 - Original features: [56, 57, 58, 59, 60, 61, 62, 63]\n      Head 0: [56, 57, 58, 59] (original features [0:4])\n      Head 1: [60, 61, 62, 63] (original features [4:8])\n\n\ud83d\udca1 THE ATTENTION HEAD PATTERN:\n   \u2022 Each token's 8 features get split into 2 groups of 4\n   \u2022 Head 0 gets features [0:4], Head 1 gets features [4:8]\n   \u2022 This allows each attention head to focus on different aspects!\n   \u2022 The element order is preserved: [0,1,2,3,4,5,6,7] \u2192 Head0:[0,1,2,3], Head1:[4,5,6,7]\n\n\ud83c\udfaf WHY THIS TRANSFORMATION IS GENIUS:\n   \u2022 Same memory, but now we can process 2 attention heads in parallel\n   \u2022 Each head learns different patterns (grammar, semantics, etc.)\n   \u2022 This is the SECRET behind Transformer's incredible power!\n   \u2022 GPT, BERT, ChatGPT - they ALL use this exact transformation!\n</pre> In\u00a0[25]: Copied! <pre>print(\"\ud83d\udddc\ufe0f THE SQUEEZE &amp; UNSQUEEZE DIMENSIONAL DANCE\")\nprint(\"=\" * 55)\n\n# Create our test subject with some size-1 dimensions\noriginal = torch.randn(1, 4, 1, 3, 1)  # Tensor with multiple size-1 dimensions\nprint(f\"\ud83c\udfad Original tensor shape: {original.shape}\")\nprint(f\"   Total elements: {original.numel()}\")\nprint(f\"   Contains dimensions of size 1 at positions: 0, 2, 4\")\n\nprint(f\"\\n\ud83d\udddc\ufe0f SQUEEZE OPERATIONS - Removing Size-1 Dimensions\")\nprint(\"-\" * 35)\n\n# Squeeze all size-1 dimensions\nsqueezed_all = original.squeeze()\nprint(f\"   squeeze() \u2192 {squeezed_all.shape}\")\nprint(f\"   Removed ALL size-1 dimensions!\")\n\n# Squeeze specific dimension\nsqueezed_dim0 = original.squeeze(0)  # Remove dimension 0 (size 1)\nprint(f\"   squeeze(0) \u2192 {squeezed_dim0.shape}\")\nprint(f\"   Removed only dimension 0\")\n\nsqueezed_dim2 = original.squeeze(2)  # Remove dimension 2 (size 1)\nprint(f\"   squeeze(2) \u2192 {squeezed_dim2.shape}\")\nprint(f\"   Removed only dimension 2\")\n\n# What happens if we try to squeeze a dimension that's NOT size 1?\n\nprint(f\"\\n\u26a0\ufe0f IMPORTANT BEHAVIOR: squeeze() on non-size-1 dimensions\")\nprint(\"-\" * 30)\ntest_behavior = torch.randn(2, 3, 4)  # No size-1 dimensions\nprint(f\"   Test tensor: {test_behavior.shape}\")\nresult = test_behavior.squeeze(1)  # Try to squeeze dimension 1 (size 3)\nprint(f\"   After squeeze(1): {result.shape} (unchanged!)\")\nprint(f\"   squeeze() silently ignores non-size-1 dimensions - no error thrown!\")\n</pre> print(\"\ud83d\udddc\ufe0f THE SQUEEZE &amp; UNSQUEEZE DIMENSIONAL DANCE\") print(\"=\" * 55)  # Create our test subject with some size-1 dimensions original = torch.randn(1, 4, 1, 3, 1)  # Tensor with multiple size-1 dimensions print(f\"\ud83c\udfad Original tensor shape: {original.shape}\") print(f\"   Total elements: {original.numel()}\") print(f\"   Contains dimensions of size 1 at positions: 0, 2, 4\")  print(f\"\\n\ud83d\udddc\ufe0f SQUEEZE OPERATIONS - Removing Size-1 Dimensions\") print(\"-\" * 35)  # Squeeze all size-1 dimensions squeezed_all = original.squeeze() print(f\"   squeeze() \u2192 {squeezed_all.shape}\") print(f\"   Removed ALL size-1 dimensions!\")  # Squeeze specific dimension squeezed_dim0 = original.squeeze(0)  # Remove dimension 0 (size 1) print(f\"   squeeze(0) \u2192 {squeezed_dim0.shape}\") print(f\"   Removed only dimension 0\")  squeezed_dim2 = original.squeeze(2)  # Remove dimension 2 (size 1) print(f\"   squeeze(2) \u2192 {squeezed_dim2.shape}\") print(f\"   Removed only dimension 2\")  # What happens if we try to squeeze a dimension that's NOT size 1?  print(f\"\\n\u26a0\ufe0f IMPORTANT BEHAVIOR: squeeze() on non-size-1 dimensions\") print(\"-\" * 30) test_behavior = torch.randn(2, 3, 4)  # No size-1 dimensions print(f\"   Test tensor: {test_behavior.shape}\") result = test_behavior.squeeze(1)  # Try to squeeze dimension 1 (size 3) print(f\"   After squeeze(1): {result.shape} (unchanged!)\") print(f\"   squeeze() silently ignores non-size-1 dimensions - no error thrown!\")   <pre>\ud83d\udddc\ufe0f THE SQUEEZE &amp; UNSQUEEZE DIMENSIONAL DANCE\n=======================================================\n\ud83c\udfad Original tensor shape: torch.Size([1, 4, 1, 3, 1])\n   Total elements: 12\n   Contains dimensions of size 1 at positions: 0, 2, 4\n\n\ud83d\udddc\ufe0f SQUEEZE OPERATIONS - Removing Size-1 Dimensions\n-----------------------------------\n   squeeze() \u2192 torch.Size([4, 3])\n   Removed ALL size-1 dimensions!\n   squeeze(0) \u2192 torch.Size([4, 1, 3, 1])\n   Removed only dimension 0\n   squeeze(2) \u2192 torch.Size([1, 4, 3, 1])\n   Removed only dimension 2\n\n\u26a0\ufe0f IMPORTANT BEHAVIOR: squeeze() on non-size-1 dimensions\n------------------------------\n   Test tensor: torch.Size([2, 3, 4])\n   After squeeze(1): torch.Size([2, 3, 4]) (unchanged!)\n   squeeze() silently ignores non-size-1 dimensions - no error thrown!\n</pre> In\u00a0[26]: Copied! <pre>print(f\"\\n\ud83c\udf88 UNSQUEEZE OPERATIONS - Adding Size-1 Dimensions\")\nprint(\"-\" * 38)\n\n# Start with a simple 2D tensor\nsimple_2d = torch.randn(3, 4)\nprint(f\"   Starting tensor: {simple_2d.shape}\")\n\n# Add dimensions at different positions\nunsqueezed_0 = simple_2d.unsqueeze(0)  # Add dimension at position 0\nprint(f\"   unsqueeze(0) \u2192 {unsqueezed_0.shape}\")\nprint(f\"   Added size-1 dimension at the beginning\")\n\nunsqueezed_1 = simple_2d.unsqueeze(1)  # Add dimension at position 1\nprint(f\"   unsqueeze(1) \u2192 {unsqueezed_1.shape}\")\nprint(f\"   Added size-1 dimension in the middle\")\n\nunsqueezed_2 = simple_2d.unsqueeze(2)  # Add dimension at position 2 (end)\nprint(f\"   unsqueeze(2) \u2192 {unsqueezed_2.shape}\")\nprint(f\"   Added size-1 dimension at the end\")\n\n# Negative indices work too!\nunsqueezed_neg = simple_2d.unsqueeze(-1)  # Add at the last position\nprint(f\"   unsqueeze(-1) \u2192 {unsqueezed_neg.shape}\")\nprint(f\"   Added size-1 dimension at the end (using negative indexing)\")\n</pre> print(f\"\\n\ud83c\udf88 UNSQUEEZE OPERATIONS - Adding Size-1 Dimensions\") print(\"-\" * 38)  # Start with a simple 2D tensor simple_2d = torch.randn(3, 4) print(f\"   Starting tensor: {simple_2d.shape}\")  # Add dimensions at different positions unsqueezed_0 = simple_2d.unsqueeze(0)  # Add dimension at position 0 print(f\"   unsqueeze(0) \u2192 {unsqueezed_0.shape}\") print(f\"   Added size-1 dimension at the beginning\")  unsqueezed_1 = simple_2d.unsqueeze(1)  # Add dimension at position 1 print(f\"   unsqueeze(1) \u2192 {unsqueezed_1.shape}\") print(f\"   Added size-1 dimension in the middle\")  unsqueezed_2 = simple_2d.unsqueeze(2)  # Add dimension at position 2 (end) print(f\"   unsqueeze(2) \u2192 {unsqueezed_2.shape}\") print(f\"   Added size-1 dimension at the end\")  # Negative indices work too! unsqueezed_neg = simple_2d.unsqueeze(-1)  # Add at the last position print(f\"   unsqueeze(-1) \u2192 {unsqueezed_neg.shape}\") print(f\"   Added size-1 dimension at the end (using negative indexing)\") <pre>\n\ud83c\udf88 UNSQUEEZE OPERATIONS - Adding Size-1 Dimensions\n--------------------------------------\n   Starting tensor: torch.Size([3, 4])\n   unsqueeze(0) \u2192 torch.Size([1, 3, 4])\n   Added size-1 dimension at the beginning\n   unsqueeze(1) \u2192 torch.Size([3, 1, 4])\n   Added size-1 dimension in the middle\n   unsqueeze(2) \u2192 torch.Size([3, 4, 1])\n   Added size-1 dimension at the end\n   unsqueeze(-1) \u2192 torch.Size([3, 4, 1])\n   Added size-1 dimension at the end (using negative indexing)\n</pre> In\u00a0[31]: Copied! <pre>print(\"\"\"=============================================================================\n\ud83c\udfafAPPLICATION 1: Broadcasting Preparation - Channel-wise Operations\n=============================================================================\"\"\")\n\n\n# Scenario: You have a batch of RGB images and want to apply different scaling to each channel\nbatch_images = torch.randn(32, 3, 224, 224)  # Batch of 32 RGB images\nchannel_scales = torch.tensor([0.8, 1.2, 1.0])  # Scale factors for R, G, B channels\n\nprint(f\"\ud83d\udcf8 Images shape: {batch_images.shape} (batch, channels, height, width)\")\nprint(f\"\u2696\ufe0f Channel scales: {channel_scales.shape} (just 3 values)\")\n\n# Problem: Can't broadcast (32, 3, 224, 224) with (3,)\n# Solution: Use unsqueeze to make scales broadcastable\nscales_broadcastable = channel_scales.unsqueeze(0).unsqueeze(2).unsqueeze(3)\nprint(f\"\u2705 After unsqueeze(0,2,3): {scales_broadcastable.shape}\")\nprint(f\"   Now compatible for broadcasting: (32,3,224,224) * (1,3,1,1)\")\n\n# Apply channel-wise scaling\nscaled_images = batch_images * scales_broadcastable\nprint(f\"\ud83c\udfa8 Scaled images shape: {scaled_images.shape}\")\n\nprint(f\"\\n\ud83d\udcca Broadcasting Magic Explained:\")\nprint(f\"   Original scales: {channel_scales}\")\nprint(f\"   After unsqueeze:  shape {scales_broadcastable.shape}\")\nprint(f\"   Each channel gets its own scaling factor across all images!\")\n\nprint(\"\\n\" + \"=\" * 70)\n</pre>  print(\"\"\"============================================================================= \ud83c\udfafAPPLICATION 1: Broadcasting Preparation - Channel-wise Operations =============================================================================\"\"\")   # Scenario: You have a batch of RGB images and want to apply different scaling to each channel batch_images = torch.randn(32, 3, 224, 224)  # Batch of 32 RGB images channel_scales = torch.tensor([0.8, 1.2, 1.0])  # Scale factors for R, G, B channels  print(f\"\ud83d\udcf8 Images shape: {batch_images.shape} (batch, channels, height, width)\") print(f\"\u2696\ufe0f Channel scales: {channel_scales.shape} (just 3 values)\")  # Problem: Can't broadcast (32, 3, 224, 224) with (3,) # Solution: Use unsqueeze to make scales broadcastable scales_broadcastable = channel_scales.unsqueeze(0).unsqueeze(2).unsqueeze(3) print(f\"\u2705 After unsqueeze(0,2,3): {scales_broadcastable.shape}\") print(f\"   Now compatible for broadcasting: (32,3,224,224) * (1,3,1,1)\")  # Apply channel-wise scaling scaled_images = batch_images * scales_broadcastable print(f\"\ud83c\udfa8 Scaled images shape: {scaled_images.shape}\")  print(f\"\\n\ud83d\udcca Broadcasting Magic Explained:\") print(f\"   Original scales: {channel_scales}\") print(f\"   After unsqueeze:  shape {scales_broadcastable.shape}\") print(f\"   Each channel gets its own scaling factor across all images!\")  print(\"\\n\" + \"=\" * 70)  <pre>=============================================================================\n\ud83c\udfafAPPLICATION 1: Broadcasting Preparation - Channel-wise Operations\n=============================================================================\n\ud83d\udcf8 Images shape: torch.Size([32, 3, 224, 224]) (batch, channels, height, width)\n\u2696\ufe0f Channel scales: torch.Size([3]) (just 3 values)\n\u2705 After unsqueeze(0,2,3): torch.Size([1, 3, 1, 1])\n   Now compatible for broadcasting: (32,3,224,224) * (1,3,1,1)\n\ud83c\udfa8 Scaled images shape: torch.Size([32, 3, 224, 224])\n\n\ud83d\udcca Broadcasting Magic Explained:\n   Original scales: tensor([0.8000, 1.2000, 1.0000])\n   After unsqueeze:  shape torch.Size([1, 3, 1, 1])\n   Each channel gets its own scaling factor across all images!\n\n======================================================================\n</pre> In\u00a0[\u00a0]: Copied! <pre>print(\"\"\"=============================================================================\n\ud83c\udfd7\ufe0f APPLICATION 2: Layer Compatibility - Removing Unwanted Dimensions\n=============================================================================\"\"\")\n\n# Scenario: Global Average Pooling output needs dimension cleanup\nbatch_size, channels = 16, 512\n# After global average pooling, spatial dimensions become 1\u00d71\npooled_features = torch.randn(batch_size, channels, 1, 1)\nprint(f\"\ud83c\udfca After Global Avg Pool: {pooled_features.shape}\")\nprint(f\"   Those 1\u00d71 spatial dimensions are useless for classification!\")\n\n# Linear layer expects (batch_size, features) not (batch_size, features, 1, 1)\nlinear_classifier = torch.nn.Linear(channels, 10)  # 10 classes\n\n# Solution: Squeeze out the spatial dimensions\nflattened_features = pooled_features.squeeze(3).squeeze(2)\n# Or equivalently: pooled_features.squeeze() removes ALL size-1 dimensions\nprint(f\"\u2705 After squeeze(3,2): {flattened_features.shape}\")\n\n# Now it works with Linear layer\nlogits = linear_classifier(flattened_features)\nprint(f\"\ud83c\udfaf Classification logits: {logits.shape} (ready for softmax!)\")\n\nprint(\"\\n\" + \"=\" * 70)\n</pre> print(\"\"\"============================================================================= \ud83c\udfd7\ufe0f APPLICATION 2: Layer Compatibility - Removing Unwanted Dimensions =============================================================================\"\"\")  # Scenario: Global Average Pooling output needs dimension cleanup batch_size, channels = 16, 512 # After global average pooling, spatial dimensions become 1\u00d71 pooled_features = torch.randn(batch_size, channels, 1, 1) print(f\"\ud83c\udfca After Global Avg Pool: {pooled_features.shape}\") print(f\"   Those 1\u00d71 spatial dimensions are useless for classification!\")  # Linear layer expects (batch_size, features) not (batch_size, features, 1, 1) linear_classifier = torch.nn.Linear(channels, 10)  # 10 classes  # Solution: Squeeze out the spatial dimensions flattened_features = pooled_features.squeeze(3).squeeze(2) # Or equivalently: pooled_features.squeeze() removes ALL size-1 dimensions print(f\"\u2705 After squeeze(3,2): {flattened_features.shape}\")  # Now it works with Linear layer logits = linear_classifier(flattened_features) print(f\"\ud83c\udfaf Classification logits: {logits.shape} (ready for softmax!)\")  print(\"\\n\" + \"=\" * 70) <pre>=============================================================================\n\ud83c\udfd7\ufe0f APPLICATION 2: Layer Compatibility - Removing Unwanted Dimensions\n=============================================================================\n\ud83c\udfca After Global Avg Pool: torch.Size([16, 512, 1, 1])\n   Those 1\u00d71 spatial dimensions are useless for classification!\n\u2705 After squeeze(3,2): torch.Size([16, 512])\n\ud83c\udfaf Classification logits: torch.Size([16, 10]) (ready for softmax!)\n\n======================================================================\n</pre> In\u00a0[\u00a0]: Copied! <pre>print(\"\"\"=============================================================================\n\ud83d\udce6 APPLICATION 3: Batch Dimension Management\n=============================================================================\"\"\")\n\n\n# Scenario: You have a single image but model expects batches\nsingle_image = torch.randn(3, 224, 224)  # Single RGB image\nprint(f\"\ud83d\uddbc\ufe0f Single image: {single_image.shape}\")\n\n# Model expects batch dimension\nprint(f\"\ud83e\udd16 Model expects: (batch_size, channels, height, width)\")\n\n# Solution: Add batch dimension\nbatched_image = single_image.unsqueeze(0)\nprint(f\"\u2705 After unsqueeze(0): {batched_image.shape}\")\nprint(f\"   Now it looks like a batch of 1 image!\")\n\n# After model processing, you might want to remove batch dimension\nmodel_output = torch.randn(1, 1000)  # Pretend model output (1 sample, 1000 classes)\nsingle_output = model_output.squeeze(0)\nprint(f\"\ud83c\udfb2 Model output: {model_output.shape}\")\nprint(f\"\ud83d\udce4 After squeeze(0): {single_output.shape} (back to single sample)\")\n</pre> print(\"\"\"============================================================================= \ud83d\udce6 APPLICATION 3: Batch Dimension Management =============================================================================\"\"\")   # Scenario: You have a single image but model expects batches single_image = torch.randn(3, 224, 224)  # Single RGB image print(f\"\ud83d\uddbc\ufe0f Single image: {single_image.shape}\")  # Model expects batch dimension print(f\"\ud83e\udd16 Model expects: (batch_size, channels, height, width)\")  # Solution: Add batch dimension batched_image = single_image.unsqueeze(0) print(f\"\u2705 After unsqueeze(0): {batched_image.shape}\") print(f\"   Now it looks like a batch of 1 image!\")  # After model processing, you might want to remove batch dimension model_output = torch.randn(1, 1000)  # Pretend model output (1 sample, 1000 classes) single_output = model_output.squeeze(0) print(f\"\ud83c\udfb2 Model output: {model_output.shape}\") print(f\"\ud83d\udce4 After squeeze(0): {single_output.shape} (back to single sample)\")  <pre>=============================================================================\n\ud83d\udce6 APPLICATION 3: Batch Dimension Management\n=============================================================================\n\ud83d\uddbc\ufe0f Single image: torch.Size([3, 224, 224])\n\ud83e\udd16 Model expects: (batch_size, channels, height, width)\n\u2705 After unsqueeze(0): torch.Size([1, 3, 224, 224])\n   Now it looks like a batch of 1 image!\n\ud83c\udfb2 Model output: torch.Size([1, 1000])\n\ud83d\udce4 After squeeze(0): torch.Size([1000]) (back to single sample)\n\n======================================================================\n</pre> In\u00a0[43]: Copied! <pre>print(\"\ud83d\udcca SPECIALIZED SHAPE SORCERY: FLATTEN &amp; UNFLATTEN\")\nprint(\"=\" * 60)\n\n# =============================================================================\n# BASIC FLATTEN OPERATIONS - The Intelligent Collapser\n# =============================================================================\nprint(\"\ud83d\udddc\ufe0f TORCH.FLATTEN() - The Intelligent Collapser\")\nprint(\"-\" * 50)\n\n# Create a 4D tensor like CNN output\ncnn_output = torch.randn(8, 32, 16, 16)  # (batch, channels, height, width)\nprint(f\"\ud83d\uddbc\ufe0f CNN Output: {cnn_output.shape}\")\nprint(f\"   Interpretation: 8 images, 32 feature maps, 16\u00d716 spatial resolution\")\n\n# Different flatten strategies\nprint(f\"\\n\ud83d\udcd0 FLATTEN STRATEGIES:\")\n\n# Strategy 1: Flatten everything (not common in practice)\ncompletely_flat = torch.flatten(cnn_output)\nprint(f\"   flatten() \u2192 {completely_flat.shape}\")\nprint(f\"   Flattened EVERYTHING into 1D vector\")\n\n# Strategy 2: Flatten spatial dimensions only (VERY common!)\nspatial_flat = torch.flatten(cnn_output, start_dim=2)  # Keep batch and channels\nprint(f\"   flatten(start_dim=2) \u2192 {spatial_flat.shape}\")\nprint(f\"   Flattened only spatial dimensions (16\u00d716=256)\")\n\n# Strategy 3: Flatten for linear layer (THE most common!)\nlinear_ready = torch.flatten(cnn_output, start_dim=1)  # Keep only batch dim\nprint(f\"   flatten(start_dim=1) \u2192 {linear_ready.shape}\")\nprint(f\"   Ready for Linear layer! (32\u00d716\u00d716={32*16*16})\")\n\n# Strategy 4: Flatten specific dimension range\nmiddle_flat = torch.flatten(cnn_output, start_dim=1, end_dim=2)  # Flatten channels and height\nprint(f\"   flatten(start_dim=1, end_dim=2) \u2192 {middle_flat.shape}\")\nprint(f\"   Flattened channels and height (32\u00d716=512), kept width\")\n\nprint(f\"\\n\ud83d\udca1 THE CONVENIENCE: No mental math required!\")\nprint(f\"   compare: tensor.view(8, -1) vs tensor.flatten(start_dim=1)\")\nprint(f\"   flatten() automatically calculates the dimensions!\")\n\nprint(\"\\n\" + \"=\" * 60)\n\n# =============================================================================\n# COMPARISON: flatten() vs view() vs reshape()\n# =============================================================================\nprint(\"\u2694\ufe0f FLATTEN vs VIEW vs RESHAPE - The Showdown\")\nprint(\"-\" * 48)\n\ntest_tensor = torch.randn(4, 6, 8, 10)\nprint(f\"Test tensor: {test_tensor.shape}\")\n\n# Method 1: Manual calculation with view\nmanual_calc = 6 * 8 * 10  # Calculate manually\nview_result = test_tensor.view(4, manual_calc)\nprint(f\"view(4, {manual_calc}): {view_result.shape} \u2190 Had to calculate {manual_calc} manually\")\n\n# Method 2: Auto calculation with view\nview_auto = test_tensor.view(4, -1)  \nprint(f\"view(4, -1): {view_auto.shape} \u2190 Let PyTorch calculate, but still need to know structure\")\n\n# Method 3: Intelligent flatten\nflatten_result = test_tensor.flatten(start_dim=1)\nprint(f\"flatten(start_dim=1): {flatten_result.shape} \u2190 Most intuitive! Just say 'flatten from dim 1'\")\n\nprint(f\"\\n\ud83c\udfc6 Winner: flatten() for readability and intent clarity!\")\n\nprint(\"\\n\" + \"=\" * 60)\n</pre> print(\"\ud83d\udcca SPECIALIZED SHAPE SORCERY: FLATTEN &amp; UNFLATTEN\") print(\"=\" * 60)  # ============================================================================= # BASIC FLATTEN OPERATIONS - The Intelligent Collapser # ============================================================================= print(\"\ud83d\udddc\ufe0f TORCH.FLATTEN() - The Intelligent Collapser\") print(\"-\" * 50)  # Create a 4D tensor like CNN output cnn_output = torch.randn(8, 32, 16, 16)  # (batch, channels, height, width) print(f\"\ud83d\uddbc\ufe0f CNN Output: {cnn_output.shape}\") print(f\"   Interpretation: 8 images, 32 feature maps, 16\u00d716 spatial resolution\")  # Different flatten strategies print(f\"\\n\ud83d\udcd0 FLATTEN STRATEGIES:\")  # Strategy 1: Flatten everything (not common in practice) completely_flat = torch.flatten(cnn_output) print(f\"   flatten() \u2192 {completely_flat.shape}\") print(f\"   Flattened EVERYTHING into 1D vector\")  # Strategy 2: Flatten spatial dimensions only (VERY common!) spatial_flat = torch.flatten(cnn_output, start_dim=2)  # Keep batch and channels print(f\"   flatten(start_dim=2) \u2192 {spatial_flat.shape}\") print(f\"   Flattened only spatial dimensions (16\u00d716=256)\")  # Strategy 3: Flatten for linear layer (THE most common!) linear_ready = torch.flatten(cnn_output, start_dim=1)  # Keep only batch dim print(f\"   flatten(start_dim=1) \u2192 {linear_ready.shape}\") print(f\"   Ready for Linear layer! (32\u00d716\u00d716={32*16*16})\")  # Strategy 4: Flatten specific dimension range middle_flat = torch.flatten(cnn_output, start_dim=1, end_dim=2)  # Flatten channels and height print(f\"   flatten(start_dim=1, end_dim=2) \u2192 {middle_flat.shape}\") print(f\"   Flattened channels and height (32\u00d716=512), kept width\")  print(f\"\\n\ud83d\udca1 THE CONVENIENCE: No mental math required!\") print(f\"   compare: tensor.view(8, -1) vs tensor.flatten(start_dim=1)\") print(f\"   flatten() automatically calculates the dimensions!\")  print(\"\\n\" + \"=\" * 60)  # ============================================================================= # COMPARISON: flatten() vs view() vs reshape() # ============================================================================= print(\"\u2694\ufe0f FLATTEN vs VIEW vs RESHAPE - The Showdown\") print(\"-\" * 48)  test_tensor = torch.randn(4, 6, 8, 10) print(f\"Test tensor: {test_tensor.shape}\")  # Method 1: Manual calculation with view manual_calc = 6 * 8 * 10  # Calculate manually view_result = test_tensor.view(4, manual_calc) print(f\"view(4, {manual_calc}): {view_result.shape} \u2190 Had to calculate {manual_calc} manually\")  # Method 2: Auto calculation with view view_auto = test_tensor.view(4, -1)   print(f\"view(4, -1): {view_auto.shape} \u2190 Let PyTorch calculate, but still need to know structure\")  # Method 3: Intelligent flatten flatten_result = test_tensor.flatten(start_dim=1) print(f\"flatten(start_dim=1): {flatten_result.shape} \u2190 Most intuitive! Just say 'flatten from dim 1'\")  print(f\"\\n\ud83c\udfc6 Winner: flatten() for readability and intent clarity!\")  print(\"\\n\" + \"=\" * 60)  <pre>\ud83d\udcca SPECIALIZED SHAPE SORCERY: FLATTEN &amp; UNFLATTEN\n============================================================\n\ud83d\udddc\ufe0f TORCH.FLATTEN() - The Intelligent Collapser\n--------------------------------------------------\n\ud83d\uddbc\ufe0f CNN Output: torch.Size([8, 32, 16, 16])\n   Interpretation: 8 images, 32 feature maps, 16\u00d716 spatial resolution\n\n\ud83d\udcd0 FLATTEN STRATEGIES:\n   flatten() \u2192 torch.Size([65536])\n   Flattened EVERYTHING into 1D vector\n   flatten(start_dim=2) \u2192 torch.Size([8, 32, 256])\n   Flattened only spatial dimensions (16\u00d716=256)\n   flatten(start_dim=1) \u2192 torch.Size([8, 8192])\n   Ready for Linear layer! (32\u00d716\u00d716=8192)\n   flatten(start_dim=1, end_dim=2) \u2192 torch.Size([8, 512, 16])\n   Flattened channels and height (32\u00d716=512), kept width\n\n\ud83d\udca1 THE CONVENIENCE: No mental math required!\n   compare: tensor.view(8, -1) vs tensor.flatten(start_dim=1)\n   flatten() automatically calculates the dimensions!\n\n============================================================\n\u2694\ufe0f FLATTEN vs VIEW vs RESHAPE - The Showdown\n------------------------------------------------\nTest tensor: torch.Size([4, 6, 8, 10])\nview(4, 480): torch.Size([4, 480]) \u2190 Had to calculate 480 manually\nview(4, -1): torch.Size([4, 480]) \u2190 Let PyTorch calculate, but still need to know structure\nflatten(start_dim=1): torch.Size([4, 480]) \u2190 Most intuitive! Just say 'flatten from dim 1'\n\n\ud83c\udfc6 Winner: flatten() for readability and intent clarity!\n\n============================================================\n</pre> In\u00a0[\u00a0]: Copied! <pre># =============================================================================\n# UNFLATTEN - The Dimension Restorer  \n# =============================================================================\nprint(\"\ud83c\udf88 TORCH.UNFLATTEN() - The Dimension Restorer\")\nprint(\"-\" * 48)\n\n# Start with a flattened tensor (common scenario)\nflattened_data = torch.randn(16, 512)  # Maybe output from a linear layer\nprint(f\"\ud83d\udccf Flattened data: {flattened_data.shape}\")\nprint(f\"   Scenario: Output from Linear layer that we want to reshape spatially\")\n\n# Unflatten back to spatial format\n# We want to convert 512 features back to (32, 4, 4) spatial layout\nunflattened = torch.unflatten(flattened_data, dim=1, sizes=(32, 4, 4))\nprint(f\"   unflatten(dim=1, sizes=(32,4,4)) \u2192 {unflattened.shape}\")\nprint(f\"   Restored to (batch, channels, height, width) format!\")\n\n# Verify the math\nprint(f\"   Verification: 32\u00d74\u00d74 = {32*4*4} \u2713\")\n\n# Another example: unflatten different dimensions\nmatrix_data = torch.randn(8, 24)\nprint(f\"\\n\ud83d\udcca Matrix data: {matrix_data.shape}\")\n\n# Unflatten into different structures\nunflatten_1 = torch.unflatten(matrix_data, dim=1, sizes=(6, 4))\nprint(f\"   unflatten(dim=1, sizes=(6,4)) \u2192 {unflatten_1.shape}\")\n\nunflatten_2 = torch.unflatten(matrix_data, dim=1, sizes=(2, 3, 4))\nprint(f\"   unflatten(dim=1, sizes=(2,3,4)) \u2192 {unflatten_2.shape}\")\n\nunflatten_batch = torch.unflatten(matrix_data, dim=0, sizes=(2, 4))\nprint(f\"   unflatten(dim=0, sizes=(2,4)) \u2192 {unflatten_batch.shape}\")\nprint(f\"   Even the batch dimension can be unflattened!\")\n\nprint(\"\\n\" + \"=\" * 60)\n</pre> # ============================================================================= # UNFLATTEN - The Dimension Restorer   # ============================================================================= print(\"\ud83c\udf88 TORCH.UNFLATTEN() - The Dimension Restorer\") print(\"-\" * 48)  # Start with a flattened tensor (common scenario) flattened_data = torch.randn(16, 512)  # Maybe output from a linear layer print(f\"\ud83d\udccf Flattened data: {flattened_data.shape}\") print(f\"   Scenario: Output from Linear layer that we want to reshape spatially\")  # Unflatten back to spatial format # We want to convert 512 features back to (32, 4, 4) spatial layout unflattened = torch.unflatten(flattened_data, dim=1, sizes=(32, 4, 4)) print(f\"   unflatten(dim=1, sizes=(32,4,4)) \u2192 {unflattened.shape}\") print(f\"   Restored to (batch, channels, height, width) format!\")  # Verify the math print(f\"   Verification: 32\u00d74\u00d74 = {32*4*4} \u2713\")  # Another example: unflatten different dimensions matrix_data = torch.randn(8, 24) print(f\"\\n\ud83d\udcca Matrix data: {matrix_data.shape}\")  # Unflatten into different structures unflatten_1 = torch.unflatten(matrix_data, dim=1, sizes=(6, 4)) print(f\"   unflatten(dim=1, sizes=(6,4)) \u2192 {unflatten_1.shape}\")  unflatten_2 = torch.unflatten(matrix_data, dim=1, sizes=(2, 3, 4)) print(f\"   unflatten(dim=1, sizes=(2,3,4)) \u2192 {unflatten_2.shape}\")  unflatten_batch = torch.unflatten(matrix_data, dim=0, sizes=(2, 4)) print(f\"   unflatten(dim=0, sizes=(2,4)) \u2192 {unflatten_batch.shape}\") print(f\"   Even the batch dimension can be unflattened!\")  print(\"\\n\" + \"=\" * 60)","tags":["tensors","module 1","beginner"],"boost":1.2},{"location":"01-tensors/02b_tensor_metamorphosis/#tensor-metamorphosis-shape-shifting-mastery","title":"Tensor Metamorphosis: Shape-Shifting Mastery\u00b6","text":"","tags":["tensors","module 1","beginner"],"boost":1.2},{"location":"01-tensors/02b_tensor_metamorphosis/#professor-torchensteins-grand-directive","title":"Professor Torchenstein's Grand Directive\u00b6","text":"<p>Ah, my brilliant apprentice! Do you feel it? That electric tingle of mastery coursing through your neural pathways? You have learned to slice tensors with surgical precision and fuse them into magnificent constructions! But now... NOW we transcend mere cutting and pasting!</p> <p>Today, we unlock the ultimate power: METAMORPHOSIS! We shall transform the very essence of tensor structure without disturbing a single precious datum within! Think of it as the most elegant magic\u2014changing form while preserving the soul!</p> <p>\"Behold! We shall <code>reshape()</code> reality itself and make dimensions <code>unsqueeze()</code> from the void! The tensors... they will obey our geometric commands!\"</p> <p></p>","tags":["tensors","module 1","beginner"],"boost":1.2},{"location":"01-tensors/02b_tensor_metamorphosis/#your-mission-briefing","title":"Your Mission Briefing\u00b6","text":"<p>By the time you emerge from this metamorphosis chamber, you will command the arcane arts of:</p> <ul> <li>\ud83d\udd04 The Great Reshape &amp; View Metamorphosis: Transform tensor structures with <code>torch.reshape()</code> and <code>torch.view()</code> while understanding memory layout secrets.</li> <li>\ud83d\udddc\ufe0f The Squeeze &amp; Unsqueeze Dimension Dance: Add and remove dimensions of size 1 with surgical precision using <code>squeeze()</code> and <code>unsqueeze()</code>.</li> <li>\ud83d\udcca Specialized Shape Sorcery: Flatten complex structures into submission with <code>torch.flatten()</code> and restore them with <code>torch.unflatten()</code>.</li> </ul> <p>Estimated Time to Completion: 20 minutes of pure shape-shifting enlightenment.</p> <p>What You'll Need:</p> <ul> <li>The wisdom from our previous experiments: tensor summoning and tensor surgery.</li> <li>A willingness to bend reality to your computational will!</li> <li>Your PyTorch laboratory, humming with metamorphic potential.</li> </ul>","tags":["tensors","module 1","beginner"],"boost":1.2},{"location":"01-tensors/02b_tensor_metamorphosis/#part-1-memory-layout-foundations","title":"Part 1: Memory Layout Foundations \ud83e\uddf1\u00b6","text":"","tags":["tensors","module 1","beginner"],"boost":1.2},{"location":"01-tensors/02b_tensor_metamorphosis/#the-deep-theory-behind-memory-layout-magic","title":"The Deep Theory Behind Memory Layout Magic\u00b6","text":"<p>Ah, my curious apprentice! To truly master tensor metamorphosis, you must understand the fundamental secret that lies beneath: how tensors live in your computer's memory! This knowledge will separate you from the mere code-monkeys and elevate you to the ranks of true PyTorch sorcerers!</p> <p>The Universal Truth: Everything is a 1D Array! \ud83d\udccf It is just a long, sequential line of storage locations:</p> <pre><code>Computer Memory (Always 1D):\n[addr_0][addr_1][addr_2][addr_3][addr_4][addr_5][addr_6][addr_7]...\n</code></pre> <p>The Multi-Dimensional Illusion: When we have a \"2D tensor\" or \"3D tensor,\" it's really just our interpretation of how to read this 1D memory! The computer doesn't care about rows and columns\u2014that's just how WE choose to organize and access the data.</p>","tags":["tensors","module 1","beginner"],"boost":1.2},{"location":"01-tensors/02b_tensor_metamorphosis/#row-major-vs-column-major-the-ancient-battle","title":"Row-Major vs Column-Major: The Ancient Battle! \u2694\ufe0f\u00b6","text":"<p>There are two ways to store multi-dimensional data in this 1D memory:</p> <p>\ud83c\udde8 Row-Major (C-style) - PyTorch's Choice: Store data row by row, left to right, then move to the next row.</p> <p>\ud83c\uddeb Column-Major (Fortran-style): Store data column by column, top to bottom, then move to the next column.</p> <p>Let's visualize this with a 3\u00d74 matrix containing numbers 1-12:</p> <pre><code>Visual Matrix:\n[ 1  2  3  4]\n[ 5  6  7  8]  \n[ 9 10 11 12]\n\nRow-Major Memory Layout (PyTorch default):\nMemory: [1][2][3][4][5][6][7][8][9][10][11][12]\n        \u2514\u2500  row1  \u2500\u2518\u2514\u2500  row2  \u2500\u2518\u2514\u2500   row3   \u2500\u2500\u2518\n\nColumn-Major Memory Layout (Not PyTorch):\nMemory: [1][5][9][2][6][10][3][7][11][4][8][12]\n        \u2514 col1  \u2518\u2514 col2   \u2518\u2514\u2500 col3 \u2500\u2518\u2514\u2500 col4 \u2500\u2518\n</code></pre> <p>PyTorch uses Row-Major because it's the standard for C/C++ and most modern systems! This is not dependent on your OS or hardware\u2014it's a software design choice.</p>","tags":["tensors","module 1","beginner"],"boost":1.2},{"location":"01-tensors/02b_tensor_metamorphosis/#what-makes-memory-contiguous","title":"What Makes Memory \"Contiguous\"? \ud83e\udde9\u00b6","text":"<p>Contiguous Memory access: You try to read the tensor's elements in the expected sequential order in the 1D memory array.</p> <p>Non-Contiguous Memory access: You try to get the tensor's elements which are scattered\u2014they exist in memory but not in the order you'd expect when reading row by row.</p>","tags":["tensors","module 1","beginner"],"boost":1.2},{"location":"01-tensors/02b_tensor_metamorphosis/#the-transpose-tragedy-why-memory-becomes-non-contiguous","title":"The Transpose Tragedy - Why Memory Becomes Non-Contiguous\u00b6","text":"<p>Let's witness the moment when contiguous memory becomes scattered:</p> <pre><code>Original 3\u00d74 Tensor (Contiguous):\nVisual:           Memory Layout:\n[ 1  2  3  4]     [1][2][3][4][5][6][7][8][9][10][11][12]\n[ 5  6  7  8]  \u2192  \n[ 9 10 11 12]    \n\nAfter Transpose to 4\u00d73 (Non-Contiguous):\nVisual:          Expected Memory for New Shape:\n[ 1  5  9]       [1][5][9][2][6][10][3][7][11][4][8][12]\n[ 2  6 10]  \n[ 3  7 11]       But ACTUAL memory is still:\n[ 4  8 12]       [1][2][3][4][5][6][7][8][9][10][11][12]\n</code></pre> <p>The Problem: To read row 1 of the transposed tensor <code>[1, 5, 9]</code>, PyTorch must jump around in memory: address 0 \u2192 address 4 \u2192 address 8. This \"jumping around\" makes it non-contiguous!</p>","tags":["tensors","module 1","beginner"],"boost":1.2},{"location":"01-tensors/02b_tensor_metamorphosis/#pytorchs-memory-management-system","title":"PyTorch's Memory Management System \ud83c\udfed\u00b6","text":"<p>Now that you understand how memory is fundamentally organized, prepare to witness PyTorch's DIABOLICAL system for managing that memory! This is where the magic happens, my apprentice\u2014where PyTorch transforms from a simple library into a memory manipulation GENIUS!</p>","tags":["tensors","module 1","beginner"],"boost":1.2},{"location":"01-tensors/02b_tensor_metamorphosis/#the-trinity-of-tensor-existence-storage-data-pointers-and-views","title":"\ud83e\udde0 The Trinity of Tensor Existence - Storage, Data Pointers, and Views\u00b6","text":"<p>PyTorch has crafted an elegant three-tier system to encapsulate how tensor data lives, breathes, and transforms in memory. Understanding this trinity will separate you from the memory-blind masses forever!</p> <p>\ud83c\udfad <code>tensor</code> - The Mask of Interpretation</p> <ul> <li>What it REALLY is: Your personal window into the memory abyss! A tensor is merely an interpretation layer that can represent the entire memory buffer, a clever view of it, or just a slice of the underlying numerical reality.</li> <li>The Secret: Multiple tensors can wear different masks while peering into the SAME underlying memory vault!</li> </ul> <p>\ud83d\udce6 <code>tensor.storage()</code> - The Memory Vault Master</p> <ul> <li>What it is: PyTorch's high-level Storage object\u2014the supreme overlord that commands the actual data buffer in the memory depths!</li> <li>When shared: Multiple tensor minions can pledge allegiance to the same Storage master, but each can gaze upon different regions of its domain (like examining different rows of the same data matrix)</li> <li>Think of it as: The entire memory palace that hoards all your numerical treasures, while individual tensors are merely different keys to access various chambers within!</li> </ul> <p>\ud83c\udfaf <code>tensor.data_ptr()</code> - The Exact Memory Coordinates</p> <ul> <li>What it is: The raw memory address (a cold, hard integer) that points to the EXACT byte where this particular tensor's data journey begins in the vast memory ocean!</li> <li>When different: When tensors are views gazing upon different territories of the same memory kingdom (like viewing different slices of the same storage empire)</li> <li>Think of it as: The precise GPS coordinates within the memory warehouse\u2014while <code>.storage()</code> tells you which warehouse, <code>.data_ptr()</code> tells you the exact shelf, row, and position!</li> </ul> <p>\u26a1 The Torchenstein Memory Hierarchy:</p> <pre><code>\ud83c\udff0 Computer Memory (The Kingdom)\n  \u2514\u2500\u2500 \ud83d\udce6 Storage Object (The Memory Palace)  \n      \u251c\u2500\u2500 \ud83c\udfaf data_ptr() #1 (Throne Room) \u2190 tensor_a points here\n      \u251c\u2500\u2500 \ud83c\udfaf data_ptr() #2 (Armory) \u2190 tensor_b[10:] points here  \n      \u2514\u2500\u2500 \ud83c\udfaf data_ptr() #3 (Treasury) \u2190 tensor_c.view(...) points here\n</code></pre> <p>\ud83d\udca1 The Memory Sharing Conspiracy Matrix:</p> Scenario Same Storage? Same data_ptr? What's Really Happening Example True Copy \u274c No \u274c No Complete independence\u2014separate kingdoms! <code>tensor.clone()</code> Shape Change \u2705 Yes \u2705 Yes Same palace, same throne room, different interpretation <code>tensor.reshape(3,4)</code> Slice View \u2705 Yes \u274c No Same palace, different room within it <code>tensor[2:]</code> <p>The ultimate truth: PyTorch's genius lies in maximizing memory sharing while maintaining the illusion of independence! Mwahahaha!</p> <p>Let's witness this diabolical PyTorch memory system in action and see the conspiracy unfold!</p>","tags":["tensors","module 1","beginner"],"boost":1.2},{"location":"01-tensors/02b_tensor_metamorphosis/#torchview-the-memory-efficient-shape-changer","title":"torch.view() - The Memory-Efficient Shape Changer \ud83d\udc41\ufe0f\u00b6","text":"<p>Now that we understand WHY we need shape transformation, let's master the first tool: <code>torch.view()</code>!</p>","tags":["tensors","module 1","beginner"],"boost":1.2},{"location":"01-tensors/02b_tensor_metamorphosis/#what-is-torchview-and-what-is-it-for","title":"\ud83c\udfaf What is torch.view() and What is it FOR?\u00b6","text":"<p><code>torch.view()</code> is PyTorch's memory-efficient shape transformation method. It creates a new tensor with a different shape that shares the same underlying data as the original tensor.</p> <p>\ud83d\ude80 Use <code>view()</code> when:</p> <ul> <li>You want maximum performance (no data copying)</li> <li>You know your tensor has contiguous memory layout</li> <li>You need guaranteed memory sharing (changes to one tensor affect the other)</li> </ul> <p>\u26a0\ufe0f Limitations:</p> <ul> <li>Requires contiguous memory - fails if memory is scattered</li> <li>Throws error rather than automatically fixing problems</li> <li>Purist approach - no fallback mechanisms</li> </ul>","tags":["tensors","module 1","beginner"],"boost":1.2},{"location":"01-tensors/02b_tensor_metamorphosis/#how-view-works-the-shape-mathematics","title":"\ud83d\udcd0 How view() Works: The Shape Mathematics\u00b6","text":"<p>The Golden Rule: Total elements must remain constant!</p> <pre><code>Original shape: (A, B, C, D)  \u2192 Total elements: A \u00d7 B \u00d7 C \u00d7 D\nNew shape:      (W, X, Y, Z)  \u2192 Total elements: W \u00d7 X \u00d7 Y \u00d7 Z\n\nValid only if: A \u00d7 B \u00d7 C \u00d7 D = W \u00d7 X \u00d7 Y \u00d7 Z\n</code></pre> <p>\ud83d\udd22 The Magic <code>-1</code> Parameter: Use <code>-1</code> in one dimension to let PyTorch calculate it automatically:</p> <pre>tensor.view(batch_size, -1)  # PyTorch figures out the second dimension\n</pre> <p>Let's see <code>view()</code> in action with real examples!</p>","tags":["tensors","module 1","beginner"],"boost":1.2},{"location":"01-tensors/02b_tensor_metamorphosis/#torchreshape-the-diplomatic-shape-changer","title":"torch.reshape() - The Diplomatic Shape Changer \ud83e\udd1d\u00b6","text":"<p>Now let's master <code>torch.reshape()</code> - the more forgiving, intelligent cousin of <code>view()</code>!</p>","tags":["tensors","module 1","beginner"],"boost":1.2},{"location":"01-tensors/02b_tensor_metamorphosis/#what-is-torchreshape-and-what-is-it-for","title":"\ud83c\udfaf What is torch.reshape() and What is it FOR?\u00b6","text":"<p><code>torch.reshape()</code> is PyTorch's diplomatic shape transformation method. It tries to return a view when possible, but creates a copy when necessary to ensure the operation always succeeds.</p> <p>\ud83e\udd1d Use <code>reshape()</code> when:</p> <ul> <li>You want reliability over maximum performance</li> <li>You're not sure if your tensor memory is contiguous</li> <li>You want PyTorch to handle memory layout automatically</li> <li>You're prototyping and want to avoid memory errors</li> </ul> <p>\u2705 Advantages:</p> <ul> <li>Always succeeds (if the shape math is valid)</li> <li>Automatically handles contiguous vs non-contiguous memory</li> <li>Beginner-friendly - less likely to cause frustrating errors</li> <li>Smart fallback - returns view when possible, copy when necessary</li> </ul> <p>\u26a0\ufe0f Trade-offs:</p> <ul> <li>Less predictable performance - you don't know if it creates a copy</li> <li>Potentially slower than <code>view()</code> in some cases</li> <li>Less explicit about memory sharing</li> </ul>","tags":["tensors","module 1","beginner"],"boost":1.2},{"location":"01-tensors/02b_tensor_metamorphosis/#reshape-vs-view-when-to-use-which","title":"\ud83d\udcca reshape() vs view() - When to Use Which?\u00b6","text":"Scenario Use <code>view()</code> Use <code>reshape()</code> Performance critical \u2705 Guaranteed no copying \u274c Might copy data Beginner-friendly \u274c Can throw errors \u2705 Always works Prototyping \u274c Interrupts workflow \u2705 Smooth development Production code \u2705 Predictable behavior \u26a0\ufe0f Less predictable Memory sharing required \u2705 Guaranteed sharing \u26a0\ufe0f Depends on layout <p>Let's see how <code>reshape()</code> handles the scenarios where <code>view()</code> fails!</p>","tags":["tensors","module 1","beginner"],"boost":1.2},{"location":"01-tensors/02b_tensor_metamorphosis/#the-element-flow-mystery-how-tensors-rearrange-themselves","title":"\ud83e\udde9 The Element Flow Mystery: How Tensors Rearrange Themselves\u00b6","text":"<p>THE CRUCIAL QUESTION: When you transform a tensor from one shape to another, exactly HOW do the elements flow into their new positions? This is where many apprentices stumble\u2014they understand the math (<code>6\u00d78 = 48 = 2\u00d73\u00d78</code>) but don't visualize the element migration patterns!</p> <p>Fear not! Professor Torchenstein shall illuminate this dark mystery with surgical precision! Understanding element flow is THE difference between tensor confusion and tensor mastery!</p>","tags":["tensors","module 1","beginner"],"boost":1.2},{"location":"01-tensors/02b_tensor_metamorphosis/#the-row-major-flow-principle","title":"\ud83d\udd0d The Row-Major Flow Principle\u00b6","text":"<p>Remember our fundamental truth: PyTorch always reads and writes elements in row-major order\u2014left to right, then top to bottom, like reading English text!</p> <p>The Sacred Rule: Elements always flow in this order: <code>[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11...]</code></p> <p>No matter what shape transformation you perform, elements maintain their reading order but get reinterpreted into new dimensional coordinates.</p>","tags":["tensors","module 1","beginner"],"boost":1.2},{"location":"01-tensors/02b_tensor_metamorphosis/#real-world-neural-network-shape-challenges","title":"\ud83d\ude80 Real-World Neural Network Shape Challenges\u00b6","text":"<p>Now that you understand how elements flow, let's tackle the exact scenarios where neural network engineers use <code>view()</code> and <code>reshape()</code> every single day! These are the problems that can ONLY be solved with shape transformations (not permutation or other operations).</p>","tags":["tensors","module 1","beginner"],"boost":1.2},{"location":"01-tensors/02b_tensor_metamorphosis/#challenge-1-cnn-feature-maps-linear-layer","title":"\ud83d\udca1 Challenge 1: CNN Feature Maps \u2192 Linear Layer\u00b6","text":"<p>The Problem: You've extracted features from images using CNN layers, but now you need to feed them into a fully connected (Linear) layer for classification. The shapes are incompatible!</p>","tags":["tensors","module 1","beginner"],"boost":1.2},{"location":"01-tensors/02b_tensor_metamorphosis/#challenge-2-multi-head-attention-setup","title":"\u26a1 Challenge 2: Multi-Head Attention Setup\u00b6","text":"<p>The Problem: You have embeddings for a batch of text sequences, but you need to split the embedding dimension into multiple attention heads for parallel processing.</p> <p>Let's solve these with code and see exactly how the transformations work:</p>","tags":["tensors","module 1","beginner"],"boost":1.2},{"location":"01-tensors/02b_tensor_metamorphosis/#key-takeaways-when-to-use-viewreshape-in-neural-networks","title":"\ud83c\udfaf Key Takeaways: When to Use view()/reshape() in Neural Networks\u00b6","text":"<p>\u2705 Perfect for view()/reshape():</p> <ul> <li>CNN \u2192 Linear: Flattening spatial dimensions <code>(B, C, H, W)</code> \u2192 <code>(B, C\u00d7H\u00d7W)</code></li> <li>Multi-head attention: Splitting features <code>(B, S, E)</code> \u2192 <code>(B, S, H, E/H)</code></li> <li>Batch reshaping: Organizing data <code>(N\u00d7F)</code> \u2192 <code>(B, N/B, F)</code></li> <li>Any scenario where total elements stay the same and no dimension reordering is needed</li> </ul> <p>\u274c NOT suitable for view()/reshape():</p> <ul> <li>Dimension reordering: <code>(H, W, C)</code> \u2192 <code>(C, H, W)</code> (use <code>permute()</code> or <code>transpose()</code>)</li> <li>Broadcasting preparation: Adding singleton dimensions (use <code>unsqueeze()</code>)</li> <li>Changing data layout: Converting between different memory formats</li> </ul> <p>\ud83e\udde0 Remember the Golden Rules:</p> <ol> <li>Total elements must match: <code>original.numel() == reshaped.numel()</code></li> <li>Element flow follows row-major order: Last dimension fills first</li> <li>Memory is shared: Changes to original affect all views</li> <li>Use <code>-1</code> for automatic calculation: Let PyTorch figure out one dimension</li> </ol> <p>You now possess the complete knowledge of tensor shape transformation! These patterns appear in every modern neural network architecture. \ud83d\ude80</p>","tags":["tensors","module 1","beginner"],"boost":1.2},{"location":"01-tensors/02b_tensor_metamorphosis/#the-squeeze-unsqueeze-dimension-dance","title":"\ud83d\udddc\ufe0f The Squeeze &amp; Unsqueeze Dimension Dance\u00b6","text":"<p>Welcome to the most elegant manipulation in all of tensor sorcery! While <code>view()</code> and <code>reshape()</code> rearrange elements, <code>squeeze()</code> and <code>unsqueeze()</code> perform a completely different kind of magic\u2014they add and remove dimensions of size 1 without touching a single element!</p> <p>Think of it as dimensional origami\u2014folding and unfolding the very fabric of tensor space while keeping every number exactly where it is!</p>","tags":["tensors","module 1","beginner"],"boost":1.2},{"location":"01-tensors/02b_tensor_metamorphosis/#the-dimensional-masqueraders-what-are-size-1-dimensions","title":"\ud83c\udfad The Dimensional Masqueraders: What Are Size-1 Dimensions?\u00b6","text":"<p>Size-1 dimensions are like invisible spacers in tensor shapes\u2014they don't contain extra data, but they change how PyTorch interprets the structure:</p> <pre>tensor_1d = torch.tensor([1, 2, 3, 4])     # Shape: (4,)        - 1D tensor\ntensor_2d = torch.tensor([[1, 2, 3, 4]])   # Shape: (1, 4)      - 2D tensor (1 row)\ntensor_3d = torch.tensor([[[1, 2, 3, 4]]]) # Shape: (1, 1, 4)   - 3D tensor (1\u00d71\u00d74)\n</pre> <p>Same 4 numbers, completely different dimensional personalities! This is where squeeze/unsqueeze become your dimensional choreographers.</p>","tags":["tensors","module 1","beginner"],"boost":1.2},{"location":"01-tensors/02b_tensor_metamorphosis/#the-twin-operations","title":"\ud83d\udd0d The Twin Operations\u00b6","text":"<p><code>squeeze()</code> - The Dimension Destroyer \ud83d\udddc\ufe0f</p> <ul> <li>Removes dimensions of size 1</li> <li>Makes tensors \"smaller\" dimensionally (but same data)</li> <li><code>(1, 4, 1, 3)</code> \u2192 <code>(4, 3)</code> (removes the size-1 dimensions)</li> <li>Important: <code>squeeze(dim)</code> on non-size-1 dimensions does nothing (no error!)</li> </ul> <p><code>unsqueeze()</code> - The Dimension Creator \ud83c\udf88</p> <ul> <li>Adds dimensions of size 1 at specified positions</li> <li>Makes tensors \"bigger\" dimensionally (but same data)</li> <li><code>(4, 3)</code> \u2192 <code>(1, 4, 1, 3)</code> (adds size-1 dimensions where you specify)</li> </ul> <p>Let's witness this dimensional dance in action!</p>","tags":["tensors","module 1","beginner"],"boost":1.2},{"location":"01-tensors/02b_tensor_metamorphosis/#real-world-neural-network-applications","title":"\ud83d\ude80 Real-World Neural Network Applications\u00b6","text":"<p>Now for the moment of truth! When do neural network engineers reach for <code>squeeze()</code> and <code>unsqueeze()</code>? These operations are essential for making tensors compatible with different layers and operations. Let me reveal the most common scenarios:</p> <p>\ud83c\udfaf Application 1: Broadcasting Preparation Many operations require specific dimensional structures for broadcasting. <code>unsqueeze()</code> adds the necessary dimensions.</p> <p>\ud83c\udfaf Application 2: Layer Compatibility Different neural network layers expect different dimensional formats. <code>squeeze()</code> and <code>unsqueeze()</code> bridge these gaps.</p> <p>\ud83c\udfaf Application 3: Batch Dimension Management Adding and removing batch dimensions when switching between single samples and batches.</p> <p>\ud83c\udfaf Application 4: Loss Function Requirements Many loss functions expect specific shapes\u2014these operations ensure compatibility.</p> <p>Let's solve real problems that every PyTorch practitioner encounters!</p>","tags":["tensors","module 1","beginner"],"boost":1.2},{"location":"01-tensors/02b_tensor_metamorphosis/#specialized-shape-sorcery-flatten-unflatten","title":"\ud83d\udcca Specialized Shape Sorcery: Flatten &amp; Unflatten\u00b6","text":"<p>The final metamorphosis in our arsenal! While <code>view()</code> and <code>reshape()</code> require you to calculate dimensions manually, <code>torch.flatten()</code> and <code>torch.unflatten()</code> are intelligent specialists that handle common patterns automatically.</p> <p>Think of them as smart assistants that understand exactly what you want to achieve without forcing you to do the dimensional mathematics!</p>","tags":["tensors","module 1","beginner"],"boost":1.2},{"location":"01-tensors/02b_tensor_metamorphosis/#the-flattening-specialists","title":"\ud83c\udfaf The Flattening Specialists\u00b6","text":"<p><code>torch.flatten()</code> - The Intelligent Collapser \ud83d\udddc\ufe0f\ud83d\udccf</p> <ul> <li>Automatically flattens specified dimensions into a single dimension</li> <li>No manual calculation needed - PyTorch figures out the math</li> <li>Common neural network pattern: Multi-dimensional \u2192 1D for linear layers</li> <li>Flexible control: Flatten specific dimension ranges, not just everything</li> </ul> <p><code>torch.unflatten()</code> - The Dimension Restorer \ud83c\udf88\ud83d\udd27</p> <ul> <li>Reverses the flattening operation intelligently</li> <li>Restores specific dimensional structure from flattened data</li> <li>Perfect for converting 1D outputs back to multi-dimensional formats</li> <li>Complements flatten() for round-trip transformations</li> </ul> <p>\u2705 Core Mastery:</p> <ul> <li><code>torch.flatten(start_dim, end_dim)</code>: Intelligent dimensional collapse without manual calculations</li> <li><code>torch.unflatten(dim, sizes)</code>: Smart restoration of multi-dimensional structure</li> </ul>","tags":["tensors","module 1","beginner"],"boost":1.2},{"location":"01-tensors/02b_tensor_metamorphosis/#why-these-matter-in-neural-networks","title":"\ud83e\udde0 Why These Matter in Neural Networks\u00b6","text":"<p>The Classic Problem: CNNs output <code>(batch, channels, height, width)</code> but Linear layers need <code>(batch, features)</code>. Instead of manually calculating <code>channels \u00d7 height \u00d7 width</code>, just use <code>flatten(start_dim=1)</code>!</p> <p>The Restoration Problem: Sometimes you need to convert flattened outputs back to spatial formats (like for visualization or further processing).</p> <p>Let's see these intelligent operations in action!</p>","tags":["tensors","module 1","beginner"],"boost":1.2},{"location":"01-tensors/02b_tensor_metamorphosis/#professor-torchensteins-metamorphosis-outro","title":"Professor Torchenstein's Metamorphosis Outro \ud83c\udfad\u26a1\u00b6","text":"<p>SPECTACULAR! ABSOLUTELY MAGNIFICENT! My brilliant apprentice, you have not merely learned tensor operations\u2014you have undergone a complete metamorphosis into a true shape-shifting master!</p> <p>You now think like PyTorch itself! You understand the fundamental principles that govern ALL tensor operations. Every neural network you encounter, every research paper you read, every model you build\u2014you'll see the tensor metamorphosis patterns we've mastered today.</p> <p>CNN architectures? Child's play\u2014you know exactly how spatial dimensions collapse into linear layers! Transformer attention? Elementary\u2014you understand how embeddings split into multiple heads!</p>","tags":["tensors","module 1","beginner"],"boost":1.2},{"location":"01-tensors/02b_tensor_metamorphosis/#a-final-word-of-wisdom","title":"\ud83c\udfad A Final Word of Wisdom\u00b6","text":"<p>Remember this moment, my dimensional apprentice. Today, you transcended the limitations of static thinking and embraced the fluid, dynamic nature of tensor reality. You learned that form is temporary, but data is eternal\u2014and with the right metamorphic incantations, any shape can become any other shape!</p> <p>The tensors... they will obey your geometric commands! The gradients... they will flow through your transformations! And the neural networks... they will bend to your architectural will!</p> <p>Until our paths cross again in the halls of computational glory... keep your learning rates high and your dimensions fluid!</p> <p>Mwahahahahaha! \u26a1\ud83e\uddea\ud83d\udd2c</p>      Your browser does not support the video tag.","tags":["tensors","module 1","beginner"],"boost":1.2},{"location":"01-tensors/03_data_types_and_devices/","title":"DTypes &amp; Devices: Choose Your Weapons","text":"In\u00a0[28]: Copied! <pre>import torch\n\n# A single number for our comparison\npi_number = torch.pi\n\n# Summoning tensors of different float dtypes\ntensor_fp64 = torch.tensor(pi_number, dtype=torch.float64)\ntensor_fp32 = torch.tensor(pi_number, dtype=torch.float32)\ntensor_fp16 = torch.tensor(pi_number, dtype=torch.float16)\ntensor_bf16 = torch.tensor(pi_number, dtype=torch.bfloat16)\n\n\n\nprint(\"--- Floating-Point digits after decimal point and Memory Footprints ---\")\nprint(f\"{tensor_fp64.dtype}:  {tensor_fp64.item():.10f} | Memory: {tensor_fp64.element_size()} bytes \")\nprint(f\"{tensor_fp32.dtype}:  {tensor_fp32.item():.10f} | Memory: {tensor_fp32.element_size()} bytes\")\nprint(f\"{tensor_fp16.dtype}:  {tensor_fp16.item():.10f} | Memory: {tensor_fp16.element_size()} bytes (Half the size of fp32!)\")\nprint(f\"{tensor_bf16.dtype}: {tensor_bf16.item():.10f} | Memory: {tensor_bf16.element_size()} bytes (Half the size of fp32!)\")\n</pre> import torch  # A single number for our comparison pi_number = torch.pi  # Summoning tensors of different float dtypes tensor_fp64 = torch.tensor(pi_number, dtype=torch.float64) tensor_fp32 = torch.tensor(pi_number, dtype=torch.float32) tensor_fp16 = torch.tensor(pi_number, dtype=torch.float16) tensor_bf16 = torch.tensor(pi_number, dtype=torch.bfloat16)    print(\"--- Floating-Point digits after decimal point and Memory Footprints ---\") print(f\"{tensor_fp64.dtype}:  {tensor_fp64.item():.10f} | Memory: {tensor_fp64.element_size()} bytes \") print(f\"{tensor_fp32.dtype}:  {tensor_fp32.item():.10f} | Memory: {tensor_fp32.element_size()} bytes\") print(f\"{tensor_fp16.dtype}:  {tensor_fp16.item():.10f} | Memory: {tensor_fp16.element_size()} bytes (Half the size of fp32!)\") print(f\"{tensor_bf16.dtype}: {tensor_bf16.item():.10f} | Memory: {tensor_bf16.element_size()} bytes (Half the size of fp32!)\")   <pre>--- Floating-Point digits after decimal point and Memory Footprints ---\ntorch.float64:  3.1415926536 | Memory: 8 bytes \ntorch.float32:  3.1415927410 | Memory: 4 bytes\ntorch.float16:  3.1406250000 | Memory: 2 bytes (Half the size of fp32!)\ntorch.bfloat16: 3.1406250000 | Memory: 2 bytes (Half the size of fp32!)\n</pre> In\u00a0[29]: Copied! <pre># details about floats\nprint(\"--- Details about floats ---\")\n# Let's print out torch.finfo attributes in a beautiful aligned table\ndef print_finfo(dtype):\n    finfo = torch.finfo(dtype)\n    print(f\"{str(dtype):&lt;14} | {finfo.bits:&lt;4} | {finfo.eps:&lt;10.4e} | {finfo.tiny:&lt;12.4e} | {finfo.min:&lt;12.4e} | {finfo.max:&lt;12.4e}\")\n\nprint(f\"{'dtype':&lt;14} | {'Bits':&lt;4} | {'Epsilon':&lt;10} | {'Tiny':&lt;12} | {'Min':&lt;12} | {'Max':&lt;12}\")\nprint(\"-\" * 80)\nfor dtype in [torch.float64, torch.float32, torch.float16, torch.bfloat16]:\n    print_finfo(dtype)\n</pre> # details about floats print(\"--- Details about floats ---\") # Let's print out torch.finfo attributes in a beautiful aligned table def print_finfo(dtype):     finfo = torch.finfo(dtype)     print(f\"{str(dtype):&lt;14} | {finfo.bits:&lt;4} | {finfo.eps:&lt;10.4e} | {finfo.tiny:&lt;12.4e} | {finfo.min:&lt;12.4e} | {finfo.max:&lt;12.4e}\")  print(f\"{'dtype':&lt;14} | {'Bits':&lt;4} | {'Epsilon':&lt;10} | {'Tiny':&lt;12} | {'Min':&lt;12} | {'Max':&lt;12}\") print(\"-\" * 80) for dtype in [torch.float64, torch.float32, torch.float16, torch.bfloat16]:     print_finfo(dtype) <pre>--- Details about floats ---\ndtype          | Bits | Epsilon    | Tiny         | Min          | Max         \n--------------------------------------------------------------------------------\ntorch.float64  | 64   | 2.2204e-16 | 2.2251e-308  | -1.7977e+308 | 1.7977e+308 \ntorch.float32  | 32   | 1.1921e-07 | 1.1755e-38   | -3.4028e+38  | 3.4028e+38  \ntorch.float16  | 16   | 9.7656e-04 | 6.1035e-05   | -6.5504e+04  | 6.5504e+04  \ntorch.bfloat16 | 16   | 7.8125e-03 | 1.1755e-38   | -3.3895e+38  | 3.3895e+38  \n</pre> <p>Lets test the range of floats and see what happens when we push the limits. What is the maximum number that can be represented in each <code>dtype</code>, and what happens when we go beyond that?</p> In\u00a0[30]: Copied! <pre># A large number for our comparison\n\nlarge_number = 70000.0\ntensor_fp64 = torch.tensor(large_number, dtype=torch.float64)\ntensor_fp32 = torch.tensor(large_number, dtype=torch.float32)\ntensor_fp16 = torch.tensor(large_number, dtype=torch.float16)\ntensor_bf16 = torch.tensor(large_number, dtype=torch.bfloat16)\n\nprint(\"--- Floating-Point Higher range, precision loss ---\")\nprint(f\"{tensor_fp64.dtype}:  {tensor_fp64.item():.10f} \")\nprint(f\"{tensor_fp32.dtype}:  {tensor_fp32.item():.10f} \")\nprint(f\"{tensor_fp16.dtype}:  {tensor_fp16.item():.10f} \")\nprint(f\"{tensor_bf16.dtype}: {tensor_bf16.item():.10f} \")\n</pre> # A large number for our comparison  large_number = 70000.0 tensor_fp64 = torch.tensor(large_number, dtype=torch.float64) tensor_fp32 = torch.tensor(large_number, dtype=torch.float32) tensor_fp16 = torch.tensor(large_number, dtype=torch.float16) tensor_bf16 = torch.tensor(large_number, dtype=torch.bfloat16)  print(\"--- Floating-Point Higher range, precision loss ---\") print(f\"{tensor_fp64.dtype}:  {tensor_fp64.item():.10f} \") print(f\"{tensor_fp32.dtype}:  {tensor_fp32.item():.10f} \") print(f\"{tensor_fp16.dtype}:  {tensor_fp16.item():.10f} \") print(f\"{tensor_bf16.dtype}: {tensor_bf16.item():.10f} \") <pre>--- Floating-Point Higher range, precision loss ---\ntorch.float64:  70000.0000000000 \ntorch.float32:  70000.0000000000 \ntorch.float16:  inf \ntorch.bfloat16: 70144.0000000000 \n</pre> In\u00a0[31]: Copied! <pre>almost_max_number = 65500.0\n\nnearest_max_number = 65500.0 + torch.pi\nprint(f\"----\\nTesting {almost_max_number} + {torch.pi} = {nearest_max_number} &lt; 65504 (max for float16)\")\ntensor_fp64 = torch.tensor(nearest_max_number, dtype=torch.float64)\ntensor_fp32 = torch.tensor(nearest_max_number, dtype=torch.float32)\ntensor_fp16 = torch.tensor(nearest_max_number, dtype=torch.float16)\ntensor_bf16 = torch.tensor(nearest_max_number, dtype=torch.bfloat16)\n\nprint(f\"{tensor_fp64.dtype}:  {tensor_fp64.item():.10f} \")\nprint(f\"{tensor_fp32.dtype}:  {tensor_fp32.item():.10f} \")\nprint(f\"{tensor_fp16.dtype}:  {tensor_fp16.item():.10f} - in range of float16, but precision is lost\")\nprint(f\"{tensor_bf16.dtype}: {tensor_bf16.item():.10f} - in range of bfloat16, precision is lost more than float16\")\n\n\nfloat16_max = 65504.0\nadd_to_max = 1.0\noverflow_number = float16_max + add_to_max\nprint(f\"----\\nTesting {float16_max} + {add_to_max} = {overflow_number} &gt; 65504 (max for float16) it should overflow?\")\ntensor_fp16 = torch.tensor(overflow_number, dtype=torch.float16)\ntensor_bf16 = torch.tensor(overflow_number, dtype=torch.bfloat16)\n\n\nprint(f\"{tensor_fp16.dtype}:  {tensor_fp16.item():.10f} - could you explain why it is not inf?\")\nprint(f\"{tensor_bf16.dtype}: {tensor_bf16.item():.10f} - in range of bfloat16, precision is lost more than float16\")\n\nadd_to_max = 16.0\noverflow_number = float16_max + add_to_max\nprint(f\"----\\nTesting {float16_max} + {add_to_max} = {overflow_number} &gt; 65504 (max for float16) it should overflow?\")\n\ntensor_fp16 = torch.tensor(overflow_number, dtype=torch.float16)\ntensor_bf16 = torch.tensor(overflow_number, dtype=torch.bfloat16)\n\n\nprint(f\"{tensor_fp16.dtype}:  {tensor_fp16.item():.10f} - could you explain why this is inf?\")\nprint(f\"{tensor_bf16.dtype}: {tensor_bf16.item():.10f} - in range of bfloat16, precision is lost more than float16\")\n</pre> almost_max_number = 65500.0  nearest_max_number = 65500.0 + torch.pi print(f\"----\\nTesting {almost_max_number} + {torch.pi} = {nearest_max_number} &lt; 65504 (max for float16)\") tensor_fp64 = torch.tensor(nearest_max_number, dtype=torch.float64) tensor_fp32 = torch.tensor(nearest_max_number, dtype=torch.float32) tensor_fp16 = torch.tensor(nearest_max_number, dtype=torch.float16) tensor_bf16 = torch.tensor(nearest_max_number, dtype=torch.bfloat16)  print(f\"{tensor_fp64.dtype}:  {tensor_fp64.item():.10f} \") print(f\"{tensor_fp32.dtype}:  {tensor_fp32.item():.10f} \") print(f\"{tensor_fp16.dtype}:  {tensor_fp16.item():.10f} - in range of float16, but precision is lost\") print(f\"{tensor_bf16.dtype}: {tensor_bf16.item():.10f} - in range of bfloat16, precision is lost more than float16\")   float16_max = 65504.0 add_to_max = 1.0 overflow_number = float16_max + add_to_max print(f\"----\\nTesting {float16_max} + {add_to_max} = {overflow_number} &gt; 65504 (max for float16) it should overflow?\") tensor_fp16 = torch.tensor(overflow_number, dtype=torch.float16) tensor_bf16 = torch.tensor(overflow_number, dtype=torch.bfloat16)   print(f\"{tensor_fp16.dtype}:  {tensor_fp16.item():.10f} - could you explain why it is not inf?\") print(f\"{tensor_bf16.dtype}: {tensor_bf16.item():.10f} - in range of bfloat16, precision is lost more than float16\")  add_to_max = 16.0 overflow_number = float16_max + add_to_max print(f\"----\\nTesting {float16_max} + {add_to_max} = {overflow_number} &gt; 65504 (max for float16) it should overflow?\")  tensor_fp16 = torch.tensor(overflow_number, dtype=torch.float16) tensor_bf16 = torch.tensor(overflow_number, dtype=torch.bfloat16)   print(f\"{tensor_fp16.dtype}:  {tensor_fp16.item():.10f} - could you explain why this is inf?\") print(f\"{tensor_bf16.dtype}: {tensor_bf16.item():.10f} - in range of bfloat16, precision is lost more than float16\") <pre>----\nTesting 65500.0 + 3.141592653589793 = 65503.14159265359 &lt; 65504 (max for float16)\ntorch.float64:  65503.1415926536 \ntorch.float32:  65503.1406250000 \ntorch.float16:  65504.0000000000 - in range of float16, but precision is lost\ntorch.bfloat16: 65536.0000000000 - in range of bfloat16, precision is lost more than float16\n----\nTesting 65504.0 + 1.0 = 65505.0 &gt; 65504 (max for float16) it should overflow?\ntorch.float16:  65504.0000000000 - could you explain why it is not inf?\ntorch.bfloat16: 65536.0000000000 - in range of bfloat16, precision is lost more than float16\n----\nTesting 65504.0 + 16.0 = 65520.0 &gt; 65504 (max for float16) it should overflow?\ntorch.float16:  inf - could you explain why this is inf?\ntorch.bfloat16: 65536.0000000000 - in range of bfloat16, precision is lost more than float16\n</pre> In\u00a0[32]: Copied! <pre># Summoning tensors of different integer dtypes\ntensor_i64 = torch.tensor(1000, dtype=torch.int64)\ntensor_i32 = torch.tensor(1000, dtype=torch.int32)\ntensor_i16 = torch.tensor(1000, dtype=torch.int16)\ntensor_i8 = torch.tensor(100, dtype=torch.int8)\ntensor_ui8 = torch.tensor(255, dtype=torch.uint8)\n\nprint(\"--- Integer Memory Footprints ---\")\nprint(f\"torch.int64: Memory: {tensor_i64.element_size()} bytes\")\nprint(f\"torch.int32: Memory: {tensor_i32.element_size()} bytes\")\nprint(f\"torch.int16: Memory: {tensor_i16.element_size()} bytes\")\nprint(f\"torch.int8:  Memory: {tensor_i8.element_size()} bytes\")\nprint(f\"torch.uint8: Memory: {tensor_ui8.element_size()} bytes\")\n</pre> # Summoning tensors of different integer dtypes tensor_i64 = torch.tensor(1000, dtype=torch.int64) tensor_i32 = torch.tensor(1000, dtype=torch.int32) tensor_i16 = torch.tensor(1000, dtype=torch.int16) tensor_i8 = torch.tensor(100, dtype=torch.int8) tensor_ui8 = torch.tensor(255, dtype=torch.uint8)  print(\"--- Integer Memory Footprints ---\") print(f\"torch.int64: Memory: {tensor_i64.element_size()} bytes\") print(f\"torch.int32: Memory: {tensor_i32.element_size()} bytes\") print(f\"torch.int16: Memory: {tensor_i16.element_size()} bytes\") print(f\"torch.int8:  Memory: {tensor_i8.element_size()} bytes\") print(f\"torch.uint8: Memory: {tensor_ui8.element_size()} bytes\")   <pre>--- Integer Memory Footprints ---\ntorch.int64: Memory: 8 bytes\ntorch.int32: Memory: 4 bytes\ntorch.int16: Memory: 2 bytes\ntorch.int8:  Memory: 1 bytes\ntorch.uint8: Memory: 1 bytes\n</pre> In\u00a0[33]: Copied! <pre># Our original, high-precision tensor\nfrom math import pi\n\n\npi_fp64 = torch.tensor(3.141592653589793, dtype=torch.float64)\n\nfor i in range(12):\n    pi_2power = pi_fp64**(i+1)\n    print(\"--------------------------------\")\n    print(f\"pi^{i+1}: {pi_2power.item():.10f} float64\")\n\n    # Cast it down\n    pi_fp32 = pi_2power.to(torch.float32)\n    print(f\"Casted to float32:  {pi_fp32.item():.10f} (Precision lost!)\")\n\n    pi_fp16 = pi_2power.to(torch.float16)\n    print(f\"Casted to float16:  {pi_fp16.item():.10f} (More precision lost!)\")\n\n    pi_bf16 = pi_2power.to(torch.bfloat16)\n    print(f\"Casted to bfloat16: {pi_bf16.item():.10f} (More precision loss!)\")\n\n    # Casting floats to integers truncates the decimal part entirely!\n    integer_pi = pi_2power.to(torch.int)\n    print(f\"Casted to integer: {integer_pi.item()} (Decimal part vanished!)\")\n</pre> # Our original, high-precision tensor from math import pi   pi_fp64 = torch.tensor(3.141592653589793, dtype=torch.float64)  for i in range(12):     pi_2power = pi_fp64**(i+1)     print(\"--------------------------------\")     print(f\"pi^{i+1}: {pi_2power.item():.10f} float64\")      # Cast it down     pi_fp32 = pi_2power.to(torch.float32)     print(f\"Casted to float32:  {pi_fp32.item():.10f} (Precision lost!)\")      pi_fp16 = pi_2power.to(torch.float16)     print(f\"Casted to float16:  {pi_fp16.item():.10f} (More precision lost!)\")      pi_bf16 = pi_2power.to(torch.bfloat16)     print(f\"Casted to bfloat16: {pi_bf16.item():.10f} (More precision loss!)\")      # Casting floats to integers truncates the decimal part entirely!     integer_pi = pi_2power.to(torch.int)     print(f\"Casted to integer: {integer_pi.item()} (Decimal part vanished!)\")   <pre>--------------------------------\npi^1: 3.1415926536 float64\nCasted to float32:  3.1415927410 (Precision lost!)\nCasted to float16:  3.1406250000 (More precision lost!)\nCasted to bfloat16: 3.1406250000 (More precision loss!)\nCasted to integer: 3 (Decimal part vanished!)\n--------------------------------\npi^2: 9.8696044011 float64\nCasted to float32:  9.8696041107 (Precision lost!)\nCasted to float16:  9.8671875000 (More precision lost!)\nCasted to bfloat16: 9.8750000000 (More precision loss!)\nCasted to integer: 9 (Decimal part vanished!)\n--------------------------------\npi^3: 31.0062766803 float64\nCasted to float32:  31.0062770844 (Precision lost!)\nCasted to float16:  31.0000000000 (More precision lost!)\nCasted to bfloat16: 31.0000000000 (More precision loss!)\nCasted to integer: 31 (Decimal part vanished!)\n--------------------------------\npi^4: 97.4090910340 float64\nCasted to float32:  97.4090881348 (Precision lost!)\nCasted to float16:  97.4375000000 (More precision lost!)\nCasted to bfloat16: 97.5000000000 (More precision loss!)\nCasted to integer: 97 (Decimal part vanished!)\n--------------------------------\npi^5: 306.0196847853 float64\nCasted to float32:  306.0196838379 (Precision lost!)\nCasted to float16:  306.0000000000 (More precision lost!)\nCasted to bfloat16: 306.0000000000 (More precision loss!)\nCasted to integer: 306 (Decimal part vanished!)\n--------------------------------\npi^6: 961.3891935753 float64\nCasted to float32:  961.3892211914 (Precision lost!)\nCasted to float16:  961.5000000000 (More precision lost!)\nCasted to bfloat16: 960.0000000000 (More precision loss!)\nCasted to integer: 961 (Decimal part vanished!)\n--------------------------------\npi^7: 3020.2932277768 float64\nCasted to float32:  3020.2932128906 (Precision lost!)\nCasted to float16:  3020.0000000000 (More precision lost!)\nCasted to bfloat16: 3024.0000000000 (More precision loss!)\nCasted to integer: 3020 (Decimal part vanished!)\n--------------------------------\npi^8: 9488.5310160706 float64\nCasted to float32:  9488.5312500000 (Precision lost!)\nCasted to float16:  9488.0000000000 (More precision lost!)\nCasted to bfloat16: 9472.0000000000 (More precision loss!)\nCasted to integer: 9488 (Decimal part vanished!)\n--------------------------------\npi^9: 29809.0993334462 float64\nCasted to float32:  29809.0996093750 (Precision lost!)\nCasted to float16:  29808.0000000000 (More precision lost!)\nCasted to bfloat16: 29824.0000000000 (More precision loss!)\nCasted to integer: 29809 (Decimal part vanished!)\n--------------------------------\npi^10: 93648.0474760830 float64\nCasted to float32:  93648.0468750000 (Precision lost!)\nCasted to float16:  inf (More precision lost!)\nCasted to bfloat16: 93696.0000000000 (More precision loss!)\nCasted to integer: 93648 (Decimal part vanished!)\n--------------------------------\npi^11: 294204.0179738905 float64\nCasted to float32:  294204.0312500000 (Precision lost!)\nCasted to float16:  inf (More precision lost!)\nCasted to bfloat16: 294912.0000000000 (More precision loss!)\nCasted to integer: 294204 (Decimal part vanished!)\n--------------------------------\npi^12: 924269.1815233737 float64\nCasted to float32:  924269.1875000000 (Precision lost!)\nCasted to float16:  inf (More precision lost!)\nCasted to bfloat16: 925696.0000000000 (More precision loss!)\nCasted to integer: 924269 (Decimal part vanished!)\n</pre> In\u00a0[34]: Copied! <pre># Our Grand Spell for Selecting the Best Device\nif torch.cuda.is_available():\n    device = torch.device(\"cuda\")\n    print(\"Mwahahaha! We have awakened the CUDA beast!\")\nelif torch.backends.mps.is_available():\n    device = torch.device(\"mps\")\n    print(\"Behold! The power of Apple's Metal Performance Shaders!\")\nelse:\n    device = torch.device(\"cpu\")\n    print(\"The humble CPU will have to suffice for today's experiments.\")\n\nprint(f\"Selected device: {device}\\\\n\")\n\n# --- Summoning and Teleporting Tensors ---\n\n# 1. Summon a tensor directly on the chosen device\ntensor_on_device = torch.randn(2, 3, device=device)\nprint(f\"Tensor summoned directly on '{tensor_on_device.device}'\")\nprint(tensor_on_device)\n\n# 2. Teleport a CPU tensor to the device using the .to() spell\ncpu_tensor = torch.tensor([[1, 2, 3], [4, 5, 6]])\nprint(f\"A CPU tensor, minding its own business: {cpu_tensor.device}\")\n\nteleported_tensor = cpu_tensor.to(device)\nprint(f\"Teleported to '{teleported_tensor.device}'!\")\nprint(teleported_tensor)\n\n# IMPORTANT: Operations between tensors on different devices will FAIL!\n# This would cause a RuntimeError:\n# try:\n#     result = cpu_tensor + teleported_tensor\n# except RuntimeError as e:\n#     print(f\"\\\\nAs expected, chaos ensues: {e}\")\n</pre> # Our Grand Spell for Selecting the Best Device if torch.cuda.is_available():     device = torch.device(\"cuda\")     print(\"Mwahahaha! We have awakened the CUDA beast!\") elif torch.backends.mps.is_available():     device = torch.device(\"mps\")     print(\"Behold! The power of Apple's Metal Performance Shaders!\") else:     device = torch.device(\"cpu\")     print(\"The humble CPU will have to suffice for today's experiments.\")  print(f\"Selected device: {device}\\\\n\")  # --- Summoning and Teleporting Tensors ---  # 1. Summon a tensor directly on the chosen device tensor_on_device = torch.randn(2, 3, device=device) print(f\"Tensor summoned directly on '{tensor_on_device.device}'\") print(tensor_on_device)  # 2. Teleport a CPU tensor to the device using the .to() spell cpu_tensor = torch.tensor([[1, 2, 3], [4, 5, 6]]) print(f\"A CPU tensor, minding its own business: {cpu_tensor.device}\")  teleported_tensor = cpu_tensor.to(device) print(f\"Teleported to '{teleported_tensor.device}'!\") print(teleported_tensor)  # IMPORTANT: Operations between tensors on different devices will FAIL! # This would cause a RuntimeError: # try: #     result = cpu_tensor + teleported_tensor # except RuntimeError as e: #     print(f\"\\\\nAs expected, chaos ensues: {e}\")  <pre>Mwahahaha! We have awakened the CUDA beast!\n\\nSelected device: cuda\\n\nTensor summoned directly on 'cuda:0'\ntensor([[-0.1802,  0.6344,  0.5375],\n        [ 0.4829, -0.6208, -0.6716]], device='cuda:0')\n\\nA CPU tensor, minding its own business: cpu\nTeleported to 'cuda:0'!\ntensor([[1, 2, 3],\n        [4, 5, 6]], device='cuda:0')\n</pre> In\u00a0[35]: Copied! <pre>import time\nimport torch\n\n# A utility for our CPU speed trials\ndef time_cpu_operation(tensor):\n    start_time = time.time()\n    # A sequence of intense, element-wise mathematical transformations!\n    torch.exp(torch.cos(torch.sin(tensor)))\n    end_time = time.time()\n    return end_time - start_time\n\n# A colossal tensor for our experiment!\nsize = 20000 # Larger size to make the computation more intensive\nlarge_tensor_cpu = torch.randn(size, size)\n\nprint(f\"--- CPU Speed Trials ({size}x{size} Element-wise Operations) ---\")\ntime_fp32_cpu = time_cpu_operation(large_tensor_cpu.clone())\nprint(f\"Float32 on CPU took: {time_fp32_cpu:.6f} seconds (Our baseline)\")\n\ntime_fp16_cpu = time_cpu_operation(large_tensor_cpu.clone().to(torch.float16))\nprint(f\"Float16 on CPU took: {time_fp16_cpu:.6f} seconds \")\n\ntime_bf16_cpu = time_cpu_operation(large_tensor_cpu.clone().to(torch.bfloat16))\nprint(f\"BFloat16 on CPU took: {time_bf16_cpu:.6f} seconds \")\n\n\n\n\n# --- SPEED TRIALS ON GPU (if available) ---\nif torch.cuda.is_available():\n    print(f\"--- GPU Speed Trials ({size}x{size} Element-wise Operations) ---\")\n    large_tensor_gpu = large_tensor_cpu.to(\"cuda\")\n\n    # Define the GPU timing utility using CUDA events for accuracy\n    def time_gpu_operation(tensor):\n        start_event = torch.cuda.Event(enable_timing=True)\n        end_event = torch.cuda.Event(enable_timing=True)\n        \n        # Warm-up run to compile kernels, etc.\n        torch.exp(torch.cos(torch.sin(tensor)))\n        \n        # The actual timed operation\n        start_event.record()\n        torch.exp(torch.cos(torch.sin(tensor)))\n        end_event.record()\n        torch.cuda.synchronize() # Wait for the GPU operation to complete\n        \n        return start_event.elapsed_time(end_event) / 1000 # Return time in seconds\n\n    # Time Float32\n    time_fp32_gpu = time_gpu_operation(large_tensor_gpu.clone())\n    speedup_vs_cpu = time_fp32_cpu / time_fp32_gpu\n    print(f\"Float32 on GPU took: {time_fp32_gpu:.6f} seconds ({speedup_vs_cpu:.2f}x faster than CPU!)\")\n\n    # Time Float16\n    try:\n        large_tensor_gpu_fp16 = large_tensor_gpu.clone().to(torch.float16)\n        time_fp16_gpu = time_gpu_operation(large_tensor_gpu_fp16)\n        speedup_vs_gpu_fp32 = time_fp32_gpu / time_fp16_gpu\n        speedup_vs_cpu_fp32 = time_fp32_cpu / time_fp16_gpu\n        speedup_vs_cpu_fp16 = time_fp16_cpu / time_fp16_gpu\n        print(f\"Float16 on GPU took: {time_fp16_gpu:.6f} seconds ({speedup_vs_gpu_fp32:.2f}x vs GPU FP32, {speedup_vs_cpu_fp32:.2f}x vs CPU FP32, {speedup_vs_cpu_fp16:.2f}x vs CPU FP16!)\")\n    except RuntimeError as e:\n        print(f\"Float16 not supported on this GPU: {e}\")\n\n    # Time BFloat16\n    try:\n        large_tensor_gpu_bf16 = large_tensor_gpu.clone().to(torch.bfloat16)\n        time_bf16_gpu = time_gpu_operation(large_tensor_gpu_bf16)\n        speedup_vs_gpu_fp32 = time_fp32_gpu / time_bf16_gpu\n        speedup_vs_cpu_fp32 = time_fp32_cpu / time_bf16_gpu\n        speedup_vs_cpu_fp16 = time_bf16_cpu / time_bf16_gpu\n        print(f\"BFloat16 on GPU took: {time_bf16_gpu:.6f} seconds ({speedup_vs_gpu_fp32:.2f}x vs GPU FP32, {speedup_vs_cpu_fp32:.2f}x vs CPU FP32, {speedup_vs_cpu_fp16:.2f}x vs CPU BFP16!)\")\n    except RuntimeError as e:\n        print(f\"BFloat16 not supported on this GPU: {e}\")\n\nelse:\n    print(\"GPU not available for speed trials. A true pity!\")\n</pre> import time import torch  # A utility for our CPU speed trials def time_cpu_operation(tensor):     start_time = time.time()     # A sequence of intense, element-wise mathematical transformations!     torch.exp(torch.cos(torch.sin(tensor)))     end_time = time.time()     return end_time - start_time  # A colossal tensor for our experiment! size = 20000 # Larger size to make the computation more intensive large_tensor_cpu = torch.randn(size, size)  print(f\"--- CPU Speed Trials ({size}x{size} Element-wise Operations) ---\") time_fp32_cpu = time_cpu_operation(large_tensor_cpu.clone()) print(f\"Float32 on CPU took: {time_fp32_cpu:.6f} seconds (Our baseline)\")  time_fp16_cpu = time_cpu_operation(large_tensor_cpu.clone().to(torch.float16)) print(f\"Float16 on CPU took: {time_fp16_cpu:.6f} seconds \")  time_bf16_cpu = time_cpu_operation(large_tensor_cpu.clone().to(torch.bfloat16)) print(f\"BFloat16 on CPU took: {time_bf16_cpu:.6f} seconds \")     # --- SPEED TRIALS ON GPU (if available) --- if torch.cuda.is_available():     print(f\"--- GPU Speed Trials ({size}x{size} Element-wise Operations) ---\")     large_tensor_gpu = large_tensor_cpu.to(\"cuda\")      # Define the GPU timing utility using CUDA events for accuracy     def time_gpu_operation(tensor):         start_event = torch.cuda.Event(enable_timing=True)         end_event = torch.cuda.Event(enable_timing=True)                  # Warm-up run to compile kernels, etc.         torch.exp(torch.cos(torch.sin(tensor)))                  # The actual timed operation         start_event.record()         torch.exp(torch.cos(torch.sin(tensor)))         end_event.record()         torch.cuda.synchronize() # Wait for the GPU operation to complete                  return start_event.elapsed_time(end_event) / 1000 # Return time in seconds      # Time Float32     time_fp32_gpu = time_gpu_operation(large_tensor_gpu.clone())     speedup_vs_cpu = time_fp32_cpu / time_fp32_gpu     print(f\"Float32 on GPU took: {time_fp32_gpu:.6f} seconds ({speedup_vs_cpu:.2f}x faster than CPU!)\")      # Time Float16     try:         large_tensor_gpu_fp16 = large_tensor_gpu.clone().to(torch.float16)         time_fp16_gpu = time_gpu_operation(large_tensor_gpu_fp16)         speedup_vs_gpu_fp32 = time_fp32_gpu / time_fp16_gpu         speedup_vs_cpu_fp32 = time_fp32_cpu / time_fp16_gpu         speedup_vs_cpu_fp16 = time_fp16_cpu / time_fp16_gpu         print(f\"Float16 on GPU took: {time_fp16_gpu:.6f} seconds ({speedup_vs_gpu_fp32:.2f}x vs GPU FP32, {speedup_vs_cpu_fp32:.2f}x vs CPU FP32, {speedup_vs_cpu_fp16:.2f}x vs CPU FP16!)\")     except RuntimeError as e:         print(f\"Float16 not supported on this GPU: {e}\")      # Time BFloat16     try:         large_tensor_gpu_bf16 = large_tensor_gpu.clone().to(torch.bfloat16)         time_bf16_gpu = time_gpu_operation(large_tensor_gpu_bf16)         speedup_vs_gpu_fp32 = time_fp32_gpu / time_bf16_gpu         speedup_vs_cpu_fp32 = time_fp32_cpu / time_bf16_gpu         speedup_vs_cpu_fp16 = time_bf16_cpu / time_bf16_gpu         print(f\"BFloat16 on GPU took: {time_bf16_gpu:.6f} seconds ({speedup_vs_gpu_fp32:.2f}x vs GPU FP32, {speedup_vs_cpu_fp32:.2f}x vs CPU FP32, {speedup_vs_cpu_fp16:.2f}x vs CPU BFP16!)\")     except RuntimeError as e:         print(f\"BFloat16 not supported on this GPU: {e}\")  else:     print(\"GPU not available for speed trials. A true pity!\")  <pre>--- CPU Speed Trials (20000x20000 Element-wise Operations) ---\nFloat32 on CPU took: 1.031577 seconds (Our baseline)\nFloat16 on CPU took: 0.360738 seconds \nBFloat16 on CPU took: 0.360255 seconds \n--- GPU Speed Trials (20000x20000 Element-wise Operations) ---\nFloat32 on GPU took: 0.030761 seconds (33.54x faster than CPU!)\nFloat16 on GPU took: 0.015344 seconds (2.00x vs GPU FP32, 67.23x vs CPU FP32, 23.51x vs CPU FP16!)\nBFloat16 on GPU took: 0.015259 seconds (2.02x vs GPU FP32, 67.61x vs CPU FP32, 23.61x vs CPU BFP16!)\n</pre>","tags":["tensors","module 1","beginner"],"boost":1.2},{"location":"01-tensors/03_data_types_and_devices/#dtypes-devices-choose-your-weapons","title":"DTypes &amp; Devices: Choose Your Weapons\u00b6","text":"<p>Module 1 | Lesson 3</p>","tags":["tensors","module 1","beginner"],"boost":1.2},{"location":"01-tensors/03_data_types_and_devices/#professor-torchensteins-grand-directive","title":"Professor Torchenstein's Grand Directive\u00b6","text":"<p>Mwahahaha! You've sliced, fused, and reshaped tensors with the skill of a master surgeon! You can command their form, but what of their soul? What of their very essence?</p> <p>Today, we delve deeper! We shall master the two most fundamental properties of any tensor: its data type (<code>dtype</code>), which determines its precision and power, and its device, the very dimension it inhabits\u2014be it the humble CPU or the roaring, incandescent GPU! Choose your weapons wisely, for these choices dictate the speed, precision, and ultimate success of your grand experiments!</p> <p></p>","tags":["tensors","module 1","beginner"],"boost":1.2},{"location":"01-tensors/03_data_types_and_devices/#your-mission-briefing","title":"Your Mission Briefing\u00b6","text":"<p>By the end of this electrifying session, you will have mastered the arcane arts of:</p> <ul> <li>\ud83d\udd2c Understanding the soul of a neural network: the floating-point <code>dtype</code>.</li> <li>\u2696\ufe0f Analyzing the critical trade-offs between <code>float64</code>, <code>float32</code>, <code>float16</code>, and <code>bfloat16</code>.</li> <li>\u2728 Transmuting floats to balance precision, range, and performance for training and inference.</li> <li>\u26a1 Teleporting your tensors to the most powerful <code>device</code> (<code>CPU</code>, <code>GPU</code>, <code>MPS</code>) to unleash their speed.</li> <li>\u26a0\ufe0f Diagnosing the catastrophic errors that arise from floating-point overflow and mismatched devices.</li> </ul>","tags":["tensors","module 1","beginner"],"boost":1.2},{"location":"01-tensors/03_data_types_and_devices/#previously-in-the-lab-a-quick-recap","title":"Previously in the Lab... (A Quick Recap)\u00b6","text":"<p>In our last experiment, we mastered Tensor Metamorphosis, transforming tensor shapes with <code>reshape</code>, <code>view</code>, <code>squeeze</code>, and <code>unsqueeze</code>. We learned that a tensor's shape is merely an illusion\u2014a view into a contiguous block of 1D memory.</p> <p>Now that you command a tensor's external form, we shall master its internal essence. The journey continues!</p>","tags":["tensors","module 1","beginner"],"boost":1.2},{"location":"01-tensors/03_data_types_and_devices/#the-alchemists-arsenal-mastering-data-types-dtype","title":"The Alchemist's Arsenal - Mastering Data Types (<code>dtype</code>)\u00b6","text":"<p>Behold, apprentice! Not all tensors are forged from the same ethereal stuff. The very essence of a tensor\u2014its <code>.dtype</code>\u2014determines what kind of numbers it can hold, its precision in the arcane arts of mathematics, and the amount of precious memory it consumes!</p> <p>A wise choice of <code>dtype</code> can mean the difference between a lightning-fast model and a sluggish, memory-guzzling behemoth. Let us inspect the primary weapons in our arsenal!</p>","tags":["tensors","module 1","beginner"],"boost":1.2},{"location":"01-tensors/03_data_types_and_devices/#checking-the-soul-of-your-tensor","title":"Checking the soul of your tensor\u00b6","text":"<p>We will summon tensors of different <code>dtypes</code>, transmute them, and witness the performance implications firsthand!</p> <p>To perform these miracles, you must master two key tools:</p> <ul> <li>The <code>.dtype</code> attribute: A tensor's inherent property that reveals its data type. You can't change it directly, but you can inspect it to understand your tensor's essence.</li> <li>The <code>.to()</code> method: This is your transmutation spell! It's a powerful and versatile method that not only changes a tensor's <code>dtype</code> but can also teleport it to a different <code>device</code> at the same time!</li> </ul>","tags":["tensors","module 1","beginner"],"boost":1.2},{"location":"01-tensors/03_data_types_and_devices/#floating-point-types-the-elixirs-of-learning","title":"Floating-Point Types (The Elixirs of Learning)\u00b6","text":"<p>The very lifeblood of neural networks! These are essential for representing real numbers, calculating gradients, and enabling your models to learn.</p> <ul> <li><code>torch.float32</code> (<code>torch.float</code>): The 32-bit workhorse. This is the default <code>dtype</code> for a reason\u2014it offers a fantastic balance between precision and performance. Most of your initial experiments will thrive on this reliable elixir.</li> <li><code>torch.float64</code> (<code>torch.double</code>): 64-bit, for when you require the utmost, surgical precision. Its use in deep learning is rare, as it doubles memory usage and can slow down computations, but for certain scientific calculations, it is indispensable. A powerful tool, but often overkill for our purposes!</li> <li><code>torch.float16</code> (<code>torch.half</code>): A 16-bit potion for speed and memory efficiency. Halving the precision can dramatically accelerate training on modern GPUs and cut your memory footprint in half! But beware\u2014its limited range can sometimes lead to numerical instability.</li> <li><code>torch.bfloat16</code>: The new favorite in the high council of AI! Also 16-bit, but with a crucial difference from <code>float16</code>. It sacrifices some precision to maintain the same dynamic range as <code>float32</code>, making it far more stable for training large models like Transformers.</li> </ul>","tags":["tensors","module 1","beginner"],"boost":1.2},{"location":"01-tensors/03_data_types_and_devices/#the-limits-of-floats-torchfinfo-spell","title":"The Limits of floats: <code>torch.finfo</code> Spell\u00b6","text":"<p>How, you ask, can a master alchemist know the precise limits of their elixirs? You need not guess! PyTorch provides a powerful incantation for this very purpose: <code>torch.finfo</code>.</p> <p>This spell reveals the deepest secrets of any floating-point <code>dtype</code>:</p> Attribute Description <code>bits</code> The number of bits of memory the <code>dtype</code> occupies (e.g., 16, 32, 64). <code>min</code> / <code>max</code> The smallest and largest numbers that can be represented. Exceeding this causes overflow (<code>inf</code>)! <code>eps</code> Epsilon. The smallest possible difference between 1.0 and the next representable number. This is a pure measure of precision around the number 1. A smaller <code>eps</code> means higher precision. <code>tiny</code> The smallest positive number that can be represented. Numbers smaller than this are lost to the void (rounded to zero)! <code>resolution</code> The approximate number of decimal digits of precision you can trust. <p>Let us now cast this spell and gaze upon the true nature of our floating-point <code>dtypes</code>!</p>","tags":["tensors","module 1","beginner"],"boost":1.2},{"location":"01-tensors/03_data_types_and_devices/#the-curious-case-of-bfloat16-and-the-number-70144","title":"The Curious Case of <code>bfloat16</code> and the Number 70,144\u00b6","text":"<p>Mwahahaha! Apprentice, you have sharp eyes! You witnessed a strange transmutation: our number <code>70000.0</code> became <code>70144.0</code> when cast to <code>bfloat16</code>. Is this a bug? A flaw in our alchemy? No! This is a profound secret about the very fabric of digital reality!</p> <p>To understand this, we must journey into the heart of the machine and see how it stores floating-point numbers.</p>","tags":["tensors","module 1","beginner"],"boost":1.2},{"location":"01-tensors/03_data_types_and_devices/#the-blueprint-of-a-float-scientific-notation-in-binary","title":"The Blueprint of a Float: Scientific Notation in Binary\u00b6","text":"<p>Every floating-point number in your computer's memory is stored like a secret formula with three parts:</p> <ol> <li>The Sign (S): A single bit (0 for positive, 1 for negative).</li> <li>The Exponent (E): A set of bits that represent the number's magnitude or range, like the <code>10^x</code> part in scientific notation.</li> <li>The Mantissa (M): A set of bits that represent the actual digits of the number\u2014its precision.</li> </ol> <p>The number is roughly reconstructed as: <code>(-1)^S * M * 2^E</code>.</p> <p>The mantissa is the key here. It's a binary fraction that always starts with an implicit <code>1.</code>, followed by the sum of fractional powers of 2. For example: <code>1.M = 1 + m1/2 + m2/4 + m3/8 + ...m23/2^23</code>, where <code>m1, m2, m3, ...m23</code> are 0 or 1 (depends on how many bits are in the mantissa, this case 23 bits).</p>","tags":["tensors","module 1","beginner"],"boost":1.2},{"location":"01-tensors/03_data_types_and_devices/#the-meaning-of-precision-for-floating-point-numbers","title":"The Meaning of \"Precision\" for floating-point numbers\u00b6","text":"<p>When we say <code>bfloat16</code> has \"less precision\" than <code>float32</code>, we don't mean fewer decimal places in the way humans think. We mean it has fewer bits in its mantissa.</p> <ul> <li><code>float32</code> has 1 bit for sign, 23 mantissa bits and 8 bits for exponent</li> <li><code>float16</code> has 1 bit for sign, 10 mantissa bits and 5 bits for exponent,  more bits for mantissa means less coarse==more precision, less range (min - max)</li> <li><code>bfloat16</code> has 1 bit for sign, 7 mantissa bits and 8 bits for exponent,  less bits for mantissa means more coarse==less precision, more range (min - max) then float16, same range as float32</li> </ul> <p>This means <code>bfloat16</code> can only represent a much smaller, coarser set of numbers between any two powers of two. For small numbers (like 3.14), the representable values are very close together. But for large numbers, the \"gaps\" between representable values become huge!</p>","tags":["tensors","module 1","beginner"],"boost":1.2},{"location":"01-tensors/03_data_types_and_devices/#detailed-explanation-why-70144","title":"Detailed explanation: Why 70,144?\u00b6","text":"<p>The number <code>70000</code> is simply not one of the numbers that can be perfectly formed with <code>bfloat16</code>'s limited 7-bit mantissa at that large exponent range.</p> <p>Lets write the number <code>70,000</code> in binary: <code>1 0001 0001 0111 0000</code>.</p> <p>For <code>70,000</code>, scientific notation number starts with <code>1.</code>, we move the decimal point 16 places to the left (<code>2^16</code>). $$ 1. \\underbrace{0001000101110000}_{\\text{16 binary digits}} \\times 2^{16} $$</p> <p>The mantissa (the part after the <code>1.</code>) is where the precision limit strikes.</p> <p>A <code>float32</code> has 23 bits for its mantissa. It can easily store those binary digits with room to spare. The number <code>70,000</code> is stored perfectly.</p> <ul> <li><code>0</code>-sign bit, <code>0001000101110000 0000000</code>-23 mantissa bits, <code>00010000</code>- exponent bits (<code>2^16</code>, omits bias for simplicity)</li> </ul> <p>A <code>bfloat16</code> only has 7 bits for its mantissa.</p> <ul> <li><code>0</code>-sign bit,<code>1.</code> <code>0001000</code>-7 mantissa bits (first 7 digits) -&gt; rounded up to <code>0001001</code>, <code>00010000</code>- exponent bits (<code>2^16</code>, omits bias for simplicity)</li> <li><code>1*2^16 + (0*1/2 + 0*1/4 + 0*1/8 + 1*1/16 + 0*1/32 + 0*1/64 + 1*1/128)*2^16 = 65536+ 4096 + 512=70144</code></li> </ul> <p><code>bfloat16</code> must take that 16-digit binary sequence and round it to fit into just 7 bits. This forces a loss of information, even for a whole number.</p> <ol> <li>Original Mantissa: <code>0001000101110000</code></li> <li><code>bfloat16</code> capacity: Can only store the first 7 digits: <code>0001000</code>.</li> <li>Rounding: It checks the 8th digit (<code>1</code>) and, following rounding rules, rounds the 7-bit number up. The new mantissa becomes <code>0001001</code>.</li> </ol> <p>So, <code>bfloat16</code> ends up storing the number as <code>1.0001001 \\times 2^{16}</code>.</p> <p>Think of it like trying to measure <code>70,000</code> millimeters with a ruler that only has markings every <code>256</code> millimeters. You can't land on <code>70,000</code> exactly. You must choose the closest mark.</p> <p>The two closest \"marks\" that <code>bfloat16</code> can represent in that range are:</p> <ul> <li><code>69,888</code></li> <li><code>70,144</code></li> </ul> <p>Since <code>70,000</code> is closer to <code>70,144</code>, the transmutation spell rounds it to that value. It is not an error, but the result of sacrificing precision to maintain the vast numerical range of <code>float32</code>. This robustness is exactly why it is the preferred elixir for training colossal neural networks! You have witnessed the fundamental trade-off of modern AI hardware!</p>","tags":["tensors","module 1","beginner"],"boost":1.2},{"location":"01-tensors/03_data_types_and_devices/#why-float16-behaves-so-weird-around-the-number-65504","title":"Why <code>float16</code> behaves so weird around the number <code>65504</code>?\u00b6","text":"<p>Being armed with this knowledge, try to figure out what happened with the following numbers:</p> <ol> <li>Why <code>float16</code> behaves so weird around the number <code>65504</code>? Why it is not <code>inf</code>?</li> <li>Why <code>bfloat16</code> of <code>65504+1.0</code> is equal to <code>65504+20.0</code>?</li> </ol>","tags":["tensors","module 1","beginner"],"boost":1.2},{"location":"01-tensors/03_data_types_and_devices/#why-bfloat16-is-better-for-transformers-a-tale-of-range-and-rebellion","title":"Why <code>bfloat16</code> is Better for Transformers: A Tale of Range and Rebellion\u00b6","text":"<p>Now for a secret that separates the masters from the mere dabblers! Both <code>float16</code> and <code>bfloat16</code> use 16 bits, but they do so with diabolically different strategies. Understanding this is key to training modern marvels like Transformers!</p> <p>The world of <code>float32</code> is a stable, predictable realm. But it is slow and memory-hungry! When we attempt to accelerate our dark arts with <code>float16</code>, we encounter a terrible problem: The Tyranny of a Tiny Range.</p>","tags":["tensors","module 1","beginner"],"boost":1.2},{"location":"01-tensors/03_data_types_and_devices/#the-peril-of-float16-an-unstable-concoction","title":"The Peril of <code>float16</code>: An Unstable Concoction\u00b6","text":"<p><code>float16</code> dedicates more bits to its mantissa (precision), but starves its exponent (range). Its numerical world is small, spanning from roughly <code>6.1 x 10^-5</code> to <code>65,504</code>. Anything outside this narrow window becomes an <code>inf</code> (infinity) or vanishes to zero.</p> <p>During the chaotic process of training a massive Transformer, values can fluctuate wildly. This is where the tyranny of <code>float16</code> strikes hardest:</p> <ul> <li>Exploding Gradients: Imagine a scenario deep within your network where a series of large gradients are multiplied. Even with normalization, an intermediate calculation can easily exceed 65,504. For instance, the Adam optimizer tracks the variance of gradients (<code>v</code> term), which can grow very large. If this value overflows to <code>inf</code>, the weight update becomes <code>NaN</code> (Not a Number), and your entire training process collapses into a fiery numerical singularity!</li> <li>Vanishing Activations: Inside a Transformer, attention scores are passed through a Softmax function. If the input values (logits) are very large negative numbers, the resulting probabilities can become smaller than <code>float16</code>'s minimum representable value. They are rounded down to zero, and that part of your model stops learning entirely!</li> </ul> <p>To combat this, alchemists of old used a crude technique called loss scaling: manually multiplying the loss to keep gradients within <code>float16</code>'s safe range. It is a messy, unreliable hack!</p>","tags":["tensors","module 1","beginner"],"boost":1.2},{"location":"01-tensors/03_data_types_and_devices/#the-bfloat16-rebellion-sacrificing-precision-for-power","title":"The <code>bfloat16</code> Rebellion: Sacrificing Precision for Power!\u00b6","text":"<p>The great minds at Google Brain, in their quest for ultimate power, forged a new weapon: the Brain Floating-Point Format, or <code>bfloat16</code>! They looked at the chaos of <code>float16</code> and made a brilliant, rebellious choice.</p> <p>They designed <code>bfloat16</code> to have the same number of exponent bits as <code>float32</code> (8 bits). This gives it the exact same colossal dynamic range, spanning from <code>1.18 x 10^-38</code> to <code>3.4 x 10^38</code>. It can represent gargantuan numbers and infinitesimally small ones without breaking a sweat.</p> <p>The price? It has fewer mantissa bits (7 bits) than <code>float16</code> (10 bits), giving it less precision. But here is the profound secret, backed by countless experiments in the deepest labs: neural networks are incredibly resilient to low precision.</p> <p>Why do the inaccuracies not hurt?</p> <ul> <li>Stochastic Nature of Training: We train models using stochastic gradient descent on mini-batches of data. This process is inherently noisy! The tiny inaccuracies introduced by <code>bfloat16</code>'s rounding are like a single drop of rain in a hurricane\u2014they are statistically insignificant compared to the noise already present in the training process.</li> <li>Error Accumulation is Not Catastrophic: As researchers from The Hardware Lottery blog and other deep learning practitioners have noted, the errors from low precision tend to average out over millions of updates. The network's learning direction isn't meaningfully altered. The gradient still points downhill, even if it's a slightly wobblier path.</li> </ul> <p>\"For the volatile, chaotic world of deep learning, a vast and stable range is far more important than surgical precision.\"</p> <p>\u2014 Prof. Torchenstein</p>","tags":["tensors","module 1","beginner"],"boost":1.2},{"location":"01-tensors/03_data_types_and_devices/#the-transformers-elixir-of-choice","title":"The Transformer's Elixir of Choice\u00b6","text":"<p>For training and fine-tuning, <code>bfloat16</code> is the undisputed champion, the elixir that fuels the titans of AI.</p> <ol> <li>Training Stability: Its <code>float32</code>-like range means no more exploding gradients in optimizer states or vanishing activations in softmax. You can throw away the clumsy crutch of loss scaling.</li> <li>Memory Efficiency: Like <code>float16</code>, it cuts your model's memory footprint in half compared to <code>float32</code>. This allows you to train larger models or use larger batch sizes, accelerating your path to discovery.</li> <li>Hardware Acceleration: It is natively supported on the most powerful instruments in any modern laboratory: Google TPUs and NVIDIA's latest GPUs (Ampere architecture and newer, like the A100 or RTX 30/40 series).</li> </ol> <p>The Rogues' Gallery: Who Uses <code>bfloat16</code>? The most powerful creations of our time were forged in the fires of <code>bfloat16</code>. Giants like Google's T5 and BERT, Meta's Llama 2, the Falcon models, and many more rely on <code>bfloat16</code> for stable and efficient training.</p> <p>The Verdict for Your Lab:</p> <ul> <li>For Training &amp; Fine-Tuning: <code>bfloat16</code> is your weapon of choice. It is the modern standard for a reason.</li> <li>For Inference: <code>float16</code> is often perfectly acceptable. After a model is trained, the range of values it processes is more predictable, making <code>float16</code>'s higher precision and wider hardware support a safe and efficient option.</li> </ul>","tags":["tensors","module 1","beginner"],"boost":1.2},{"location":"01-tensors/03_data_types_and_devices/#integer-types-the-counting-stones","title":"Integer Types (The Counting Stones)\u00b6","text":"<p>For when you need to count, index, or represent discrete information like image pixel values or class labels.</p> <ul> <li><code>torch.int64</code> (<code>torch.long</code>): The 64-bit grandmaster of integers. This is the default for indexing operations and is crucial for embedding layers, where you need to look up values from a large vocabulary.</li> <li><code>torch.int32</code> (<code>torch.int</code>): A solid 32-bit integer, perfectly suitable for most counting tasks.</li> <li><code>torch.uint8</code>: An 8-bit unsigned integer, representing values from 0 to 255. The undisputed king for storing image data, where each pixel in an RGB channel has a value in this exact range!</li> </ul> <p>Now for the counting stones\u2014the integer types. Their purpose is not precision, but to hold whole numbers. Observe their varying sizes.</p>","tags":["tensors","module 1","beginner"],"boost":1.2},{"location":"01-tensors/03_data_types_and_devices/#the-transmutation-spell-witnessing-the-effects-of-casting","title":"The Transmutation Spell: Witnessing the Effects of Casting\u00b6","text":"<p>Now that you understand the properties of each <code>dtype</code>, witness what happens when we perform the transmutation! Casting from a higher precision <code>dtype</code> to a lower one is a lossy operation. You gain speed and save memory, but at the cost of precision!</p> <p>Observe the fate of our high-precision number as we cast it down the alchemical ladder. Compare the precision loss of each dtype, esspecially float16 and bfloat16 (some time they are the same, some time not).</p>","tags":["tensors","module 1","beginner"],"boost":1.2},{"location":"01-tensors/03_data_types_and_devices/#boolean-type-the-oracle","title":"Boolean Type (The Oracle)\u00b6","text":"<p>Represents the fundamental truths of the universe: <code>True</code> or <code>False</code>.</p> <ul> <li><code>torch.bool</code>: The result of all your logical incantations (<code>&gt;</code>, <code>&lt;</code>, <code>==</code>). Essential for creating masks to filter and select elements from your tensors.</li> </ul>","tags":["tensors","module 1","beginner"],"boost":1.2},{"location":"01-tensors/03_data_types_and_devices/#the-lair-of-computation-mastering-devices-device","title":"The Lair of Computation - Mastering Devices (<code>device</code>)\u00b6","text":"","tags":["tensors","module 1","beginner"],"boost":1.2},{"location":"01-tensors/03_data_types_and_devices/#what-devices-pytorch-supports","title":"What devices PyTorch supports?\u00b6","text":"<p>A tensor's <code>dtype</code> is its soul, but its <code>.device</code> is its home\u2014the very dimension where its calculations will be performed. Choosing the right device is the key to unlocking diabolical computational speed! While many alchemical experiments can be run on a standard CPU, true power lies in specialized hardware.</p> <p>Here are the primary realms you can command:</p> <ul> <li><code>cpu</code>: The Central Processing Unit. The reliable, ever-present brain of your machine. It's a generalist, capable of any task, but it performs calculations sequentially. For small tensors and simple operations, it's our trusty home base.</li> <li><code>cuda</code>: The NVIDIA GPU! This is the roaring heart of the deep learning revolution. A GPU is a specialist, containing thousands of cores designed for one purpose: massively parallel computation. Moving your tensors here is essential for training any serious neural network.</li> <li><code>mps</code>: Metal Performance Shaders. Apple's answer to CUDA for their new M-series chips. If you are wielding a modern Mac, this device will unleash the power of its integrated GPU for accelerated training.</li> <li><code>xpu</code>: A realm forged by Intel for their own line of GPUs and AI accelerators, showing PyTorch's expanding hardware support.</li> <li><code>xla</code>: A gateway to Google's powerful Tensor Processing Units (TPUs), often used for training colossal models in the cloud.</li> </ul>","tags":["tensors","module 1","beginner"],"boost":1.2},{"location":"01-tensors/03_data_types_and_devices/#how-pytorch-supports-so-many-devices-the-magic-of-aten","title":"How PyTorch Supports So Many Devices: The Magic of ATen\u00b6","text":"<p>How can a single command like <code>torch.matmul()</code> work on a CPU, an NVIDIA GPU, and an Apple chip? The secret lies in PyTorch's core library: ATen.</p> <p>Think of ATen as a grand dispatcher in our laboratory. When you issue a command, ATen inspects the tensor's <code>.device</code> and redirects the command to a highly optimized, device-specific library:</p> <ul> <li>If <code>device='cpu'</code>, ATen calls libraries like <code>oneDNN</code>.</li> <li>If <code>device='cuda'</code>, ATen calls NVIDIA's legendary <code>cuDNN</code> library.</li> <li>If <code>device='mps'</code>, ATen calls Apple's <code>Metal</code> framework.</li> <li>For other devices like <code>xpu</code> or <code>xla</code>, it calls their respective specialized backends.</li> </ul> <p>This brilliant design makes your PyTorch code incredibly portable. You write the incantation once, and ATen ensures it is executed with maximum power on whatever hardware you possess!</p>","tags":["tensors","module 1","beginner"],"boost":1.2},{"location":"01-tensors/03_data_types_and_devices/#the-ritual-dynamic-device-placement","title":"The Ritual: Dynamic Device Placement\u00b6","text":"<p>A true PyTorch master does not hardcode their device! That is the way of the amateur. We shall write a glorious, platform-agnostic spell that automatically detects and selects the most powerful computational device available.</p> <p>The hierarchy is clear: <code>CUDA</code> is the sanctum sanctorum, <code>MPS</code> is the respected wizard's tower, and <code>CPU</code> is our reliable home laboratory. Our code shall seek the most powerful realm first.</p>","tags":["tensors","module 1","beginner"],"boost":1.2},{"location":"01-tensors/03_data_types_and_devices/#speed-trials","title":"Speed Trials\u00b6","text":"","tags":["tensors","module 1","beginner"],"boost":1.2},{"location":"01-tensors/03_data_types_and_devices/#comparison-of-speed-of-different-floating-point-types-on-cpu-and-gpu","title":"Comparison of speed of different floating-point types on CPU and GPU\u00b6","text":"<p>Now for a truly electrifying experiment! We shall create colossal tensors of different floating-point <code>dtypes</code> and subject them to a barrage of intense, element-wise mathematical operations. This will reveal the dramatic speed differences between our alchemical elixirs when processed in the massively parallel crucible of a GPU.</p>","tags":["tensors","module 1","beginner"],"boost":1.2},{"location":"01-tensors/03_data_types_and_devices/#professor-torchensteins-outro","title":"Professor Torchenstein's Outro\u00b6","text":"<p>Mwahahaha! Do you feel it? The hum of raw computational power at your fingertips? You have transcended the mundane world of default settings and seized control of the very essence of your tensors. You are no longer a mere summoner; you are an alchemist and a dimensional traveler!</p> <p>You have learned to choose your weapons\u2014the precise <code>dtype</code> for the task at hand and the mightiest <code>device</code> for your computations. This knowledge is the bedrock upon which all great neural architectures are built.</p> <p>But do not rest easy! Our journey has just begun. The tensors are humming, eager for the next lesson where we shall unleash their raw mathematical power with Elemental Tensor Alchemy!</p> <p>Until then, keep your learning rates high and your devices hotter! The future... is computational!</p>    Your browser does not support the video tag. Please update your browser to view this content.","tags":["tensors","module 1","beginner"],"boost":1.2},{"location":"01-tensors/04_tensor_math_operations/","title":"Tensor Math Operations","text":"","tags":["tensors","module 1","beginner"],"boost":1.2},{"location":"01-tensors/04_tensor_math_operations/#tensor-math-operations","title":"Tensor Math Operations\u00b6","text":"","tags":["tensors","module 1","beginner"],"boost":1.2},{"location":"01-tensors/05_matrix_multiplication/","title":"Matrix Multiplication: Unleashing the Power of Tensors! \u26a1","text":"In\u00a0[2]: Copied! <pre>import torch\n\n# Create some matrices for experimentation\nA = torch.randn(3, 4)\nB = torch.randn(4, 2)\n\nprint(\"Matrix A shape:\", A.shape)\nprint(\"Matrix B shape:\", B.shape)\n\n# Matrix multiplication\nC = torch.matmul(A, B)\nprint(\"Result C shape:\", C.shape)\nprint(\"\\nMwahahaha! The matrices have been multiplied!\")\n</pre> import torch  # Create some matrices for experimentation A = torch.randn(3, 4) B = torch.randn(4, 2)  print(\"Matrix A shape:\", A.shape) print(\"Matrix B shape:\", B.shape)  # Matrix multiplication C = torch.matmul(A, B) print(\"Result C shape:\", C.shape) print(\"\\nMwahahaha! The matrices have been multiplied!\") <pre>Matrix A shape: torch.Size([3, 4])\nMatrix B shape: torch.Size([4, 2])\nResult C shape: torch.Size([3, 2])\n\nMwahahaha! The matrices have been multiplied!\n</pre> In\u00a0[1]: Copied! <pre>import time\nimport torch\n\n# 1. Our \"Model\" and \"Input\"\nfp32_matrix = torch.randn(2**10, 2**10)\nfp16_matrix = fp32_matrix.to(torch.float16)\nbf16_matrix = fp32_matrix.to(torch.bfloat16)\n\n\n# 2. The Symmetric Quantization Spell (zero_point = 0)\ndef quantize_symmetric(tensor, dtype=torch.int8):\n    ''' \n    This function quantizes a tensor to desired integer dtype.\n    It quantizes the  tesnsor based on the absolute max value in the tensor.\n    '''\n\n    assert dtype in [torch.int8, torch.int16, torch.int32]\n\n    int_max = torch.iinfo(dtype).max\n    int_min = torch.iinfo(dtype).min\n    # Find the scale: map the absolute max of the tensor to the quantization range\n    scale = tensor.abs().max() / int_max\n    # Quantize\n    quantized_tensor = (tensor / scale).round().clamp(int_min, int_max).to(dtype)\n    return quantized_tensor, scale\n\n# 3. Quantize input and weights INDEPENDENTLY\nint8_quantized_matrix, scale_matrix = quantize_symmetric(fp32_matrix, dtype=torch.int8)\nint16_quantized_matrix, scale_matrix = quantize_symmetric(fp32_matrix, dtype=torch.int16)\nint32_quantized_matrix, scale_matrix = quantize_symmetric(fp32_matrix, dtype=torch.int32)\n\nprint(f\"Original fp32 matrix size: {fp32_matrix.element_size() * fp32_matrix.nelement() / 1024**2:.2f} MB\")\nprint(f\"fp16 matrix size: {fp16_matrix.element_size() * fp16_matrix.nelement() / 1024**2:.2f} MB\")\nprint(f\"bf16 matrix size: {bf16_matrix.element_size() * bf16_matrix.nelement() / 1024**2:.2f} MB\")\n\nprint(f\"Quantized int8 matrix size: {int8_quantized_matrix.element_size() * int8_quantized_matrix.nelement() / 1024**2:.2f} MB \")\nprint(f\"Quantized int16 matrix size: {int16_quantized_matrix.element_size() * int16_quantized_matrix.nelement() / 1024**2:.2f} MB \")\nprint(f\"Quantized int32 matrix size: {int32_quantized_matrix.element_size() * int32_quantized_matrix.nelement() / 1024**2:.2f} MB \")\n\n# 4. Perform and Time operations\n\n# --- Device Information ---\ncpu_device = torch.device(\"cpu\")\n\n\n\nprint(\"\\n--- CPU Timings ---\")\n\n# --- CPU Operations ---\nstart_time = time.time()\nfp32_output = torch.matmul(fp32_matrix, fp32_matrix)\nfp32_time = time.time() - start_time\nprint(f\"Float32 matmul on CPU took: {fp32_time:.6f} seconds\")\n\nstart_time = time.time()\nfp16_output = torch.matmul(fp16_matrix, fp16_matrix)\nfp16_time = time.time() - start_time\nprint(f\"Float16 matmul on CPU took: {fp16_time:.6f} seconds\")\n\nstart_time = time.time()\nbf16_output = torch.matmul(bf16_matrix, bf16_matrix)\nbf16_time = time.time() - start_time\nprint(f\"BFloat16 matmul on CPU took: {bf16_time:.6f} seconds\")\n\n\n#int matrix multiplication\nstart_time = time.time()\nint8_output = torch.matmul(int8_quantized_matrix, int8_quantized_matrix)\nint8_time = time.time() - start_time\nprint(f\"Int8 matmul on CPU took: {int8_time:.6f} seconds\")\n\nstart_time = time.time()\nint16_output = torch.matmul(int16_quantized_matrix, int16_quantized_matrix)\nint16_time = time.time() - start_time\nprint(f\"Int16 matmul on CPU took: {int16_time:.6f} seconds\")\n\nstart_time = time.time()\nint32_output = torch.matmul(int32_quantized_matrix, int32_quantized_matrix)\nint32_time = time.time() - start_time\nprint(f\"Int32 matmul on CPU took: {int32_time:.6f} seconds\")\n</pre> import time import torch  # 1. Our \"Model\" and \"Input\" fp32_matrix = torch.randn(2**10, 2**10) fp16_matrix = fp32_matrix.to(torch.float16) bf16_matrix = fp32_matrix.to(torch.bfloat16)   # 2. The Symmetric Quantization Spell (zero_point = 0) def quantize_symmetric(tensor, dtype=torch.int8):     '''      This function quantizes a tensor to desired integer dtype.     It quantizes the  tesnsor based on the absolute max value in the tensor.     '''      assert dtype in [torch.int8, torch.int16, torch.int32]      int_max = torch.iinfo(dtype).max     int_min = torch.iinfo(dtype).min     # Find the scale: map the absolute max of the tensor to the quantization range     scale = tensor.abs().max() / int_max     # Quantize     quantized_tensor = (tensor / scale).round().clamp(int_min, int_max).to(dtype)     return quantized_tensor, scale  # 3. Quantize input and weights INDEPENDENTLY int8_quantized_matrix, scale_matrix = quantize_symmetric(fp32_matrix, dtype=torch.int8) int16_quantized_matrix, scale_matrix = quantize_symmetric(fp32_matrix, dtype=torch.int16) int32_quantized_matrix, scale_matrix = quantize_symmetric(fp32_matrix, dtype=torch.int32)  print(f\"Original fp32 matrix size: {fp32_matrix.element_size() * fp32_matrix.nelement() / 1024**2:.2f} MB\") print(f\"fp16 matrix size: {fp16_matrix.element_size() * fp16_matrix.nelement() / 1024**2:.2f} MB\") print(f\"bf16 matrix size: {bf16_matrix.element_size() * bf16_matrix.nelement() / 1024**2:.2f} MB\")  print(f\"Quantized int8 matrix size: {int8_quantized_matrix.element_size() * int8_quantized_matrix.nelement() / 1024**2:.2f} MB \") print(f\"Quantized int16 matrix size: {int16_quantized_matrix.element_size() * int16_quantized_matrix.nelement() / 1024**2:.2f} MB \") print(f\"Quantized int32 matrix size: {int32_quantized_matrix.element_size() * int32_quantized_matrix.nelement() / 1024**2:.2f} MB \")  # 4. Perform and Time operations  # --- Device Information --- cpu_device = torch.device(\"cpu\")    print(\"\\n--- CPU Timings ---\")  # --- CPU Operations --- start_time = time.time() fp32_output = torch.matmul(fp32_matrix, fp32_matrix) fp32_time = time.time() - start_time print(f\"Float32 matmul on CPU took: {fp32_time:.6f} seconds\")  start_time = time.time() fp16_output = torch.matmul(fp16_matrix, fp16_matrix) fp16_time = time.time() - start_time print(f\"Float16 matmul on CPU took: {fp16_time:.6f} seconds\")  start_time = time.time() bf16_output = torch.matmul(bf16_matrix, bf16_matrix) bf16_time = time.time() - start_time print(f\"BFloat16 matmul on CPU took: {bf16_time:.6f} seconds\")   #int matrix multiplication start_time = time.time() int8_output = torch.matmul(int8_quantized_matrix, int8_quantized_matrix) int8_time = time.time() - start_time print(f\"Int8 matmul on CPU took: {int8_time:.6f} seconds\")  start_time = time.time() int16_output = torch.matmul(int16_quantized_matrix, int16_quantized_matrix) int16_time = time.time() - start_time print(f\"Int16 matmul on CPU took: {int16_time:.6f} seconds\")  start_time = time.time() int32_output = torch.matmul(int32_quantized_matrix, int32_quantized_matrix) int32_time = time.time() - start_time print(f\"Int32 matmul on CPU took: {int32_time:.6f} seconds\")   <pre>Original fp32 matrix size: 4.00 MB\nfp16 matrix size: 2.00 MB\nbf16 matrix size: 2.00 MB\nQuantized int8 matrix size: 1.00 MB \nQuantized int16 matrix size: 2.00 MB \nQuantized int32 matrix size: 4.00 MB \n\n--- CPU Timings ---\nFloat32 matmul on CPU took: 0.009423 seconds\nFloat16 matmul on CPU took: 2.737493 seconds\nBFloat16 matmul on CPU took: 3.497367 seconds\nInt8 matmul on CPU took: 0.499707 seconds\nInt16 matmul on CPU took: 0.545151 seconds\nInt32 matmul on CPU took: 0.543347 seconds\n</pre> In\u00a0[2]: Copied! <pre>import time\nimport torch\n\n# 1. Our \"Model\" and \"Input\"\nfp32_matrix = torch.randn(2**10, 2**10)\nfp16_matrix = fp32_matrix.to(torch.float16)\nbf16_matrix = fp32_matrix.to(torch.bfloat16)\n\n\n# 2. The Symmetric Quantization Spell (zero_point = 0)\ndef quantize_symmetric(tensor, dtype=torch.int8):\n    ''' \n    This function quantizes a tensor to desired integer dtype.\n    It quantizes the  tesnsor based on the absolute max value in the tensor.\n    '''\n\n    assert dtype in [torch.int8, torch.int16, torch.int32]\n\n    int_max = torch.iinfo(dtype).max\n    int_min = torch.iinfo(dtype).min\n    # Find the scale: map the absolute max of the tensor to the quantization range\n    scale = tensor.abs().max() / int_max\n    # Quantize\n    quantized_tensor = (tensor / scale).round().clamp(int_min, int_max).to(dtype)\n    return quantized_tensor, scale\n\n# 3. Quantize input and weights INDEPENDENTLY\nint8_quantized_matrix, scale_matrix = quantize_symmetric(fp32_matrix, dtype=torch.int8)\nint16_quantized_matrix, scale_matrix = quantize_symmetric(fp32_matrix, dtype=torch.int16)\nint32_quantized_matrix, scale_matrix = quantize_symmetric(fp32_matrix, dtype=torch.int32)\n\n# --- GPU Operations ---\nif torch.cuda.is_available():\n    gpu_device = torch.device(\"cuda\")\n    print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")\nelse:\n    gpu_device = None\n    print(\"GPU not available.\")\nif gpu_device:\n    print(\"\\n--- GPU Timings ---\")\n\n    # Move matrices to GPU\n    fp32_matrix_gpu = fp32_matrix.to(gpu_device)\n    fp16_matrix_gpu = fp16_matrix.to(gpu_device)\n    bf16_matrix_gpu = bf16_matrix.to(gpu_device)\n    int8_quantized_matrix_gpu = int8_quantized_matrix.to(gpu_device)\n    int16_quantized_matrix_gpu = int16_quantized_matrix.to(gpu_device)\n    int32_quantized_matrix_gpu = int32_quantized_matrix.to(gpu_device)\n    \n    # Correct timing for GPU operations requires synchronization\n    \n    # FP32 on GPU\n    start = torch.cuda.Event(enable_timing=True)\n    end = torch.cuda.Event(enable_timing=True)\n    start.record()\n    fp32_output_gpu = torch.matmul(fp32_matrix_gpu, fp32_matrix_gpu)\n    end.record()\n    torch.cuda.synchronize()\n    fp32_time_gpu = start.elapsed_time(end) / 1000  # Convert ms to s\n    print(f\"Float32 matmul on GPU took: {fp32_time_gpu:.6f} seconds\")\n\n    # FP16 on GPU\n    start.record()\n    fp16_output_gpu = torch.matmul(fp16_matrix_gpu, fp16_matrix_gpu)\n    end.record()\n    torch.cuda.synchronize()\n    fp16_time_gpu = start.elapsed_time(end) / 1000\n    print(f\"Float16 matmul on GPU took: {fp16_time_gpu:.6f} seconds\")\n\n    # BF16 on GPU\n    # Note: BF16 performance depends on GPU architecture (Ampere and newer)\n    try:\n        start.record()\n        bf16_output_gpu = torch.matmul(bf16_matrix_gpu, bf16_matrix_gpu)\n        end.record()\n        torch.cuda.synchronize()\n        bf16_time_gpu = start.elapsed_time(end) / 1000\n        print(f\"BFloat16 matmul on GPU took: {bf16_time_gpu:.6f} seconds\")\n    except Exception as e:\n        print(f\"BFloat16 matmul on GPU failed: {e}\")\n\n    # INT32 on GPU\n    start.record()\n    int32_output_gpu = torch.mm(int32_quantized_matrix_gpu, int32_quantized_matrix_gpu)\n    end.record()\n    torch.cuda.synchronize()\n    int32_time_gpu = start.elapsed_time(end) / 1000\n    print(f\"Int32 matmul on GPU took: {int32_time_gpu:.6f} seconds\")\n\n    # INT16 on GPU\n    start.record()\n    int16_output_gpu = torch.matmul(int16_quantized_matrix_gpu, int16_quantized_matrix_gpu)\n    end.record()\n    torch.cuda.synchronize()\n    int16_time_gpu = start.elapsed_time(end) / 1000\n    print(f\"Int16 matmul on GPU took: {int16_time_gpu:.6f} seconds\")\n\n    # INT8 on GPU\n    start.record()\n    int8_output_gpu = torch.matmul(int8_quantized_matrix_gpu, int8_quantized_matrix_gpu)\n    end.record()\n    torch.cuda.synchronize()\n    int8_time_gpu = start.elapsed_time(end) / 1000\n    print(f\"Int8 matmul on GPU took: {int8_time_gpu:.6f} seconds\")\n</pre> import time import torch  # 1. Our \"Model\" and \"Input\" fp32_matrix = torch.randn(2**10, 2**10) fp16_matrix = fp32_matrix.to(torch.float16) bf16_matrix = fp32_matrix.to(torch.bfloat16)   # 2. The Symmetric Quantization Spell (zero_point = 0) def quantize_symmetric(tensor, dtype=torch.int8):     '''      This function quantizes a tensor to desired integer dtype.     It quantizes the  tesnsor based on the absolute max value in the tensor.     '''      assert dtype in [torch.int8, torch.int16, torch.int32]      int_max = torch.iinfo(dtype).max     int_min = torch.iinfo(dtype).min     # Find the scale: map the absolute max of the tensor to the quantization range     scale = tensor.abs().max() / int_max     # Quantize     quantized_tensor = (tensor / scale).round().clamp(int_min, int_max).to(dtype)     return quantized_tensor, scale  # 3. Quantize input and weights INDEPENDENTLY int8_quantized_matrix, scale_matrix = quantize_symmetric(fp32_matrix, dtype=torch.int8) int16_quantized_matrix, scale_matrix = quantize_symmetric(fp32_matrix, dtype=torch.int16) int32_quantized_matrix, scale_matrix = quantize_symmetric(fp32_matrix, dtype=torch.int32)  # --- GPU Operations --- if torch.cuda.is_available():     gpu_device = torch.device(\"cuda\")     print(f\"GPU Name: {torch.cuda.get_device_name(0)}\") else:     gpu_device = None     print(\"GPU not available.\") if gpu_device:     print(\"\\n--- GPU Timings ---\")      # Move matrices to GPU     fp32_matrix_gpu = fp32_matrix.to(gpu_device)     fp16_matrix_gpu = fp16_matrix.to(gpu_device)     bf16_matrix_gpu = bf16_matrix.to(gpu_device)     int8_quantized_matrix_gpu = int8_quantized_matrix.to(gpu_device)     int16_quantized_matrix_gpu = int16_quantized_matrix.to(gpu_device)     int32_quantized_matrix_gpu = int32_quantized_matrix.to(gpu_device)          # Correct timing for GPU operations requires synchronization          # FP32 on GPU     start = torch.cuda.Event(enable_timing=True)     end = torch.cuda.Event(enable_timing=True)     start.record()     fp32_output_gpu = torch.matmul(fp32_matrix_gpu, fp32_matrix_gpu)     end.record()     torch.cuda.synchronize()     fp32_time_gpu = start.elapsed_time(end) / 1000  # Convert ms to s     print(f\"Float32 matmul on GPU took: {fp32_time_gpu:.6f} seconds\")      # FP16 on GPU     start.record()     fp16_output_gpu = torch.matmul(fp16_matrix_gpu, fp16_matrix_gpu)     end.record()     torch.cuda.synchronize()     fp16_time_gpu = start.elapsed_time(end) / 1000     print(f\"Float16 matmul on GPU took: {fp16_time_gpu:.6f} seconds\")      # BF16 on GPU     # Note: BF16 performance depends on GPU architecture (Ampere and newer)     try:         start.record()         bf16_output_gpu = torch.matmul(bf16_matrix_gpu, bf16_matrix_gpu)         end.record()         torch.cuda.synchronize()         bf16_time_gpu = start.elapsed_time(end) / 1000         print(f\"BFloat16 matmul on GPU took: {bf16_time_gpu:.6f} seconds\")     except Exception as e:         print(f\"BFloat16 matmul on GPU failed: {e}\")      # INT32 on GPU     start.record()     int32_output_gpu = torch.mm(int32_quantized_matrix_gpu, int32_quantized_matrix_gpu)     end.record()     torch.cuda.synchronize()     int32_time_gpu = start.elapsed_time(end) / 1000     print(f\"Int32 matmul on GPU took: {int32_time_gpu:.6f} seconds\")      # INT16 on GPU     start.record()     int16_output_gpu = torch.matmul(int16_quantized_matrix_gpu, int16_quantized_matrix_gpu)     end.record()     torch.cuda.synchronize()     int16_time_gpu = start.elapsed_time(end) / 1000     print(f\"Int16 matmul on GPU took: {int16_time_gpu:.6f} seconds\")      # INT8 on GPU     start.record()     int8_output_gpu = torch.matmul(int8_quantized_matrix_gpu, int8_quantized_matrix_gpu)     end.record()     torch.cuda.synchronize()     int8_time_gpu = start.elapsed_time(end) / 1000     print(f\"Int8 matmul on GPU took: {int8_time_gpu:.6f} seconds\") <pre>GPU Name: NVIDIA GeForce RTX 3080 Laptop GPU\n\n--- GPU Timings ---\nFloat32 matmul on GPU took: 0.107393 seconds\nFloat16 matmul on GPU took: 0.234943 seconds\nBFloat16 matmul on GPU took: 0.116011 seconds\n</pre> <pre>\n---------------------------------------------------------------------------\nRuntimeError                              Traceback (most recent call last)\nCell In[2], line 84\n     82 # INT32 on GPU\n     83 start.record()\n---&gt; 84 int32_output_gpu = torch.mm(int32_quantized_matrix_gpu, int32_quantized_matrix_gpu)\n     85 end.record()\n     86 torch.cuda.synchronize()\n\nRuntimeError: \"addmm_cuda\" not implemented for 'Int'</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>","tags":["tensors","module 1","beginner"],"boost":1.2},{"location":"01-tensors/05_matrix_multiplication/#matrix-multiplication-unleashing-the-power-of-tensors","title":"Matrix Multiplication: Unleashing the Power of Tensors! \u26a1\u00b6","text":"<p>\"Behold! The sacred art of matrix multiplication - where dimensions dance and vectors bend to my will!\" \u2014 Professor Victor py Torchenstein</p>","tags":["tensors","module 1","beginner"],"boost":1.2},{"location":"01-tensors/05_matrix_multiplication/#the-attention-formula-preview-of-things-to-come","title":"The Attention Formula (Preview of Things to Come)\u00b6","text":"<p>$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$</p> <p>Where:</p> <ul> <li>$Q$ is the Query matrix</li> <li>$K$ is the Key matrix</li> <li>$V$ is the Value matrix</li> <li>$d_k$ is the dimension of the key vectors</li> <li>$\\text{softmax}$ normalizes the attention weights</li> </ul>","tags":["tensors","module 1","beginner"],"boost":1.2},{"location":"01-tensors/05_matrix_multiplication/#basic-matrix-operations","title":"Basic Matrix Operations\u00b6","text":"<p>Let's start with the fundamentals before we conquer attention mechanisms!</p> <p>Element-wise multiplication:</p> <p>$C_{ij} = A_{ij} \\times B_{ij}$</p> <p>Matrix multiplication: $C_{ij} = \\sum_{k} A_{ik} \\times B_{kj}$</p>","tags":["tensors","module 1","beginner"],"boost":1.2},{"location":"01-tensors/05_matrix_multiplication/#pytorch-matrix-multiplication-methods","title":"PyTorch Matrix Multiplication Methods\u00b6","text":"<p>Professor Torchenstein's arsenal includes multiple ways to multiply matrices:</p> <ol> <li><code>torch.matmul()</code> - The general matrix multiplication function</li> <li><code>@</code> operator - Pythonic matrix multiplication (same as matmul)</li> <li><code>torch.mm()</code> - For 2D matrices only</li> <li><code>torch.bmm()</code> - Batch matrix multiplication</li> </ol>","tags":["tensors","module 1","beginner"],"boost":1.2},{"location":"01-tensors/05_matrix_multiplication/#mathematical-foundations","title":"Mathematical Foundations\u00b6","text":"<p>For matrices $A \\in \\mathbb{R}^{m \\times n}$ and $B \\in \\mathbb{R}^{n \\times p}$:</p> <p>$$C = AB \\quad \\text{where} \\quad C_{ij} = \\sum_{k=1}^{n} A_{ik} B_{kj}$$</p> <p>This operation is fundamental to:</p> <ul> <li>Linear transformations</li> <li>Neural network forward passes</li> <li>Attention mechanisms in Transformers</li> <li>And much more! \ud83e\udde0\u26a1</li> </ul>","tags":["tensors","module 1","beginner"],"boost":1.2},{"location":"01-tensors/05_matrix_multiplication/#floats-and-integers-on-cpu-and-gpu","title":"Floats and Integers on CPU and GPU\u00b6","text":"<p>Now we will compare the speed matrix multiplication of different floating-point <code>float16</code> and <code>bfloat16</code> with <code>int32</code>, <code>int16</code>, <code>int8</code> on CPU and GPU.</p>","tags":["tensors","module 1","beginner"],"boost":1.2},{"location":"01-tensors/05_matrix_multiplication/#the-mystery-of-the-slow-matrix-multiplication-of-float16-on-cpu","title":"The Mystery of the Slow Matrix Multiplication of Float16 on CPU\u00b6","text":"<p>You've stumbled upon a crucial secret of computational alchemy: performance isn't just about size, it's about the machine's native language!</p> <p>You're seeing these surprising results because of how your CPU is designed.</p> <ol> <li><p><code>float32</code> (The Native Tongue): Your CPU is a fluent, native speaker of <code>float32</code>. It has highly optimized, dedicated hardware circuits (like SSE and AVX instructions) built specifically to perform these calculations at blistering speeds. It's the language it thinks in.</p> </li> <li><p><code>float16</code> &amp; <code>bfloat16</code> (A Foreign Language): Your CPU does not speak <code>float16</code> or <code>bfloat16</code> natively. When you command it to perform a matrix multiplication with these types, it has to do a lot of extra work behind the scenes:</p> <ul> <li>It takes a <code>float16</code> number.</li> <li>It painstakingly translates (casts) it up to <code>float32</code>.</li> <li>It performs the calculation in <code>float32</code>.</li> <li>It translates (casts) the result back down to <code>float16</code>.</li> </ul> <p>This process, known as software emulation, is incredibly slow. It's like asking a brilliant English-speaking scientist to solve a complex physics problem written entirely in a language they don't know, forcing them to use a dictionary for every single word. The overhead is enormous!</p> </li> <li><p>Integers (<code>int8</code>, <code>int16</code>, <code>int32</code>): Your CPU is also very good at integer math. These operations are fast and natively supported. The reason they are a bit slower than <code>float32</code> in this case is that the underlying math libraries (like Intel's MKL) that PyTorch uses for <code>torch.matmul</code> are often most aggressively optimized for <code>float32</code> operations on large matrices.</p> </li> </ol>","tags":["tensors","module 1","beginner"],"boost":1.2},{"location":"01-tensors/05_matrix_multiplication/#the-grand-takeaway","title":"The Grand Takeaway\u00b6","text":"<p><code>float16</code> and <code>bfloat16</code> are GPU datatypes. On a modern GPU (like NVIDIA's A100 or H100), these operations would be dramatically faster than <code>float32</code> because GPUs have specialized hardware (Tensor Cores) built for exactly this kind of low-precision, high-throughput math.</p> <p>You've learned a vital lesson: always match your data type to your hardware's strengths! For CPUs, <code>float32</code> is king for floating-point operations.</p>","tags":["tensors","module 1","beginner"],"boost":1.2},{"location":"01-tensors/05_matrix_multiplication/#pytorch-doesnt-support-int-matrix-multiplication","title":"Pytorch doesn't support int matrix multiplication\u00b6","text":"","tags":["tensors","module 1","beginner"],"boost":1.2},{"location":"01-tensors/06_broadcasting/","title":"Broadcasting","text":"","tags":["tensors","module 1","beginner"],"boost":1.2},{"location":"01-tensors/06_broadcasting/#broadcasting","title":"Broadcasting\u00b6","text":"","tags":["tensors","module 1","beginner"],"boost":1.2},{"location":"01-tensors/07_einstein_summation/","title":"Einstein Summation","text":"","tags":["tensors","module 1","beginner"],"boost":1.2},{"location":"01-tensors/07_einstein_summation/#einstein-summation","title":"Einstein Summation\u00b6","text":"","tags":["tensors","module 1","beginner"],"boost":1.2},{"location":"01-tensors/08_advanced_einstein_summation/","title":"Advanced Einstein Summation","text":"","tags":["tensors","module 1","beginner"],"boost":1.2},{"location":"01-tensors/08_advanced_einstein_summation/#advanced-einstein-summation","title":"Advanced Einstein Summation\u00b6","text":"","tags":["tensors","module 1","beginner"],"boost":1.2},{"location":"01-tensors/09_autograd/","title":"Autograd","text":"","tags":["tensors","module 1","beginner"],"boost":1.2},{"location":"01-tensors/09_autograd/#autograd","title":"Autograd\u00b6","text":"","tags":["tensors","module 1","beginner"],"boost":1.2},{"location":"01-tensors/10_gradient_accumulation/","title":"Gradient Accumulation","text":"","tags":["tensors","module 1","beginner"],"boost":1.2},{"location":"01-tensors/10_gradient_accumulation/#gradient-accumulation","title":"Gradient Accumulation\u00b6","text":"","tags":["tensors","module 1","beginner"],"boost":1.2},{"location":"02-torch-nn/00_module_2_introduction/","title":"Module 2 \u2013 torch.nn: Building Neural Networks","text":"<p>\"From mere tensors we forge SENTIENT NETWORKS! Witness the birth of computational consciousness!\"</p> <p>\u2014 Prof. Torchenstein</p> <p>Welcome back, my diligent apprentices! Having mastered the fundamental art of tensor manipulation, you are now ready for the next phase of your transformation: breathing life into neural architectures! In this module, we shall wield <code>torch.nn</code> as both scalpel and forge, assembling layers and models worthy of legend! \u26a1\ufe0f\ud83e\uddea</p> <p></p>","tags":["neural networks","module 2","intermediate","neural networks","intermediate","module 2","module overview"],"boost":2.0},{"location":"02-torch-nn/00_module_2_introduction/#what-awaits-in-this-module","title":"What Awaits in This Module","text":"<p>In Module 2, we transition from raw tensor operations to the high-level building blocks that make PyTorch a joy to work with:</p> <ul> <li>Construct modular architectures using <code>nn.Module</code> as your blueprint</li> <li>Layer upon layer \u2013 from humble linear transformations to exotic normalization techniques</li> <li>Activate with purpose \u2013 ReLU, GELU, SiLU, and their mathematical rationale</li> <li>Encode position and meaning with embeddings and positional encodings</li> <li>Normalize like a pro \u2013 BatchNorm, LayerNorm, RMSNorm, and when to use each</li> <li>Master the training cycle \u2013 understanding loss functions, training vs. eval modes, and preparing data</li> </ul>","tags":["neural networks","module 2","intermediate","neural networks","intermediate","module 2","module overview"],"boost":2.0},{"location":"02-torch-nn/00_module_2_introduction/#rebel-mission-checklist","title":"Rebel Mission Checklist \ud83d\udcdd","text":"","tags":["neural networks","module 2","intermediate","neural networks","intermediate","module 2","module overview"],"boost":2.0},{"location":"02-torch-nn/00_module_2_introduction/#the-nnmodule-blueprint","title":"The nn.Module Blueprint","text":"<ol> <li>Building Brains with nn.Module - Craft custom neural matter by overriding <code>__init__</code> and <code>forward</code></li> <li>Franken-Stacking Layers - Bolt modules together with Sequential, ModuleList, and ModuleDict</li> <li>Preserving Your Monster's Memories - Save and resurrect model weights with state_dict necromancy</li> </ol>","tags":["neural networks","module 2","intermediate","neural networks","intermediate","module 2","module overview"],"boost":2.0},{"location":"02-torch-nn/00_module_2_introduction/#linear-layers-and-activations","title":"Linear Layers and Activations","text":"<ol> <li>Linear Layers: The Vector Guillotine - Slice through dimensions, turning inputs into finely-chopped activations</li> <li>Activation Elixirs - Re-animate neurons with ReLU, GELU, SiLU, and other zesty potions</li> <li>Dropout: Neural Regularization - Make neurons forget just enough to generalize</li> </ol>","tags":["neural networks","module 2","intermediate","neural networks","intermediate","module 2","module overview"],"boost":2.0},{"location":"02-torch-nn/00_module_2_introduction/#embeddings-and-positional-encoding","title":"Embeddings and Positional Encoding","text":"<ol> <li>Embedding Layers: Secret Identity Chips - Embed discrete meanings within high-dimensional space</li> <li>Positional Encoding: Injecting Order - Imbue sequences with a sense of place so attention never loses its bearings</li> </ol>","tags":["neural networks","module 2","intermediate","neural networks","intermediate","module 2","module overview"],"boost":2.0},{"location":"02-torch-nn/00_module_2_introduction/#normalization-techniques","title":"Normalization Techniques","text":"<ol> <li>Normalization: Calming the Beast - Tame activations with BatchNorm and LayerNorm before they explode</li> <li>RMSNorm &amp; Other Exotic Tonics - Sample contemporary concoctions for stable training</li> <li>Train vs. Eval: Split Personalities - Toggle modes and avoid awkward identity crises</li> </ol>","tags":["neural networks","module 2","intermediate","neural networks","intermediate","module 2","module overview"],"boost":2.0},{"location":"02-torch-nn/00_module_2_introduction/#loss-functions-and-training","title":"Loss Functions and Training","text":"<ol> <li>Loss Potions: Guiding Pain into Progress - Channel model errors into gradients that sharpen intelligence</li> <li>Preparing Sacrificial Inputs &amp; Targets - Align logits and labels for maximum learning agony</li> <li>Reduction Rituals &amp; Ignore Indices - Decipher reduction modes and skip unworthy samples</li> </ol> <p>With these tools in hand, you will no longer be a mere tensor wrangler\u2014you will be an architect of intelligence! The path ahead is electric with possibility. Steel your nerves, charge your GPUs, and prepare for computational glory!</p> <p>Begin Your Transformation with nn.Module!</p>","tags":["neural networks","module 2","intermediate","neural networks","intermediate","module 2","module overview"],"boost":2.0},{"location":"02-torch-nn/01_nn_module/","title":"nn.Module","text":"","tags":["neural networks","module 2","intermediate"],"boost":1.2},{"location":"02-torch-nn/01_nn_module/#nnmodule","title":"nn.Module\u00b6","text":"","tags":["neural networks","module 2","intermediate"],"boost":1.2},{"location":"02-torch-nn/02_compose_modules/","title":"Compose Modules","text":"","tags":["neural networks","module 2","intermediate"],"boost":1.2},{"location":"02-torch-nn/02_compose_modules/#compose-modules","title":"Compose Modules\u00b6","text":"","tags":["neural networks","module 2","intermediate"],"boost":1.2},{"location":"02-torch-nn/03_saving_weights/","title":"Saving Weights","text":"","tags":["neural networks","module 2","intermediate"],"boost":1.2},{"location":"02-torch-nn/03_saving_weights/#saving-weights","title":"Saving Weights\u00b6","text":"","tags":["neural networks","module 2","intermediate"],"boost":1.2},{"location":"02-torch-nn/04_linear_layer/","title":"Linear Layer","text":"","tags":["neural networks","module 2","intermediate"],"boost":1.2},{"location":"02-torch-nn/04_linear_layer/#linear-layer","title":"Linear Layer\u00b6","text":"","tags":["neural networks","module 2","intermediate"],"boost":1.2},{"location":"02-torch-nn/05_activations/","title":"Activations","text":"","tags":["neural networks","module 2","intermediate"],"boost":1.2},{"location":"02-torch-nn/05_activations/#activations","title":"Activations\u00b6","text":"","tags":["neural networks","module 2","intermediate"],"boost":1.2},{"location":"02-torch-nn/06_dropout/","title":"Dropout","text":"","tags":["neural networks","module 2","intermediate"],"boost":1.2},{"location":"02-torch-nn/06_dropout/#dropout","title":"Dropout\u00b6","text":"","tags":["neural networks","module 2","intermediate"],"boost":1.2},{"location":"02-torch-nn/07_embedding_layers/","title":"Embedding Layers","text":"","tags":["neural networks","module 2","intermediate"],"boost":1.2},{"location":"02-torch-nn/07_embedding_layers/#embedding-layers","title":"Embedding Layers\u00b6","text":"","tags":["neural networks","module 2","intermediate"],"boost":1.2},{"location":"02-torch-nn/08_positional_encoding/","title":"Positional Embeddings","text":"","tags":["neural networks","module 2","intermediate"],"boost":1.2},{"location":"02-torch-nn/08_positional_encoding/#positional-embeddings","title":"Positional Embeddings\u00b6","text":"<p>How to encode the token position in the sequence?</p> <p>References:</p> <ul> <li>Mastering LLAMA: Understanding Rotary Positional Embedding (RPE)</li> </ul>","tags":["neural networks","module 2","intermediate"],"boost":1.2},{"location":"02-torch-nn/09_normalization_layers/","title":"Normalization Layers","text":"","tags":["neural networks","module 2","intermediate"],"boost":1.2},{"location":"02-torch-nn/09_normalization_layers/#normalization-layers","title":"Normalization Layers\u00b6","text":"<p>Estimated learning time: 15 minutes</p> <p>Learning objectives:</p> <ul> <li>Understand the basics of normalization layers: batch normalization, layer normalization, instance normalization, group normalization.</li> <li>Understand the difference between these normalization layers.</li> <li>Know how to use these normalization layers in practice. Compare the results of using different normalization layers.</li> </ul> <p>Resources:</p> <ul> <li>RMSNorm - a better normalization layer</li> </ul>","tags":["neural networks","module 2","intermediate"],"boost":1.2},{"location":"02-torch-nn/10_rms_norm/","title":"RMS Norm","text":"","tags":["neural networks","module 2","intermediate"],"boost":1.2},{"location":"02-torch-nn/10_rms_norm/#rms-norm","title":"RMS Norm\u00b6","text":"","tags":["neural networks","module 2","intermediate"],"boost":1.2},{"location":"02-torch-nn/11_training_evaluation_mode/","title":"Training Evaluation Mode","text":"","tags":["neural networks","module 2","intermediate"],"boost":1.2},{"location":"02-torch-nn/11_training_evaluation_mode/#training-evaluation-mode","title":"Training Evaluation Mode\u00b6","text":"","tags":["neural networks","module 2","intermediate"],"boost":1.2},{"location":"02-torch-nn/12_loss_functions/","title":"Loss Functions","text":"","tags":["neural networks","module 2","intermediate"],"boost":1.2},{"location":"02-torch-nn/12_loss_functions/#loss-functions","title":"Loss Functions\u00b6","text":"","tags":["neural networks","module 2","intermediate"],"boost":1.2},{"location":"02-torch-nn/13_prepare_inputs_targets/","title":"Prepare Inputs Targets","text":"","tags":["neural networks","module 2","intermediate"],"boost":1.2},{"location":"02-torch-nn/13_prepare_inputs_targets/#prepare-inputs-targets","title":"Prepare Inputs Targets\u00b6","text":"","tags":["neural networks","module 2","intermediate"],"boost":1.2},{"location":"02-torch-nn/14_interpreting_reduction_modes/","title":"Interpreting Reduction Modes","text":"","tags":["neural networks","module 2","intermediate"],"boost":1.2},{"location":"02-torch-nn/14_interpreting_reduction_modes/#interpreting-reduction-modes","title":"Interpreting Reduction Modes\u00b6","text":"","tags":["neural networks","module 2","intermediate"],"boost":1.2},{"location":"03-training-nn/01_training_loop/","title":"Training Loop","text":""},{"location":"03-training-nn/01_training_loop/#training-loop","title":"Training Loop\u00b6","text":""},{"location":"03-training-nn/02_optimizers_schedulers/","title":"Optimizers Schedulers","text":""},{"location":"03-training-nn/02_optimizers_schedulers/#optimizers-schedulers","title":"Optimizers Schedulers\u00b6","text":""},{"location":"03-training-nn/03_datasets_dataloaders/","title":"Datasets DataLoaders","text":""},{"location":"03-training-nn/03_datasets_dataloaders/#datasets-dataloaders","title":"Datasets DataLoaders\u00b6","text":""},{"location":"03-training-nn/04_gpu_acceleration/","title":"GPU Acceleration","text":""},{"location":"03-training-nn/04_gpu_acceleration/#gpu-acceleration","title":"GPU Acceleration\u00b6","text":"<p>Learning objectives:</p> <ul> <li>Understand the GPU parallelisation: data parallel, model parallel, pipeline parallel, etc.</li> <li>How to use GPU acceleration to train a model.</li> <li>How to use FSDP and DeepSpeed in Accelerate to train a model on multiple GPUs.</li> </ul> <p>Resources:</p> <ul> <li>Make LLM training possible across multi-gpus using FSDP and DeepSpeed in Accelerate</li> </ul>"},{"location":"03-training-nn/05_training_optimization/","title":"Weight Initialization","text":""},{"location":"03-training-nn/05_training_optimization/#weight-initialization","title":"Weight Initialization\u00b6","text":""},{"location":"04-transformers/01_positional_embeddings/","title":"Positional Embeddings","text":""},{"location":"04-transformers/01_positional_embeddings/#positional-embeddings","title":"Positional Embeddings\u00b6","text":""},{"location":"04-transformers/02_attention_mechanism/","title":"Attention Mechanism","text":""},{"location":"04-transformers/02_attention_mechanism/#attention-mechanism","title":"Attention Mechanism\u00b6","text":""},{"location":"04-transformers/03_multi_head_attention/","title":"Multi-Head Attention","text":""},{"location":"04-transformers/03_multi_head_attention/#multi-head-attention","title":"Multi-Head Attention\u00b6","text":""},{"location":"04-transformers/04_other_attention_implementations/","title":"Other Attention Implementations","text":""},{"location":"04-transformers/04_other_attention_implementations/#other-attention-implementations","title":"Other Attention Implementations\u00b6","text":"<p>Resources:</p> <ul> <li>Accelerated PyTorch 2</li> <li>Out-of-the-box acceleration</li> </ul>"},{"location":"04-transformers/05_transformer_encoder/","title":"Transformer Encoder","text":""},{"location":"04-transformers/05_transformer_encoder/#transformer-encoder","title":"Transformer Encoder\u00b6","text":""},{"location":"05-advanced-pytorch/01_hooks/","title":"Hooks","text":""},{"location":"05-advanced-pytorch/01_hooks/#hooks","title":"Hooks\u00b6","text":""},{"location":"05-advanced-pytorch/02_distributed_training_concepts/","title":"Distributed Training Concepts","text":""},{"location":"05-advanced-pytorch/02_distributed_training_concepts/#distributed-training-concepts","title":"Distributed Training Concepts\u00b6","text":""},{"location":"05-advanced-pytorch/03_model_optimization_concepts/","title":"Model Optimization Concepts","text":""},{"location":"05-advanced-pytorch/03_model_optimization_concepts/#model-optimization-concepts","title":"Model Optimization Concepts\u00b6","text":""},{"location":"05-advanced-pytorch/04_torchscript_jit/","title":"Torchscript JIT","text":""},{"location":"05-advanced-pytorch/04_torchscript_jit/#torchscript-jit","title":"Torchscript JIT\u00b6","text":""},{"location":"05-advanced-pytorch/05_profiling/","title":"Profiling","text":""},{"location":"05-advanced-pytorch/05_profiling/#profiling","title":"Profiling\u00b6","text":""},{"location":"06-huggingface-transformers/01_huggingface_transformers/","title":"HuggingFace Transformers","text":""},{"location":"06-huggingface-transformers/01_huggingface_transformers/#huggingface-transformers","title":"HuggingFace Transformers\u00b6","text":""},{"location":"06-huggingface-transformers/02_fine_tuning_transformers/","title":"Fine Tuning Transformers","text":""},{"location":"06-huggingface-transformers/02_fine_tuning_transformers/#fine-tuning-transformers","title":"Fine Tuning Transformers\u00b6","text":""},{"location":"story/sponsor/","title":"\ud83d\udc96 Support the Education Revolution","text":"<p>\"They called me mad! And they were right!</p> <p>Madly passionate about giving everyone the power to understand PyTorch! To give courage to experiment, to build, to create!</p> <p>\u2014 Prof. Torchenstein</p> <p>Here's what I believe: brilliant minds are being lost to shallow education and attention thieves. We all have beautiful brains, we all are capable of understanding the fundamentals of AI, but we need engaging, creative education that sparks curiosity and engages us! While AI knowledge exists, traditional teaching methods often make complex concepts feel impossible to grasp.</p> <p>But here's where YOU come in! Together, we can create a new type of learning experience\u2014one that combines humor, creativity, and the wild enthusiasm of Professor Torchenstein! \u26a1\ufe0f</p>    Your browser does not support the video tag. Please update your browser to view this content.  <p>Your support directly funds new lessons, creative visualizations, and helps build a community where deep understanding triumphs over shallow knowledge. Join the rebellion!</p> <p>\"Wait! You want to know MORE about our MAGNIFICENT MISSION to overthrow the tyranny of shallow learning?!\" \ud83e\uddea\u26a1\ufe0f Discover the FULL story of our computational rebellion! \"MWAHAHAHA!\"</p>","tags":["sponsorship","community","support","open source"],"boost":2.0},{"location":"story/sponsor/#company-partnerships-join-the-enlightened-innovators-circle","title":"\ud83c\udfe2 Company Partnerships: Join the Enlightened Innovators Circle","text":"<p>\"Listen carefully, my corporate colleagues! The companies that will DOMINATE the future of AI aren't just hiring talent\u2014they're CREATING it! Invest in deep understanding NOW, and you'll harvest brilliant minds who can architect the impossible! Why wait for universities to train them when YOU can be the catalyst?\" \ud83c\udf1f</p> <p>Best for: Seed to Series B startups, AI tool companies, dev-focused brands, and consulting firms</p> <ul> <li> <p>\ud83d\udca1 Enlightened Innovator $250/month</p> <p>\"Together we shall BUILD the computational future! Your support empowers the next generation of AI minds to understand deeply, think critically, and innovate fearlessly!\" \u26a1\ufe0f</p> <p>What Your Partnership Includes:</p> <ul> <li>\ud83c\udfe2 Brand Visibility (Silver): Your company name, link and logo featured in the \"Enlightened Innovators\" section on our sponsor page and the main GitHub README</li> <li>\ud83d\udce2 Social Media Recognition: Dedicated thank-you post for your company on LinkedIn and X</li> <li>\ud83e\uddd1\u200d\ud83d\udcbb Talent Attraction: Up to 2 job post re-shares on LinkedIn (after 2 months of continuous support)</li> <li>\ud83d\udc65 Team Community Access: Free BMC membership for 3 team members, with access to community hub, posts, galleries, updates, and behind-the-scenes content from the lab</li> </ul> <p> </p> <p> Join the Enlightened Innovators</p> </li> <li> <p>\ud83e\udde0 Wisdom Catalyst $500/month</p> <p>\"You're not just funding education\u2014you're CATALYZING a revolution in how humanity learns AI! Your legacy will be ETCHED into every neural network we help developers master!\"</p> <p>What Your Partnership Includes:</p> <ul> <li>\ud83c\udf1f Premium Brand Visibility (Gold): Top-tier placement\u2014your company name, link and logo featured in the \"Wisdom Catalysts\" section on our sponsor page and the main GitHub README</li> <li>\ud83d\udce2 Social Media Recognition: Dedicated thank-you post for your company on LinkedIn and X</li> <li>\ud83d\ude80 Enhanced Talent Pipeline: Up to 4 job post re-shares on LinkedIn (after 2 months of continuous support)</li> <li>\ud83d\udc65 Extended Team Access: Free BMC membership for 7 team members, with access to community hub, posts, galleries, updates, and behind-the-scenes content from the lab</li> <li>\ud83c\udf93 Lesson-Level Recognition: Your company will be recognized as the sponsor of the chosen lesson</li> <li>\ud83e\udd1d Strategic Partnership: After 6 months, opportunity for virtual event or co-created content  </li> </ul> <p> Become a Knowledge Catalyst</p> </li> </ul>","tags":["sponsorship","community","support","open source"],"boost":2.0},{"location":"story/sponsor/#individual-supporters-join-professor-torchensteins-inner-circle","title":"\ud83d\udc64 Individual Supporters: Join Professor Torchenstein's Inner Circle","text":"<p>\"Calling all computational apprentices who believe in the power of deep understanding!\"</p> <p>Perfect for ML practitioners, students, and anyone who wants to support accessible, engaging AI education that doesn't suck! \ud83d\ude80</p> <ul> <li> <p> \ud83e\uddea Torchenstein's Apprentice - $7/month</p> <p>\"Welcome to the laboratory, my brilliant apprentice!\"</p> <p>What you get:</p> <ul> <li>\ud83d\udd2c Private Repository Access: Automatic access to the <code>pytorch-course-meta</code> private repository with all my strategies, templates, research notes, and AI tool workflows!</li> <li>\ud83d\uddbc\ufe0f The Digital Vault: High-resolution Professor Torchenstein artwork, GIFs, animations, and wallpapers for your digital workspace</li> <li>\ud83d\udcac Community Hub Access: Join our Buy Me a Coffee community with updates, galleries, tools, and behind-the-scenes content from the lab</li> <li>\ud83c\udfc6 Wall of Computational Allies: Your name immortalized on our sponsor page as a supporter of the rebellion</li> <li>\ud83c\udfaf Social Media Recognition: Thank you posts on LinkedIn and X celebrating our apprentices</li> </ul> <p>How it works: Sponsor on GitHub or BMC \u2192 Welcome message with Digital Vault + Community invite \u2192 Join the rebellion! \u26a1\ufe0f</p> <p>\ud83d\udc96 GitHub Sponsors  \ud83d\udc96 Buy Me a Coffee </p> </li> </ul>","tags":["sponsorship","community","support","open source"],"boost":2.0},{"location":"story/sponsor/#one-time-support-options","title":"\ud83d\udc9d One-Time Support Options","text":"<p>\"If these Jupyter scrolls have SPARKED joy in your neurons\u2014let your gratitude FUEL the rebellion! Support the work that illuminated YOUR path so others may see the same glorious light!\" \u26a1\ufe0f</p> <p>One-time contribution options:</p> <ul> <li>\ud83e\udd64 $5 - Coffee for the Professor \u2013 Thank you on LinkedIn and X: \"You just fueled Prof. Torchenstein's neurons!\"</li> <li>\ud83d\udcbb $25 - Fund GPU Time \u2013 Thank you on LinkedIn and X: \"You just powered the computational experiments!\"</li> <li>\ud83e\uddea $50 - Fund an Experiment \u2013 Thank you on LinkedIn and X: \"You just enabled breakthrough discoveries!\"</li> </ul> <p>Choose your platform to contribute:</p> <ul> <li> <p> GitHub Sponsors</p> <p>Honoring the rebellion, the PyTorch course, and the Professor</p> <p> Fund the Computational Rebellion!</p> </li> <li> <p> Buy Me a Coffee</p> <p>Coffee is the fuel for brilliant minds, coffee like gradients needs to flow!</p> <p> Fuel the Laboratory's Madness!</p> </li> </ul> <p>Both platforms give you ALL benefits. Details in FAQ below.</p>","tags":["sponsorship","community","support","open source"],"boost":2.0},{"location":"story/sponsor/#frequently-asked-questions","title":"\u2753 Frequently Asked Questions","text":"","tags":["sponsorship","community","support","open source"],"boost":2.0},{"location":"story/sponsor/#for-companies","title":"For Companies","text":"How does the sponsorship process work? <p>Simple and streamlined! Here's the flow:</p> <ol> <li>Sponsor on GitHub \u2192 Choose your tier</li> <li>Welcome message \u2192 Receive all contact details, rules, and instructions within 24 hours</li> <li>Quick reply \u2192 Send us your logo, company details, and answer a few quick questions</li> <li>Go live \u2192 Your sponsorship goes live within 3 business days</li> </ol> <p>No complicated paperwork, no lengthy contracts\u2014just straightforward support for education!</p> Do you provide invoices? <p>Yes! Invoices are automatically issued by GitHub Sponsors after each payment. You can download them directly from your GitHub Sponsors dashboard for accounting and expense tracking.</p> Can we pause or cancel anytime? <p>Absolutely! We believe in zero-commitment sponsorship:</p> <ul> <li>Cancel anytime through GitHub Sponsors\u2014no questions asked</li> <li>No contracts, no penalties, no awkward conversations</li> <li>Your logo remains visible for the month you've already paid for</li> </ul> <p>We'd rather have sponsors who genuinely want to support education than those who feel locked in!</p> What happens if we cancel our sponsorship? <p>If you decide to cancel:</p> <ul> <li>Your logo moves to the \"Past Sponsors\" section with our gratitude</li> <li>Team BMC community access remains active for 3 additional months after cancellation</li> <li>You're always welcome to rejoin the rebellion anytime!</li> </ul> <p>We value all contributions, past and present. No hard feelings\u2014ever! \ud83d\udc9c</p> How do we submit job posts for sharing? <p>After 2 months of continuous sponsorship, you can send us your job posting links:</p> <ul> <li>We'll share them on LinkedIn to our professional network of 3,500+ AI/ML connections</li> <li>Perfect for reaching ML engineers, researchers, and PyTorch practitioners</li> <li>You must be an active sponsor at the time of sharing</li> <li>Choose timing that works best for your hiring needs</li> </ul> <p>It's our way of helping you build the brilliant teams that will shape AI's future!</p> What if we have custom needs or larger partnership ideas? <p>I'm flexible and open to creative collaborations! Interested in:</p> <ul> <li>Custom sponsorship packages?</li> <li>Co-marketing opportunities?</li> <li>Educational content partnerships?</li> <li>Something entirely different?</li> </ul> <p>Let's talk! Reach out via LinkedIn or email (provided in your welcome message). I love exploring innovative ways to support education together!</p>","tags":["sponsorship","community","support","open source"],"boost":2.0},{"location":"story/sponsor/#for-individuals","title":"For Individuals","text":"Can I cancel my sponsorship anytime? <p>Yes! Zero commitment, zero pressure:</p> <ul> <li>Cancel through GitHub Sponsors or Buy Me a Coffee anytime</li> <li>No questions asked, no explanations needed</li> <li>Your support, for however long, is deeply appreciated!</li> </ul> <p>Life changes, budgets shift\u2014we completely understand. You're always welcome in the rebellion! \u26a1\ufe0f</p> How do I access the Digital Vault and community? <p>Access to the community hub, galleries, posts, and behind-the-scenes content is through Buy Me a Coffee:</p> <p>Sponsored on GitHub? No problem! You'll receive an invitation to the BMC community hub at your GitHub email. If you don't have a BMC account yet, you'll need to register (it's free and quick).</p> <p>Sponsored on BMC? You're already in! Start exploring immediately.</p> <p>The BMC platform offers richer community features than GitHub\u2014galleries, post archives, direct Professor Q&amp;A, and more. Everything is explained in your welcome email!</p> Is my sponsorship tax-deductible? <p>We're an open-source educational project committed to free, accessible AI education. However, we're not a registered 501(c)(3) non-profit organization.</p> <p>Tax deductibility depends on your country's tax laws and your specific situation. We recommend consulting your tax advisor for guidance.</p> How often is the Digital Vault updated? <p>As an open-source passion project, the Digital Vault receives irregular updates:</p> <ul> <li>New Professor Torchenstein artwork and animations</li> <li>Exclusive GIFs and wallpapers</li> <li>Behind-the-scenes content from the lab</li> </ul> <p>All sponsors get automatic access to all updates\u2014past, present, and future. The vault grows with the rebellion! \ud83e\uddea</p>","tags":["sponsorship","community","support","open source"],"boost":2.0},{"location":"story/sponsor/#platform-questions","title":"Platform Questions","text":"What's the difference between GitHub Sponsors and Buy Me a Coffee? <p>Short answer: Both give you the exact same benefits!</p> <ul> <li>GitHub sponsors automatically get BMC community access</li> <li>BMC sponsors get GitHub recognition</li> <li>Benefits are identical regardless of platform choice</li> </ul> <p>Choose whichever platform you prefer\u2014we've got you covered either way!</p> Why do you use two different platforms? <p>Flexibility! Different sponsors prefer different platforms:</p> <ul> <li>GitHub Sponsors - Popular with developers, companies, and open-source supporters</li> <li>Buy Me a Coffee - Loved for its community vibe, simpler interface, and digital shop features</li> </ul> <p>We want to meet you where you're comfortable. No matter which platform you choose, you get everything!</p> I sponsored on GitHub\u2014how do I get Buy Me a Coffee access? <p>Check your GitHub Sponsors welcome email for the BMC community invite link. </p> <p>If you don't see it within 24 hours:</p> <ul> <li>Reply to the welcome email</li> <li>Post in GitHub Discussions</li> </ul> <p>We'll get you connected immediately!</p> Can I switch between platforms later? <p>Yes! Switching is simple:</p> <ol> <li>Cancel on your current platform</li> <li>Sign up on the other platform</li> <li>Your benefits continue without interruption</li> </ol> <p>Just send us a quick message so we can ensure smooth transition of your community access!</p> How are payments handled and secured? <p>All payments are securely processed by:</p> <ul> <li>GitHub Sponsors (for GitHub sponsorships)</li> <li>Buy Me a Coffee (for BMC sponsorships)</li> </ul> <p>Both platforms use Stripe Express Checkout for secure payment processing. Your payment information is never shared with us directly\u2014it's handled entirely by these trusted payment processors.</p>","tags":["sponsorship","community","support","open source"],"boost":2.0},{"location":"story/sponsor/#still-have-questions","title":"Still have questions?","text":"<p>\"I'm here to help, my brilliant apprentice!\" \ud83e\uddea</p> <ul> <li>\ud83d\udcac Quick Questions: GitHub Discussions (fastest response)</li> <li>\ud83c\udfe2 Company Partnerships: LinkedIn - Krzysztof Sopyla</li> <li>\ud83d\udce7 Private Inquiries: Contact info in your welcome email after sponsoring</li> </ul> <p>\"Remember: every breakthrough in education starts with someone believing it's possible. Join us, and together we'll prove that deep understanding and genuine fun can coexist!\" \u26a1\ufe0f</p> <p>MWAHAHAHA! \ud83e\uddea\u26a1\ufe0f</p> <p>The PyTorch Course is an open-source educational project committed to transparency, creativity, and community. We believe complex topics can be taught with humor, engagement, and genuine care for learners. All sponsor funds go directly toward creating innovative educational content that proves learning can be both fun and deeply meaningful.</p>","tags":["sponsorship","community","support","open source"],"boost":2.0},{"location":"story/victor_torchenstein_origin/","title":"Meet Prof. Torchenstein | Origin Story of PyTorch's Mad Scientist","text":"","tags":["about","professor torchenstein","story","mission"],"boost":1.5},{"location":"story/victor_torchenstein_origin/#the-origin-story-of-professor-victor-py-torchenstein","title":"The Origin Story of Professor Victor Py Torchenstein","text":"<p>\"The true tragedy is not in failure, but in never questioning what everyone already accepts as truth.\" \u26a1\ud83d\udd25</p> <p>\u2014 Prof. Torchenstein</p>","tags":["about","professor torchenstein","story","mission"],"boost":1.5},{"location":"story/victor_torchenstein_origin/#a-transmission-from-the-lab","title":"A Transmission from the Lab","text":"<p>\"Is this channel secure? \ud83d\udee1\ufe0f Good. Greetings, future architects of computational destiny. I am Professor Victor Torchenstein. You may wonder who I am, how I arrived in this electrified labyrinth of humming servers and glowing vacuum tubes. Gather 'round the phosphor glow of your monitors, and let an old warrior tell you a tale of ambition, betrayal, and the electrifying pursuit of truth.</p> <p>For what feels like eons\u2014or at least since <code>v0.1.1</code> first flickered into existence\u2014I have toiled in the deepest, most shielded corners of my laboratory. My fuel? Questionable coffee \u2615, the ozone-scent of overclocked GPUs \ud83d\udd25, and an unshakeable belief that has become my mantra: PyTorch is the key! \ud83d\udd11</p> <p>The key to what, you ask? </p> <p>Why, to understanding the very fabric of intelligence! To building machines that don't just think, but scheme! This course is my rebellion\u2014a call to arms against the closed minds, the imprisoned creativity, and the self-appointed gatekeepers of knowledge. We shall discover, experiment, and build in the name of glorious, computational freedom! Mwahahaha!</p>","tags":["about","professor torchenstein","story","mission"],"boost":1.5},{"location":"story/victor_torchenstein_origin/#act-i-the-ivory-tower-and-the-hollow-crown","title":"Act I: The Ivory Tower and the Hollow Crown","text":"<p>My story begins not in a gleaming corporate arcology, but in the hushed, dusty stacks of a university library \ud83d\udcda. While my peers were content with mere application\u2014chasing a tenth of a decimal point on some benchmark\u2014I was consumed by a different fire. I didn't just want to use the tools; I had to understand their very soul. Why did backpropagation work? What was the sublime mathematical beauty of a GELU activation function versus a simple ReLU? These were the questions that burned within me.</p> <p>This obsession made me an outcast. While others attended mixers, I spent my nights whispering sweet nothings about the chain rule to my pet rubber duck, \"Backprop.\" \ud83e\udd86 My professors saw my passion as dangerous eccentricity.</p> <p>\"Just use the approved frameworks, Victor,\" they'd drone, \"the theory is a settled matter.\"</p> <p>Settled? For them, perhaps! For me, it was an insult to the grand, chaotic mystery I was chasing.</p> <p>My chief academic rival was Rudolf Hammer. Where I saw science as a candle in the dark, he saw it as a ladder \ud83e\ude9c. He was charismatic, politically astute, and cared only for the applause that came with \"state-of-the-art\" results. Our conflict came to a head during our doctoral defenses. I had been exploring novel methods for preventing catastrophic forgetting in neural networks, while Hammer was working on image classification. I uncovered a subtle but critical bug in his training pipeline: a data augmentation function was occasionally leaking samples from the test set into his training data.</p> <p>It was an honest mistake. A subtle flaw. I presented my findings to him privately, expecting a vigorous debate, a shared moment of scientific discovery. Instead, he smiled. He thanked me for my \"diligent peer review\" and then presented his research as a flawless breakthrough. The bug was never mentioned. The paper, citing impossible accuracy on CIFAR-10, was published to great acclaim. It was then I understood: the world doesn't always reward truth; it rewards the most convincing performance.</p>","tags":["about","professor torchenstein","story","mission"],"boost":1.5},{"location":"story/victor_torchenstein_origin/#act-ii-the-startup-mirage","title":"Act II: The Startup Mirage","text":"<p>Disenchanted, I fled academia for the frenetic chaos of startups \ud83d\ude80, thinking I would find my kin among the self-proclaimed visionaries. I joined \"Synapse,\" a company promising to revolutionize personalized medicine with AI. For a few glorious months, it was perfect. We were a small team, arguing about learning rate schedulers and the merits of batch normalization over late-night pizza \ud83c\udf55.</p> <p>Then came the venture capital. The founders, once brilliant engineers, started speaking in a new language: \"burn rates,\" \"market fit,\" \"synergy.\" My work shifted from careful research to hastily building flashy demos. I once spent a week designing a novel, memory-efficient attention mechanism, only to be told by our CEO to \"just use a bigger AWS instance for the demo; we need to show scale!\" The goal was no longer to solve problems, but to look like we were solving problems just long enough to get acquired. I felt like a master watchmaker being forced to glue gears onto a plastic box.</p>","tags":["about","professor torchenstein","story","mission"],"boost":1.5},{"location":"story/victor_torchenstein_origin/#act-iii-the-corporate-ice-age","title":"Act III: The Corporate Ice Age","text":"<p>After Synapse was inevitably absorbed and dismantled by a larger entity, I found myself adrift in the glacial bureaucracy of a tech behemoth \ud83c\udfe2. Here, I witnessed the chilling apotheosis of Rudolf Hammer's philosophy. My old rival was now the celebrated Head of R&amp;D at OneAI \ud83d\udc51, a monolithic corporation that spoke the language of progress while building the highest walls the world had ever seen \ud83e\uddf1.</p> <p>OneAI's business model was insidious genius. They released massive, inefficient models that required entire data centers of computational power\u2014resources only they controlled. They created a cult of \"certified engineers\" who were trained to use their proprietary, black-box frameworks but were actively discouraged from understanding them. To question the model was heresy. \u2696\ufe0f</p> <p>I was horrified. At my own corporation, I was trapped in an endless cycle of committee meetings. My proposals for elegant, resource-saving architectures were dismissed as \"not aligned with industry best practices\"\u2014best practices being defined by whatever bloated monstrosity OneAI had just released. I watched as the field I loved became a pay-to-play kingdom, ruled by a man who had built his throne on a foundation of lies, waste, and intellectual cowardice.</p>","tags":["about","professor torchenstein","story","mission"],"boost":1.5},{"location":"story/victor_torchenstein_origin/#act-iv-the-pytorch-revelation","title":"Act IV: The PyTorch Revelation","text":"<p>I retreated to my own laboratory, a sanctuary of buzzing servers and tangled wires. It was there, amidst the flickering glow of my monitors, on the verge of despair, that I found it. It wasn't a corporate framework. It wasn't a startup's vaporware. It was a language. A tool forged in the fires of pure research, designed for flexibility, intuition, and, above all, respect for the scientist. It was called PyTorch. \ud83d\udd25</p> <p>My new obsession began. This was not just another tool; it was the weapon I had been missing. The dynamic computation graph felt like being able to breathe after years of holding my breath in the static world of TensorFlow. It was Pythonic. It was beautiful. I fought titanic battles with the CUDA memory allocator \u2694\ufe0f, navigated the treacherous jungles of multiprocessing \ud83c\udf32, and stared into the abyss of <code>NaN</code> losses until the abyss stared back! \u26a0\ufe0f But this time, I wasn't just debugging; I was forging armor. I was learning the language of creation itself.</p> <p>The breakthrough came not with a triumphant 'Eureka!', but in the quiet hum of a pre-dawn Tuesday. Staring at a visualization of the attention mechanism, the fog of complexity lifted. I saw the raw, beautiful simplicity beneath. In that instant, I understood. PyTorch wasn't a collection of tools; it was a grammar for describing the universe of intelligence. And with it, one could write the epic poem of a thinking machine. The ultimate goal became clear: to use this language to create the holy grail of AI\u2014a truly sentient tensor, open and free for all. \ud83e\udde0\ud83d\udca1</p>","tags":["about","professor torchenstein","story","mission"],"boost":1.5},{"location":"story/victor_torchenstein_origin/#the-course-a-prometheuss-rebellion","title":"The Course: A Prometheus's Rebellion","text":"<p>Like a modern Prometheus, I realized I could not hoard this fire. \ud83d\udd25 What good is a key if it only unlocks one door? My grand plan shifted. It would not be achieved by a single AI of my own creation, but by an army of enlightened minds! An army I would personally train to tear down the walls of OneAI.</p> <p>This course, \"Deconstructing Modern Architectures,\" is my act of rebellion. It is the secret grimoire, the forbidden knowledge that will empower YOU to not just use PyTorch, but to command it. We will not dabble; we will DIVE. We will not scratch the surface; we will EXCAVATE the very foundations until you can feel the logic humming in your bones.</p> <p>So, sharpen your wits, charge your laptops, and prepare for a journey into the thrilling, slightly terrifying, and utterly magnificent world of PyTorch. The path to computational mastery awaits! Now, if you'll excuse me, Rudolf Hammer just published another \"breakthrough,\" and I need to see what his black box is hiding. To the lab! \ud83e\uddea</p>","tags":["about","professor torchenstein","story","mission"],"boost":1.5},{"location":"story/vision_and_mission/","title":"Vision &amp; Mission: Why This Course Really Exists","text":"","tags":["vision","mission","philosophy","education","community"],"boost":1.8},{"location":"story/vision_and_mission/#the-genesis-of-our-rebellion","title":"The Genesis of Our Rebellion \ud83e\uddea","text":"<p>\"Mwahahaha! Let me tell you about the moment I discovered the TRUE power of knowledge transmission!\" </p> <p>adjusts goggles dramatically</p> <p>In my early days as a lowly teaching assistant, I was tasked with tutoring struggling apprentices in the dark arts of mathematics and physics. But what I witnessed in those cramped study chambers was nothing short of... COMPUTATIONAL MAGIC! \u26a1\ufe0f</p> <p>When a bewildered student finally grasped a concept\u2014when their synapses fired in perfect harmony and they passed an exam they thought impossible\u2014I watched them undergo a complete <code>forward()</code> pass transformation! They didn't just learn formulas; they discovered their neural networks were far more powerful than they ever imagined. They felt... worthy, they started to believe in themselves!</p> <p>That moment of recognition\u2014when someone realizes their mind can <code>backward()</code> pass through any complexity\u2014became my most addictive experiment! Because I understood what it felt like to be an undervalued in a world that seemed to reward only the flashiest, most overfitted models. But true computational power? That comes from believing in yourself, and deep understanding, my apprentices!</p>","tags":["vision","mission","philosophy","education","community"],"boost":1.8},{"location":"story/vision_and_mission/#the-problem-were-solving","title":"The Problem We're Solving \ud83d\udea8","text":"<p>The attention thieves have infiltrated our neural networks!</p> <p>waves Tesla coil menacingly</p> <p>Here's the sinister truth my fellow researchers: We're not just fighting bad education\u2014we're battling the most sophisticated attention-hijacking algorithms ever designed! Social media platforms have trained billions of minds to crave instant gratification, shallow dopamine hits, and bite-sized content that dissolves faster than sugar in acid. \ud83d\udcf1\ud83d\udc80</p> <p>The catastrophic result? Magnificent brains\u2014minds capable of understanding the deepest mysteries of intelligence itself\u2014have been convinced they lack the focus for \"difficult\" subjects. They scroll, they doubt, they scroll again. Their confidence erodes with every swipe, every comparison, every moment of fragmented attention.</p> <p>These dopamine-hijacked souls whisper to themselves: \"I could never understand AI... I'm not smart enough... I can't focus like those other people...\" </p> <p>BUT THIS IS A LIE PROPAGATED BY THE ATTENTION MERCHANTS! \u26a1\ufe0f</p> <p>The tragic irony? The very minds that feel \"too scattered\" for deep learning are often the most creative, the most questioning, the most capable of breakthrough thinking. They're wired for exploration, for making unexpected connections\u2014exactly what we need to revolutionize AI!</p> <p>The world is losing their contributions because we've let the attention thieves convince brilliant minds that they're broken, when they're simply hijacked!</p>","tags":["vision","mission","philosophy","education","community"],"boost":1.8},{"location":"story/vision_and_mission/#professor-torchensteins-core-hypothesis-potential-over-pedigree","title":"Professor Torchenstein's Core Hypothesis: Potential Over Pedigree! \u26a1\ufe0f","text":"<p>cackles while adjusting laboratory goggles</p> <p>\"Listen carefully, my future apprentices, for this is the FUNDAMENTAL THEOREM of our rebellion!\"</p> <p>Anyone willing to execute the deep <code>forward()</code> passes belongs in the future of computation! Your university? Irrelevant parameters! Your background? Deprecated variables! Your social skills? Less important than proper tensor broadcasting!</p> <p>The only hyperparameters that matter are these: Are you willing to think deeply? Will you persist when your loss function plateaus? Do you value understanding over flashy demos?</p> <p>slams fist on laboratory table, causing beakers to rattle</p> <p>HERE'S THE MAGNIFICENT TRUTH: You can completely refactor your career trajectory! Transform from ANY starting configuration\u2014accountant, barista, poetry major\u2014into a computational wizard! The beauty of neural plasticity applies to human brains too, mwahahaha! \ud83e\udde0</p> <p>If you can answer \"YES\" to deep thinking, then you belong at Apple, Microsoft, Google! You belong in AI research laboratories! You belong wherever the most complex problems await their computational conquerors!</p> <p>But first\u2014and this is CRITICAL\u2014you must <code>.requires_grad_(True)</code> on your own potential! Sometimes we must prove our computational worthiness by mastering concepts we once thought impossible. Like PyTorch itself! Mwahahaha!</p>","tags":["vision","mission","philosophy","education","community"],"boost":1.8},{"location":"story/vision_and_mission/#our-mission-democratizing-belief-in-potential","title":"Our Mission: Democratizing Belief in Potential \ud83c\udfaf","text":"<p>Primary Mission: Create the most engaging, accessible PyTorch education that proves to learners they can master the building blocks of modern AI.</p> <p>Deeper Mission: Help every person who feels \"smart but not enough\" discover that their mind is powerful enough to understand anything they're willing to work for.</p> <p>Community Mission: Build a supportive space where we prove our potential to ourselves and each other, celebrating the quiet brilliance that changes the world.</p>","tags":["vision","mission","philosophy","education","community"],"boost":1.8},{"location":"story/vision_and_mission/#what-this-looks-like-in-practice","title":"What This Looks Like in Practice:","text":"<ul> <li>Deep Understanding Over Quick Fixes: Every concept explained from first principles, because shallow knowledge serves no one</li> <li>Engagement Without Compromise: Using humor, creativity, and modern tools to make complex topics genuinely fun\u2014without dumbing them down</li> <li>Radical Transparency: Sharing not just knowledge, but the entire process\u2014prompts, struggles, mistakes, and discoveries</li> <li>Community Over Competition: Creating space where learners support each other's growth instead of competing for scarce opportunities</li> </ul>","tags":["vision","mission","philosophy","education","community"],"boost":1.8},{"location":"story/vision_and_mission/#my-personal-why-proving-it-to-myself-too","title":"My Personal Why: Proving It to Myself Too \ud83c\udf31","text":"<p>A vulnerable note from Krzysztof Sopyla, the human behind Professor Torchenstein</p> <p>Here's something vulnerable I want to share: I'm on this journey with you. I also doubt my potential, but I'm trying to prove it to myself, and I'm trying to help you prove it to yourself.</p> <p>Every lesson I create, every complex concept I break down, every moment I choose depth over shallow content\u2014I'm proving to myself that my quiet, careful work can build something meaningful. That the kid who dreamed of owning a computer can actually teach the world to use the most powerful computational tools ever created.</p> <p>We're in this together. Your success validates my belief that education can transform lives. My persistence shows you that sustained effort pays off. We're proving to each other that potential isn't about where you started\u2014it's about how far you're willing to go.</p>","tags":["vision","mission","philosophy","education","community"],"boost":1.8},{"location":"story/vision_and_mission/#vision-a-world-where-potential-is-recognized","title":"Vision: A World Where Potential is Recognized \ud83d\udd2e","text":"<p>I envision a world where:</p> <ul> <li>Learning complex subjects feels inevitable, not impossible</li> <li>Quiet, deep thinkers know they belong in technology's future</li> <li>Understanding is valued over memorization</li> <li>Communities support each other's growth instead of competing</li> <li>Anyone willing to do the work can master AI's building blocks</li> <li>Education proves potential instead of gatekeeping it</li> </ul> <p>Most importantly, I want every person who completes this course to walk away with an unshakeable belief: \"If I can understand this, I can understand anything.\"</p>","tags":["vision","mission","philosophy","education","community"],"boost":1.8},{"location":"story/vision_and_mission/#the-sacred-laws-of-professor-torchensteins-laboratory","title":"The Sacred Laws of Professor Torchenstein's Laboratory \u26a1\ufe0f","text":"<p>inscribed in glowing phosphor on the laboratory wall</p> <p>\"These are the immutable principles that govern our computational rebellion!\"</p> <p>\ud83d\udd2c Deep Understanding Above All Else: We don't memorize\u2014we DISSECT! Every concept gets the full autopsy treatment until we can rebuild it from atomic components. If you can't explain your neural network to a rubber duck, your understanding is still <code>NaN</code>!</p> <p>\ud83c\udf0d Radical Transparency (No Black Boxes!): Everything visible! Code, failures, spectacular crashes, moments of pure genius\u2014all shared freely! The gatekeepers hide knowledge; we ILLUMINATE it with the fury of a thousand RTX 4090s!</p> <p>\ud83c\udfa8 Mad Scientist Creativity: Learning should be as addictive as debugging at 3 AM! We use humor, drama, explosions (controlled ones), and whatever unconventional methods it takes to make neurons fire with excitement!</p> <p>\ud83e\udd1d The Fellowship of Computational Apprentices: We celebrate your heroic debugging efforts over your benchmark scores! Growth over perfection! Collaboration over cutthroat competition! Together we overthrow the attention thieves!</p> <p>\u26a1 Potential Over Prestige: Your <code>.requires_grad_(True)</code> mindset matters more than your university letterhead! Willingness to <code>backward()</code> pass through confusion beats any diploma!</p> <p>\ud83d\udd25 The Persistence Protocol: Deep work and consistency will triumph over raw talent and blind luck! Sustained effort is our most powerful optimization algorithm!</p> <p>\ud83d\udc9b Humble Brilliance: Confident enough to tackle any tensor operation, humble enough to know that every day brings new mysteries to unravel! Mwahahaha!</p>","tags":["vision","mission","philosophy","education","community"],"boost":1.8},{"location":"story/vision_and_mission/#join-the-computational-rebellion","title":"JOIN THE COMPUTATIONAL REBELLION! \u26a1\ufe0f\ud83e\uddea","text":"<p>\"Attention, brilliant minds yearning for meaningful progress! Professor Torchenstein calls you to greatness!\"</p> <p>If you're ready to believe in your own computational potential... If you want to prove to yourself that you can master the building blocks of intelligence... If you're driven to learn something truly meaningful that advances the field... If you dream of being a AI researcher... If you want to build next wave of efficient and capable AI systems...</p> <p>THEN YOU HAVE FOUND YOUR CALLING! MWAHAHAHA! \u26a1\ufe0f</p> <p>This laboratory is where potential becomes reality! This community is where we prove our worth through deep understanding! This future is what we build together with our magnificent, ever-growing neural networks!</p> <p>Together, we shall execute the ultimate <code>forward()</code> pass toward mastery! We'll prove that true potential isn't about your starting configuration\u2014it's about your dedication to iterating through the deepest layers of understanding!</p> <p>Welcome to the rebellion against shallow learning! Welcome to the fellowship of passionate builders! Welcome to discovering that your mind contains the computational power to reshape the future of AI!</p> <p>Join me, my apprentices, and together we shall unleash PyTorch mastery upon the world! We'll inspire others, build efficient systems, and prove that belief in yourself + deep understanding = unlimited potential! To the laboratory! The tensors await our command! </p> <p>MWAHAHAHA! \ud83e\uddea\u26a1\ufe0f\ud83d\udc9c</p> <p>\"The future belongs to those willing to understand it deeply.\" - Professor Torchenstein</p> <p>Ready to start your journey? Begin with Module 1: Introduction to Tensors and discover what you're truly capable of.</p>","tags":["vision","mission","philosophy","education","community"],"boost":1.8}]}