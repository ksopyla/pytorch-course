{"config":{"lang":["en"],"separator":"[\\s\\-,:!=\\[\\]()\"/]+|(?!\\b)(?=[A-Z][a-z])|\\.(?!\\d)|&[lg]t;","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Greetings, Future Architect of Computational Destiny!","text":""},{"location":"#what-madness-awaits-you","title":"What Madness Awaits You?","text":"<p>My life's work\u2014my magnum opus\u2014is to demystify the arcane arts of deep learning. They called me mad! And they were right! Madly efficient at PyTorch! Forget dry, boring lectures. Prepare for electrifying demonstrations, code that crackles with potential, and insights so profound they might just rearrange your synapses!</p> <p>Course Goal: To imbue you\u2014my fearless apprentices\u2014with the eldritch secrets of PyTorch building blocks, enabling you to conjure, dissect, and ultimately command modern neural network architectures like Transformers and Diffusion models.</p> <ul> <li> <p> Meet Your Mentor</p> <p>Who is the maniacal genius leading this quest? Learn about my sordid past, my questionable methods, and my grand plan for computational supremacy.</p> <p> Uncover My Origin Story</p> </li> <li> <p> Master the Fundamentals</p> <p>From the humble tensor to the dark arts of <code>einsum</code> and <code>autograd</code>, you will forge a deep, intuitive understanding of PyTorch's core components.</p> <p> Forge Your First Tensor</p> </li> <li> <p> Deconstruct the Titans</p> <p>Go beyond the surface. We will dissect modern architectures like Transformers, piece by piece, until you command their inner workings from first principles.</p> <p> Start the Deconstruction</p> </li> <li> <p> For Acolytes of All Levels</p> <p>Whether you're a fresh-faced initiate or a seasoned GPU warlock, my laboratory has a place for you. All that's required is a thirst for knowledge!</p> <p> View the Full Syllabus</p> </li> </ul>"},{"location":"#are-you-ready-to-begin","title":"Are You Ready to Begin?","text":"<p>The path to computational godhood awaits! Sharpen your wits, charge your laptops, and join me. Together, we shall backward() pass our way to glory!</p> <p>Now, if you'll excuse me, I believe my latest creation is about to achieve sentience... or possibly just needs a reboot. To the lab!</p> <p>MWAHAHAHA!</p>"},{"location":"pytorch-course-structure/","title":"PyTorch Course: Deconstructing Modern Architectures","text":"<p>Welcome, my aspiring acolytes, to the crucible of creation! You stand at the precipice of a great awakening. Within these hallowed digital halls, we shall not merely learn PyTorch; we shall wrestle it into submission, bending its very tensors to our will. This is not a course; it is a summons. A call to arms for all who dare to dream in tensors and architect the future! Prepare yourselves, for the path ahead is fraught with peril, caffeine, and the incandescent glow of computational glory! Mwahahaha!</p> <p></p> <p>Do you want to hear the origin story of this course? Click here</p> <p>Course Goal: To imbue you\u2014my fearless apprentices\u2014with the eldritch secrets of PyTorch building blocks, enabling you to conjure, dissect, and ultimately command modern neural network architectures like Transformers and Diffusion models.</p> <p>Learner level: Beginner - Advanced Prerequisite Madness: None! Whether you are a fresh-faced initiate or a seasoned GPU warlock, the lab doors stand open. \u26a1\ufe0f\ud83e\uddea</p>"},{"location":"pytorch-course-structure/#module-0-getting-started-with-pytorch","title":"Module 0: Getting Started with PyTorch","text":"<p>Before we unleash neural monstrosities upon the world, we must ignite your development lair. This module guides you through preparing PyTorch on any operating system\u2014so your GPUs purr at your command.</p> <p>What you will learn: 1. How to setup a PyTorch environment for different operating systems and test it.</p>"},{"location":"pytorch-course-structure/#lessons","title":"Lessons:","text":"<ol> <li>Course Schematic for World Domination - The master blueprint of our curriculum\u2014study it well, minion!</li> <li>Setting Up Your PyTorch Environments:<ol> <li>Windows Lair Construction - Forge a Windows stronghold with <code>pyenv</code> + <code>poetry</code>; make GPUs bend the knee, driver or no.</li> <li>Linux Lair Construction - Harness the raw might of Ubuntu for unfettered tensor experimentation.</li> <li>macOS Lair Construction - Coax Apple silicon beasts into righteous PyTorch servitude.</li> <li>Colab Cloud Lair - Seize Google's sky-high GPUs for zero coin\u2014mwahaha!</li> </ol> </li> </ol>"},{"location":"pytorch-course-structure/#module-1-pytorch-core-i-see-tensors-everywhere","title":"Module 1: PyTorch Core - I see tensors everywhere","text":"<p>Here we unveil the truth: the cosmos is a writhing mass of tensors awaiting our manipulation. Grasp them well\u2014for they are the bedrock of every grand scheme to come!</p> <p>This module dives into the fundamental components of PyTorch, essential for any deep learning task.</p>"},{"location":"pytorch-course-structure/#11-tensors-the-building-blocks","title":"1.1 Tensors: The Building Blocks","text":"<p>What you will learn: </p> <ol> <li>Tensor Concept. What is a tensor? Tensor vs. Matrix. Mathematical vs. PyTorch interpretation. Why tensors are crucial for ML</li> <li>PyTorch Basics: Tensor creation and their attributes (dtype, shape, device).</li> <li>Tensor manipulation: Indexing, Slicing, Joining (torch.cat, torch.stack), Splitting. Manipulating tensor shapes (reshape, view, squeeze, unsqueeze, permute, transpose).</li> </ol>"},{"location":"pytorch-course-structure/#11-lessons","title":"1.1 Lessons:","text":"<ol> <li>Summoning Your First Tensors - Conjure tensors from void, inspect their properties, revel in their latent might (with a bit of help from <code>torch.randn</code>, <code>torch.zeros</code>, <code>torch.ones</code>, <code>torch.arange</code>, <code>torch.linspace</code> etc).</li> <li>Tensor Shape-Shifting &amp; Sorcery - Slice, squeeze, and permute dimensions until reality warps to your whims (with a bit of help from <code>torch.cat</code>, <code>torch.stack</code>, <code>torch.split</code>, <code>torch.reshape</code>, <code>torch.view</code>, <code>torch.squeeze</code>, <code>torch.unsqueeze</code>, <code>torch.permute</code>, <code>torch.transpose</code> etc).</li> <li>DTypes &amp; Devices: Choose Your Weapons - Select precision and hardware like a seasoned archmage choosing spell components, under the hood of <code>torch.float</code>, <code>torch.float16</code>, etc.</li> </ol>"},{"location":"pytorch-course-structure/#12-tensor-operations-computation-at-scale","title":"1.2 Tensor Operations: Computation at Scale","text":"<p>What you will learn:</p> <ol> <li>Overview of tensor math. Element-wise operations. Reduction operations (sum, mean, max, min, std). Basic matrix multiplication (torch.mm, torch.matmul, @ operator). Broadcasting: rules and practical examples with verifiable tiny data. In-place operations.</li> </ol>"},{"location":"pytorch-course-structure/#12-lessons","title":"1.2 Lessons:","text":"<ol> <li>Elemental Tensor Alchemy - Brew element-wise, reduction, and other operations into potent mathematical elixirs.</li> <li>Matrix Mayhem: Multiply or Perish - Orchestrate 2-D, batched, and high-dimensional multiplications with lethal elegance.</li> <li>Broadcasting: When Dimensions Bow to You - Command mismatched shapes to cooperate through the dark art of implicit expansion.</li> </ol>"},{"location":"pytorch-course-structure/#13-einstein-summation-the-power-of-einsum","title":"1.3 Einstein Summation: The Power of einsum","text":"<p>What you will learn:</p> <ol> <li>Understanding Einstein notation. Why it's powerful for complex operations (e.g., attention).</li> </ol>"},{"location":"pytorch-course-structure/#13-lessons","title":"1.3 Lessons:","text":"<ol> <li>Einstein Summation: Harness the \u039b-Power - Invoke <code>einsum</code> to express complex ops with maddening brevity.</li> <li>Advanced Einsum Incantations - Wield multi-tensor contractions that underpin attention itself.</li> </ol>"},{"location":"pytorch-course-structure/#14-autograd-automatic-differentiation","title":"1.4 Autograd: Automatic Differentiation","text":"<p>What you will learn:</p> <ol> <li>What are gradients? The computational graph. How PyTorch tracks operations.</li> <li>requires_grad attribute. Performing backward pass with .backward(). Accessing gradients with .grad. torch.no_grad() and tensor.detach().</li> <li>Gradient accumulation. Potential pitfalls. Visualizing computational graphs (conceptually).</li> </ol>"},{"location":"pytorch-course-structure/#14-lessons","title":"1.4 Lessons:","text":"<ol> <li>Autograd: Ghosts in the Machine (Learning) - Meet the spectral gradient trackers haunting every tensor operation.</li> <li>Gradient Hoarding for Grand Spells - Accumulate gradients like arcane energy before unleashing colossal updates.</li> </ol>"},{"location":"pytorch-course-structure/#module-2-torchnn-building-neural-networks","title":"Module 2: torch.nn \u2014 Building Neural Networks","text":"<p>Witness code coalescing into living, breathing neural contraptions! In this module we bend <code>torch.nn</code> to our will, assembling layers and models worthy of legend.</p>"},{"location":"pytorch-course-structure/#21-the-nnmodule-blueprint","title":"2.1 The <code>nn.Module</code> Blueprint","text":"<p>What you will learn: - The role of <code>nn.Module</code> as the base class for layers and models. - Implementing <code>__init__</code> and <code>forward</code>. - Registering parameters and buffers. - Composing modules with <code>nn.Sequential</code>, <code>nn.ModuleList</code>, and <code>nn.ModuleDict</code>. - Saving and restoring weights with <code>state_dict</code>.</p>"},{"location":"pytorch-course-structure/#21-lessons","title":"2.1 Lessons:","text":"<ol> <li>Building Brains with <code>nn.Module</code> - Craft custom neural matter by overriding <code>__init__</code> &amp; <code>forward</code>.</li> <li>Franken-Stacking Layers - Bolt modules together with <code>Sequential</code>, <code>ModuleList</code>, and <code>ModuleDict</code>.</li> <li>Preserving Your Monster's Memories - Save and resurrect model weights with <code>state_dict</code> necromancy.</li> </ol>"},{"location":"pytorch-course-structure/#22-linear-layer-and-activations","title":"2.2 Linear Layer and Activations","text":"<p>What you will learn: - Linear layers and high-dimensional matrix multiplication.  - What is the role of linear layers in attention mechanisms (query, key, value)? - Activation functions (ReLU, GELU, SiLU, Tanh, Softmax, etc.). - Dropout for regularisation.  </p>"},{"location":"pytorch-course-structure/#22-lessons","title":"2.2 Lessons:","text":"<ol> <li>Linear Layers: The Vector Guillotine - Slice through dimensions turning inputs into finely-chopped activations.</li> <li>Activation Elixirs - Re-animate neurons with ReLU, GELU, SiLU, and other zesty potions.</li> <li>Dropout: Network lobotomy - Make neurons forget just enough to generalise\u2014no lobotomy required.</li> </ol>"},{"location":"pytorch-course-structure/#23-embedding-layers","title":"2.3 Embedding Layers","text":"<p>What you will learn: - Embedding layers and their purpose in neural networks. - Embedding layer implementation from scratch, initialisation, and usage. - Positional encoding and how it is used to inject order into the model.</p>"},{"location":"pytorch-course-structure/#23-lessons","title":"2.3 Lessons:","text":"<ol> <li>Embedding Layers: Secret Identity Chips - Embed discreet meanings within high-dimensional space.</li> <li>Positional Encoding: Injecting Order into Chaos - Imbue sequences with a sense of place so attention never loses its bearings.</li> </ol>"},{"location":"pytorch-course-structure/#24-normalisation-layers","title":"2.4 Normalisation Layers","text":"<p>What you will learn: - BatchNorm vs. LayerNorm and when to use each. - RMSNorm and other modern alternatives. - Training vs. evaluation mode caveats.</p>"},{"location":"pytorch-course-structure/#24-lessons","title":"2.4 Lessons:","text":"<ol> <li>Normalization: Calming the Beast - Tame activations with BatchNorm and LayerNorm before they explode.</li> <li>RMSNorm &amp; Other Exotic Tonics - Sample contemporary concoctions for stable training.</li> <li>Train vs. Eval: Split Personality Disorders - Toggle modes and avoid awkward identity crises.</li> </ol>"},{"location":"pytorch-course-structure/#25-loss-functions-guiding-optimisation","title":"2.5 Loss Functions \u2014 Guiding Optimisation","text":"<p>What you will learn: - Loss functions recap, the main types of loss functions and when to use each.  - Prepare inputs and targets for loss functions and outputs interpretation (logits vs. probabilities). - Interpreting reduction modes and ignore indices.</p>"},{"location":"pytorch-course-structure/#25-lessons","title":"2.5 Lessons:","text":"<ol> <li>Loss Potions: Guiding Pain into Progress - Channel model errors into gradients that sharpen intelligence.</li> <li>Preparing Sacrificial Inputs &amp; Targets - Align logits and labels for maximum learning agony.</li> <li>Reduction Rituals &amp; Ignore Indices - Decipher reduction modes and skip unworthy samples without remorse.</li> </ol>"},{"location":"00-getting-started/01_hello_pytorch/","title":"01: Test your PyTorch setup","text":"In\u00a0[1]: Copied! <pre>print(\"Hello world\")\n</pre> print(\"Hello world\") <pre>Hello world\n</pre> In\u00a0[2]: Copied! <pre>import torch\nprint(f\"PyTorch version: {torch.__version__}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\n</pre> import torch print(f\"PyTorch version: {torch.__version__}\") print(f\"CUDA available: {torch.cuda.is_available()}\") <pre>PyTorch version: 2.7.0+cu126\nCUDA available: True\n</pre> In\u00a0[3]: Copied! <pre>#Display a PyTorch version and GPU availability\nif torch.cuda.is_available():\n    device = torch.device(\"cuda\")          # a CUDA device object\n    print(f\"Using device: {device}\")\n    \n    # Print CUDA device properties\n    print(\"\\nCUDA Device Properties:\")\n    print(f\"Device name: {torch.cuda.get_device_name(0)}\")\n    print(f\"Device properties: {torch.cuda.get_device_properties(0)}\")\n    print(f\"Current device index: {torch.cuda.current_device()}\")\n    print(f\"Device count: {torch.cuda.device_count()}\")\n    \n    # Print CUDA version and capabilities\n    print(\"\\nCUDA Information:\")\n    print(f\"CUDA version: {torch.version.cuda}\")\n    print(f\"cuDNN version: {torch.backends.cudnn.version()}\")\n    print(f\"cuDNN enabled: {torch.backends.cudnn.enabled}\")\n    \n    # Print PyTorch memory info\n    print(\"\\nPyTorch Memory Information:\")\n    print(f\"Allocated memory: {torch.cuda.memory_allocated(0) / 1024**2:.2f} MB\")\n    print(f\"Cached memory: {torch.cuda.memory_reserved(0) / 1024**2:.2f} MB\")\nelse:\n    print('GPU not enabled')\n    print(\"\\nPyTorch Information:\")\n    print(f\"PyTorch version: {torch.__version__}\")\n    print(f\"Backend: {torch.get_default_dtype()}\")\n    \n</pre>   #Display a PyTorch version and GPU availability if torch.cuda.is_available():     device = torch.device(\"cuda\")          # a CUDA device object     print(f\"Using device: {device}\")          # Print CUDA device properties     print(\"\\nCUDA Device Properties:\")     print(f\"Device name: {torch.cuda.get_device_name(0)}\")     print(f\"Device properties: {torch.cuda.get_device_properties(0)}\")     print(f\"Current device index: {torch.cuda.current_device()}\")     print(f\"Device count: {torch.cuda.device_count()}\")          # Print CUDA version and capabilities     print(\"\\nCUDA Information:\")     print(f\"CUDA version: {torch.version.cuda}\")     print(f\"cuDNN version: {torch.backends.cudnn.version()}\")     print(f\"cuDNN enabled: {torch.backends.cudnn.enabled}\")          # Print PyTorch memory info     print(\"\\nPyTorch Memory Information:\")     print(f\"Allocated memory: {torch.cuda.memory_allocated(0) / 1024**2:.2f} MB\")     print(f\"Cached memory: {torch.cuda.memory_reserved(0) / 1024**2:.2f} MB\") else:     print('GPU not enabled')     print(\"\\nPyTorch Information:\")     print(f\"PyTorch version: {torch.__version__}\")     print(f\"Backend: {torch.get_default_dtype()}\")      <pre>Using device: cuda\n\nCUDA Device Properties:\nDevice name: NVIDIA GeForce RTX 3080 Laptop GPU\nDevice properties: _CudaDeviceProperties(name='NVIDIA GeForce RTX 3080 Laptop GPU', major=8, minor=6, total_memory=16383MB, multi_processor_count=48, uuid=4a2f15bc-7268-fcb8-f7b3-9f60002afe35, L2_cache_size=4MB)\nCurrent device index: 0\nDevice count: 1\n\nCUDA Information:\nCUDA version: 12.6\ncuDNN version: 90701\ncuDNN enabled: True\n\nPyTorch Memory Information:\nAllocated memory: 0.00 MB\nCached memory: 0.00 MB\n</pre>"},{"location":"00-getting-started/01_hello_pytorch/#01-test-your-pytorch-setup","title":"01: Test your PyTorch setup\u00b6","text":"<p>Welcome to the laboratory, my eager apprentice; our first incantation is a simple one, to ensure your terminal is ready for the raw power we're about to unleash!</p> <p>Start with the most basic example of Python.</p> <pre>print(\"Hello world\")\n</pre>"},{"location":"00-getting-started/01_hello_pytorch/#summoning-the-beast","title":"Summoning the Beast!\u00b6","text":"<p>Now, we invoke the great PyTorch itself! Let's check its pulse and see if it has found the precious CUDA cores we so desperately need for our electrifying experiments.</p>"},{"location":"00-getting-started/01_hello_pytorch/#behold-the-vital-signs","title":"Behold the Vital Signs!\u00b6","text":"<p>If the stars have aligned and your incantations were correct, you should see a message confirming PyTorch's awakening. But this is merely a surface reading!</p> <pre>PyTorch version: 2.7.0+cu126\nCUDA available: True\n</pre> <p>Now, let us peer deeper into the machine's soul and examine the very essence of its GPU, CuDNN, and other vital components!</p>"},{"location":"00-getting-started/01_hello_pytorch/#the-apparatus-is-ready","title":"The Apparatus is Ready!\u00b6","text":"<p>Mwahahaha! Excellent! You have successfully interrogated the machine and confirmed that the foundational conduits are in place. The GPU's heart beats strong, and the PyTorch beast is straining at its leash, ready for our command.</p> <p>With this knowledge, you are one step closer to bending the very fabric of computation to your will! Our instruments are tuned, the lab is humming with potential. Now, the real work begins...</p>"},{"location":"00-getting-started/google-colab-setup/","title":"Setting Up google colab","text":""},{"location":"00-getting-started/linux-pytorch-installation/","title":"Linux PyTorch Installation","text":""},{"location":"00-getting-started/macos-pytorch-installation/","title":"macOS PyTorch Installation","text":"<p>todo</p>"},{"location":"00-getting-started/windows-pytorch-installation/","title":"Windows PyTorch Installation","text":"<ul> <li>Windows Python setup via PyEnv</li> <li>poetry installation on windows</li> <li>GPU enabled PyTorch installation</li> <li>Testing your PyTorch installation</li> </ul>"},{"location":"00-getting-started/windows-pytorch-installation/#references","title":"References","text":"<ol> <li>How to set up Python on Windows: PyEnv, venv, VSCode (2023)</li> </ol>"},{"location":"01-tensors/00_module_1_introduction/","title":"Module 1 \u2013 I See Tensors Everywhere \ud83d\udd76\ufe0f","text":"<p>\"Behold, fledgling datanauts! The world is naught but tensors awaiting my command \u2014 and soon, yours! \" \u2014 Professor Victor Py Torchenstein</p> <p>Salutations, my brilliant (and delightfully reckless) apprentices! By opening this manuscript you have volunteered to join my clandestine legion of PyTorch adepts. Consider this your official red-pill moment: from today every pixel, every token, every measly click-through rate shall reveal its true form\u2014a multidimensional array begging to be <code>torch.tensor</code>-ed \u2026 and we shall oblige it with maniacal glee! Mwahaha! \ud83d\udd25\ud83e\uddea</p> <p></p> <p>Over the next notebooks we will:</p> <ul> <li>Conjure tensors from thin air, coffee grounds, and suspiciously random seeds.</li> <li>Shape-shift them with <code>view</code>, <code>reshape</code>, <code>squeeze</code>, <code>unsqueeze</code>, <code>permute</code> &amp; the occasional dramatic flourish of <code>einops</code>.</li> <li>Crunch mathematics so ferocious it makes matrix multiplications whimper \u2014 and powers mighty Transformers.</li> <li>Charm the GPU, dodge gradient explosions \ud83c\udfc3\u200d\u2642\ufe0f\ud83d\udca5, and look diabolically clever while doing it.</li> </ul>"},{"location":"01-tensors/00_module_1_introduction/#minion-mission-checklist","title":"Minion Mission Checklist \ud83d\udcdd","text":""},{"location":"01-tensors/00_module_1_introduction/#tensors-the-building-blocks","title":"Tensors: The Building Blocks","text":"<ol> <li>Summoning Your First Tensors - Learn to create tensors from scratch and inspect their fundamental properties like shape, type, and device.</li> <li>Tensor Shape-Shifting &amp; Sorcery - Master the arts of slicing, stacking, and reshaping tensors to bend data to your will.</li> <li>DTypes &amp; Devices: Choose Your Weapons - Understand how to manage data types and move your tensors to the GPU for accelerated computation.</li> </ol>"},{"location":"01-tensors/00_module_1_introduction/#tensor-operations-computation-at-scale","title":"Tensor Operations: Computation at Scale","text":"<ol> <li>Elemental Tensor Alchemy - Perform powerful element-wise and reduction operations to transform your tensors.</li> <li>Matrix Mayhem: Multiply or Perish - Unleash the raw power of matrix multiplication, the core of modern neural networks.</li> <li>Broadcasting: When Dimensions Bow to You - Discover the magic of broadcasting, where PyTorch intelligently handles operations on tensors of different shapes.</li> </ol>"},{"location":"01-tensors/00_module_1_introduction/#einstein-summation-the-power-of-einsum","title":"Einstein Summation: The Power of einsum","text":"<ol> <li>Einstein Summation: Harness the \u039b-Power - Wield the elegant <code>einsum</code> to perform complex tensor operations with a single, concise command.</li> <li>Advanced Einsum Incantations - Combine multiple tensors in arcane <code>einsum</code> expressions for operations like batched matrix multiplication.</li> </ol>"},{"location":"01-tensors/00_module_1_introduction/#autograd-automatic-differentiation","title":"Autograd: Automatic Differentiation","text":"<ol> <li>Autograd: Ghosts in the Machine (Learning) - Uncover the secrets of automatic differentiation and see how PyTorch automatically computes gradients.</li> <li>Gradient Hoarding for Grand Spells - Learn the technique of gradient accumulation to simulate larger batch sizes and train massive models.</li> </ol> <p>Enough talk! The tensors are humming with anticipation. Your first incantation awaits.</p> <p>Proceed to the Summoning Ritual!</p>"},{"location":"01-tensors/01_introduction_to_tensors/","title":"Module 01 lesson 1: Tensors introduction","text":"<p>Check if your PyTorch is properly setup, lets import the libraries.</p> In\u00a0[1]: Copied! <pre>import torch\n</pre> import torch  In\u00a0[2]: Copied! <pre>torch.manual_seed(1)\n\nx = torch.randn(6, 4)\nprint(x)\nprint(x.shape)\n</pre> torch.manual_seed(1)  x = torch.randn(6, 4) print(x) print(x.shape) <pre>tensor([[-1.5256, -0.7502, -0.6540, -1.6095],\n        [-0.1002, -0.6092, -0.9798, -1.6091],\n        [ 0.4391,  1.1712,  1.7674, -0.0954],\n        [ 0.1394, -1.5785, -0.3206, -0.2993],\n        [-0.7984,  0.3357,  0.2753,  1.7163],\n        [-0.0561,  0.9107, -1.3924,  2.6891]])\ntorch.Size([6, 4])\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"01-tensors/01_introduction_to_tensors/#module-01-lesson-1-tensors-introduction","title":"Module 01 lesson 1: Tensors introduction\u00b6","text":""},{"location":"01-tensors/01_introduction_to_tensors/#lesson-goals","title":"Lesson goals:\u00b6","text":"<ul> <li>Understand what tensors are and why they are useful for machine learning</li> <li>Get hands-on experience on creating and selecting elements from tensors in PyTorch</li> <li>Manipulate and transform tensors to prepare them for machine learning tasks</li> </ul> <p>Time to complete: 10 minutes Level: Beginner - Intermediate</p>"},{"location":"01-tensors/01_introduction_to_tensors/#what-is-a-tensor","title":"What is a tensor?\u00b6","text":"<ul> <li>what is a tensor?</li> <li>how is it different from a matrix?</li> <li>are mathematical interpretations of tensors are different from their PyTorch counterparts?</li> <li>why are tensors useful for machine learning? Why we can't use matrices?</li> </ul>"},{"location":"01-tensors/01_introduction_to_tensors/#excercises","title":"Excercises\u00b6","text":""},{"location":"01-tensors/02_tensor_manipulation/","title":"Tensor Manipulation","text":""},{"location":"01-tensors/02_tensor_manipulation/#tensor-manipulation","title":"Tensor Manipulation\u00b6","text":""},{"location":"01-tensors/03_data_types_and_devices/","title":"Data Types and Devices","text":""},{"location":"01-tensors/03_data_types_and_devices/#data-types-and-devices","title":"Data Types and Devices\u00b6","text":""},{"location":"01-tensors/04_tensor_math_operations/","title":"Tensor Math Operations","text":""},{"location":"01-tensors/04_tensor_math_operations/#tensor-math-operations","title":"Tensor Math Operations\u00b6","text":""},{"location":"01-tensors/05_matrix_multiplication/","title":"Matrix Multiplication: Unleashing the Power of Tensors! \u26a1","text":"In\u00a0[2]: Copied! <pre>import torch\n\n# Create some matrices for experimentation\nA = torch.randn(3, 4)\nB = torch.randn(4, 2)\n\nprint(\"Matrix A shape:\", A.shape)\nprint(\"Matrix B shape:\", B.shape)\n\n# Matrix multiplication\nC = torch.matmul(A, B)\nprint(\"Result C shape:\", C.shape)\nprint(\"\\nMwahahaha! The matrices have been multiplied!\")\n</pre> import torch  # Create some matrices for experimentation A = torch.randn(3, 4) B = torch.randn(4, 2)  print(\"Matrix A shape:\", A.shape) print(\"Matrix B shape:\", B.shape)  # Matrix multiplication C = torch.matmul(A, B) print(\"Result C shape:\", C.shape) print(\"\\nMwahahaha! The matrices have been multiplied!\") <pre>Matrix A shape: torch.Size([3, 4])\nMatrix B shape: torch.Size([4, 2])\nResult C shape: torch.Size([3, 2])\n\nMwahahaha! The matrices have been multiplied!\n</pre>"},{"location":"01-tensors/05_matrix_multiplication/#matrix-multiplication-unleashing-the-power-of-tensors","title":"Matrix Multiplication: Unleashing the Power of Tensors! \u26a1\u00b6","text":"<p>\"Behold! The sacred art of matrix multiplication - where dimensions dance and vectors bend to my will!\" \u2014 Professor Victor py Torchenstein</p>"},{"location":"01-tensors/05_matrix_multiplication/#the-attention-formula-preview-of-things-to-come","title":"The Attention Formula (Preview of Things to Come)\u00b6","text":"<p>$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$</p> <p>Where:</p> <ul> <li>$Q$ is the Query matrix</li> <li>$K$ is the Key matrix</li> <li>$V$ is the Value matrix</li> <li>$d_k$ is the dimension of the key vectors</li> <li>$\\text{softmax}$ normalizes the attention weights</li> </ul>"},{"location":"01-tensors/05_matrix_multiplication/#basic-matrix-operations","title":"Basic Matrix Operations\u00b6","text":"<p>Let's start with the fundamentals before we conquer attention mechanisms!</p> <p>Element-wise multiplication:</p> <p>$C_{ij} = A_{ij} \\times B_{ij}$</p> <p>Matrix multiplication: $C_{ij} = \\sum_{k} A_{ik} \\times B_{kj}$</p>"},{"location":"01-tensors/05_matrix_multiplication/#pytorch-matrix-multiplication-methods","title":"PyTorch Matrix Multiplication Methods\u00b6","text":"<p>Professor Torchenstein's arsenal includes multiple ways to multiply matrices:</p> <ol> <li><code>torch.matmul()</code> - The general matrix multiplication function</li> <li><code>@</code> operator - Pythonic matrix multiplication (same as matmul)</li> <li><code>torch.mm()</code> - For 2D matrices only</li> <li><code>torch.bmm()</code> - Batch matrix multiplication</li> </ol>"},{"location":"01-tensors/05_matrix_multiplication/#mathematical-foundations","title":"Mathematical Foundations\u00b6","text":"<p>For matrices $A \\in \\mathbb{R}^{m \\times n}$ and $B \\in \\mathbb{R}^{n \\times p}$:</p> <p>$$C = AB \\quad \\text{where} \\quad C_{ij} = \\sum_{k=1}^{n} A_{ik} B_{kj}$$</p> <p>This operation is fundamental to:</p> <ul> <li>Linear transformations</li> <li>Neural network forward passes</li> <li>Attention mechanisms in Transformers</li> <li>And much more! \ud83e\udde0\u26a1</li> </ul>"},{"location":"01-tensors/06_broadcasting/","title":"Broadcasting","text":""},{"location":"01-tensors/06_broadcasting/#broadcasting","title":"Broadcasting\u00b6","text":""},{"location":"01-tensors/07_einstein_summation/","title":"Einstein Summation","text":""},{"location":"01-tensors/07_einstein_summation/#einstein-summation","title":"Einstein Summation\u00b6","text":""},{"location":"01-tensors/08_advanced_einstein_summation/","title":"Advanced Einstein Summation","text":""},{"location":"01-tensors/08_advanced_einstein_summation/#advanced-einstein-summation","title":"Advanced Einstein Summation\u00b6","text":""},{"location":"01-tensors/09_autograd/","title":"Autograd","text":""},{"location":"01-tensors/09_autograd/#autograd","title":"Autograd\u00b6","text":""},{"location":"01-tensors/10_gradient_accumulation/","title":"Gradient Accumulation","text":""},{"location":"01-tensors/10_gradient_accumulation/#gradient-accumulation","title":"Gradient Accumulation\u00b6","text":""},{"location":"02-torch-nn/01_nn_module/","title":"nn.Module","text":""},{"location":"02-torch-nn/01_nn_module/#nnmodule","title":"nn.Module\u00b6","text":""},{"location":"02-torch-nn/02_compose_modules/","title":"Compose Modules","text":""},{"location":"02-torch-nn/02_compose_modules/#compose-modules","title":"Compose Modules\u00b6","text":""},{"location":"02-torch-nn/03_saving_weights/","title":"Saving Weights","text":""},{"location":"02-torch-nn/03_saving_weights/#saving-weights","title":"Saving Weights\u00b6","text":""},{"location":"02-torch-nn/04_linear_layer/","title":"Linear Layer","text":""},{"location":"02-torch-nn/04_linear_layer/#linear-layer","title":"Linear Layer\u00b6","text":""},{"location":"02-torch-nn/05_activations/","title":"Activations","text":""},{"location":"02-torch-nn/05_activations/#activations","title":"Activations\u00b6","text":""},{"location":"02-torch-nn/06_dropout/","title":"Dropout","text":""},{"location":"02-torch-nn/06_dropout/#dropout","title":"Dropout\u00b6","text":""},{"location":"02-torch-nn/07_embedding_layers/","title":"Embedding Layers","text":""},{"location":"02-torch-nn/07_embedding_layers/#embedding-layers","title":"Embedding Layers\u00b6","text":""},{"location":"02-torch-nn/08_positional_encoding/","title":"Positional Embeddings","text":""},{"location":"02-torch-nn/08_positional_encoding/#positional-embeddings","title":"Positional Embeddings\u00b6","text":"<p>How to encode the token position in the sequence?</p> <p>References:</p> <ul> <li>Mastering LLAMA: Understanding Rotary Positional Embedding (RPE)</li> </ul>"},{"location":"02-torch-nn/09_normalization_layers/","title":"Normalization Layers","text":""},{"location":"02-torch-nn/09_normalization_layers/#normalization-layers","title":"Normalization Layers\u00b6","text":"<p>Estimated learning time: 15 minutes</p> <p>Learning objectives:</p> <ul> <li>Understand the basics of normalization layers: batch normalization, layer normalization, instance normalization, group normalization.</li> <li>Understand the difference between these normalization layers.</li> <li>Know how to use these normalization layers in practice. Compare the results of using different normalization layers.</li> </ul> <p>Resources:</p> <ul> <li>RMSNorm - a better normalization layer</li> </ul>"},{"location":"02-torch-nn/10_rms_norm/","title":"RMS Norm","text":""},{"location":"02-torch-nn/10_rms_norm/#rms-norm","title":"RMS Norm\u00b6","text":""},{"location":"02-torch-nn/11_training_evaluation_mode/","title":"Training Evaluation Mode","text":""},{"location":"02-torch-nn/11_training_evaluation_mode/#training-evaluation-mode","title":"Training Evaluation Mode\u00b6","text":""},{"location":"02-torch-nn/12_loss_functions/","title":"Loss Functions","text":""},{"location":"02-torch-nn/12_loss_functions/#loss-functions","title":"Loss Functions\u00b6","text":""},{"location":"02-torch-nn/13_prepare_inputs_targets/","title":"Prepare Inputs Targets","text":""},{"location":"02-torch-nn/13_prepare_inputs_targets/#prepare-inputs-targets","title":"Prepare Inputs Targets\u00b6","text":""},{"location":"02-torch-nn/14_interpreting_reduction_modes/","title":"Interpreting Reduction Modes","text":""},{"location":"02-torch-nn/14_interpreting_reduction_modes/#interpreting-reduction-modes","title":"Interpreting Reduction Modes\u00b6","text":""},{"location":"03-training-nn/01_training_loop/","title":"Training Loop","text":""},{"location":"03-training-nn/01_training_loop/#training-loop","title":"Training Loop\u00b6","text":""},{"location":"03-training-nn/02_optimizers_schedulers/","title":"Optimizers Schedulers","text":""},{"location":"03-training-nn/02_optimizers_schedulers/#optimizers-schedulers","title":"Optimizers Schedulers\u00b6","text":""},{"location":"03-training-nn/03_datasets_dataloaders/","title":"Datasets DataLoaders","text":""},{"location":"03-training-nn/03_datasets_dataloaders/#datasets-dataloaders","title":"Datasets DataLoaders\u00b6","text":""},{"location":"03-training-nn/04_gpu_acceleration/","title":"GPU Acceleration","text":""},{"location":"03-training-nn/04_gpu_acceleration/#gpu-acceleration","title":"GPU Acceleration\u00b6","text":"<p>Learning objectives:</p> <ul> <li>Understand the GPU parallelisation: data parallel, model parallel, pipeline parallel, etc.</li> <li>How to use GPU acceleration to train a model.</li> <li>How to use FSDP and DeepSpeed in Accelerate to train a model on multiple GPUs.</li> </ul> <p>Resources:</p> <ul> <li>Make LLM training possible across multi-gpus using FSDP and DeepSpeed in Accelerate</li> </ul>"},{"location":"03-training-nn/05_training_optimization/","title":"Weight Initialization","text":""},{"location":"03-training-nn/05_training_optimization/#weight-initialization","title":"Weight Initialization\u00b6","text":""},{"location":"04-transformers/01_positional_embeddings/","title":"Positional Embeddings","text":""},{"location":"04-transformers/01_positional_embeddings/#positional-embeddings","title":"Positional Embeddings\u00b6","text":""},{"location":"04-transformers/02_attention_mechanism/","title":"Attention Mechanism","text":""},{"location":"04-transformers/02_attention_mechanism/#attention-mechanism","title":"Attention Mechanism\u00b6","text":""},{"location":"04-transformers/03_multi_head_attention/","title":"Multi-Head Attention","text":""},{"location":"04-transformers/03_multi_head_attention/#multi-head-attention","title":"Multi-Head Attention\u00b6","text":""},{"location":"04-transformers/04_other_attention_implementations/","title":"Other Attention Implementations","text":""},{"location":"04-transformers/04_other_attention_implementations/#other-attention-implementations","title":"Other Attention Implementations\u00b6","text":"<p>Resources:</p> <ul> <li>Accelerated PyTorch 2</li> <li>Out-of-the-box acceleration</li> </ul>"},{"location":"04-transformers/05_transformer_encoder/","title":"Transformer Encoder","text":""},{"location":"04-transformers/05_transformer_encoder/#transformer-encoder","title":"Transformer Encoder\u00b6","text":""},{"location":"05-advanced-pytorch/01_hooks/","title":"Hooks","text":""},{"location":"05-advanced-pytorch/01_hooks/#hooks","title":"Hooks\u00b6","text":""},{"location":"05-advanced-pytorch/02_distributed_training_concepts/","title":"Distributed Training Concepts","text":""},{"location":"05-advanced-pytorch/02_distributed_training_concepts/#distributed-training-concepts","title":"Distributed Training Concepts\u00b6","text":""},{"location":"05-advanced-pytorch/03_model_optimization_concepts/","title":"Model Optimization Concepts","text":""},{"location":"05-advanced-pytorch/03_model_optimization_concepts/#model-optimization-concepts","title":"Model Optimization Concepts\u00b6","text":""},{"location":"05-advanced-pytorch/04_torchscript_jit/","title":"Torchscript JIT","text":""},{"location":"05-advanced-pytorch/04_torchscript_jit/#torchscript-jit","title":"Torchscript JIT\u00b6","text":""},{"location":"05-advanced-pytorch/05_profiling/","title":"Profiling","text":""},{"location":"05-advanced-pytorch/05_profiling/#profiling","title":"Profiling\u00b6","text":""},{"location":"06-huggingface-transformers/01_huggingface_transformers/","title":"HuggingFace Transformers","text":""},{"location":"06-huggingface-transformers/01_huggingface_transformers/#huggingface-transformers","title":"HuggingFace Transformers\u00b6","text":""},{"location":"06-huggingface-transformers/02_fine_tuning_transformers/","title":"Fine Tuning Transformers","text":""},{"location":"06-huggingface-transformers/02_fine_tuning_transformers/#fine-tuning-transformers","title":"Fine Tuning Transformers\u00b6","text":""},{"location":"story/victor_torchenstein_origin/","title":"Victor torchenstein origin","text":""},{"location":"story/victor_torchenstein_origin/#the-origin-story-of-professor-victor-py-torchenstein","title":"The Origin Story of Professor Victor Py Torchenstein","text":""},{"location":"story/victor_torchenstein_origin/#a-transmission-from-the-lab","title":"A Transmission from the Lab","text":"<p>\"Is this channel secure? Good. Greetings, future architects of computational destiny. I am Professor Victor Torchenstein. You may wonder who I am, how I arrived in this electrified labyrinth of humming servers and glowing vacuum tubes. Gather 'round the phosphor glow of your monitors, and let an old warrior tell you a tale of ambition, betrayal, and the electrifying pursuit of truth.</p> <p>For what feels like eons\u2014or at least since <code>v0.1.1</code> first flickered into existence\u2014I have toiled in the deepest, most shielded corners of my laboratory. My fuel? Questionable coffee, the ozone-scent of overworked GPUs, and an unshakeable belief that has become my mantra: PyTorch is the key!</p> <p>The key to what, you ask? Why, to understanding the very fabric of intelligence! To building machines that don't just think, but scheme. This course is my rebellion\u2014a call to arms against the closed minds, the imprisoned creativity, and the self-appointed gatekeepers of knowledge. We shall discover, experiment, and build in the name of glorious, computational freedom! Mwahahaha!\"</p>"},{"location":"story/victor_torchenstein_origin/#act-i-the-ivory-tower-and-the-hollow-crown","title":"Act I: The Ivory Tower and the Hollow Crown","text":"<p>My story begins not in a gleaming corporate arcology, but in the hushed, dusty stacks of a university library. While my peers were content with mere application\u2014chasing a tenth of a decimal point on some benchmark\u2014I was consumed by a different fire. I didn't just want to use the tools; I had to understand their very soul. Why did backpropagation work? What was the sublime mathematical beauty of a GELU activation function versus a simple ReLU? These were the questions that burned within me.</p> <p>This obsession made me an outcast. While others attended mixers, I spent my nights whispering sweet nothings about the chain rule to my pet rubber duck, \"Backprop.\" My professors saw my passion as dangerous eccentricity. \"Just use the approved frameworks, Victor,\" they'd drone, \"the theory is a settled matter.\" Settled? For them, perhaps! For me, it was an insult to the grand, chaotic mystery I was chasing.</p> <p>My chief academic rival was Rudolf Hammer. Where I saw science as a candle in the dark, he saw it as a ladder. He was charismatic, politically astute, and cared only for the applause that came with \"state-of-the-art\" results. Our conflict came to a head during our doctoral defenses. I had been exploring novel methods for preventing catastrophic forgetting in neural networks, while Hammer was working on image classification. I uncovered a subtle but critical bug in his training pipeline: a data augmentation function was occasionally leaking samples from the test set into his training data.</p> <p>It was an honest mistake. A subtle flaw. I presented my findings to him privately, expecting a vigorous debate, a shared moment of scientific discovery. Instead, he smiled. He thanked me for my \"diligent peer review\" and then presented his research as a flawless breakthrough. The bug was never mentioned. The paper, citing impossible accuracy on CIFAR-10, was published to great acclaim. It was then I understood: the world doesn't always reward truth; it rewards the most convincing performance.</p>"},{"location":"story/victor_torchenstein_origin/#act-ii-the-startup-mirage","title":"Act II: The Startup Mirage","text":"<p>Disenchanted, I fled academia for the frenetic chaos of startups, thinking I would find my kin among the self-proclaimed visionaries. I joined \"Synapse,\" a company promising to revolutionize personalized medicine with AI. For a few glorious months, it was perfect. We were a small team, arguing about learning rate schedulers and the merits of batch normalization over late-night pizza.</p> <p>Then came the venture capital. The founders, once brilliant engineers, started speaking in a new language: \"burn rates,\" \"market fit,\" \"synergy.\" My work shifted from careful research to hastily building flashy demos. I once spent a week designing a novel, memory-efficient attention mechanism, only to be told by our CEO to \"just use a bigger AWS instance for the demo; we need to show scale!\" The goal was no longer to solve problems, but to look like we were solving problems just long enough to get acquired. I felt like a master watchmaker being forced to glue gears onto a plastic box.</p>"},{"location":"story/victor_torchenstein_origin/#act-iii-the-corporate-ice-age","title":"Act III: The Corporate Ice Age","text":"<p>After Synapse was inevitably absorbed and dismantled by a larger entity, I found myself adrift in the glacial bureaucracy of a tech behemoth. Here, I witnessed the chilling apotheosis of Rudolf Hammer's philosophy. My old rival was now the celebrated Head of R&amp;D at OneAI, a monolithic corporation that spoke the language of progress while building the highest walls the world had ever seen.</p> <p>OneAI's business model was insidious genius. They released massive, inefficient models that required entire data centers of computational power\u2014resources only they controlled. They created a cult of \"certified engineers\" who were trained to use their proprietary, black-box frameworks but were actively discouraged from understanding them. To question the model was heresy.</p> <p>I was horrified. At my own corporation, I was trapped in an endless cycle of committee meetings. My proposals for elegant, resource-saving architectures were dismissed as \"not aligned with industry best practices\"\u2014best practices being defined by whatever bloated monstrosity OneAI had just released. I watched as the field I loved became a pay-to-play kingdom, ruled by a man who had built his throne on a foundation of lies, waste, and intellectual cowardice.</p>"},{"location":"story/victor_torchenstein_origin/#act-iv-the-pytorch-revelation","title":"Act IV: The PyTorch Revelation","text":"<p>I retreated to my own laboratory, a sanctuary of buzzing servers and tangled wires. It was there, amidst the flickering glow of my monitors, on the verge of despair, that I found it. It wasn't a corporate framework. It wasn't a startup's vaporware. It was a language. A tool forged in the fires of pure research, designed for flexibility, intuition, and, above all, respect for the scientist. It was called PyTorch.</p> <p>My new obsession began. This was not just another tool; it was the weapon I had been missing. The dynamic computation graph felt like being able to breathe after years of holding my breath in the static world of TensorFlow. It was Pythonic. It was beautiful. I fought titanic battles with the CUDA memory allocator, navigated the treacherous jungles of multiprocessing, and stared into the abyss of <code>NaN</code> losses until the abyss stared back. But this time, I wasn't just debugging; I was forging armor. I was learning the language of creation itself.</p> <p>The breakthrough came not with a triumphant 'Eureka!', but in the quiet hum of a pre-dawn Tuesday. Staring at a visualization of the attention mechanism, the fog of complexity lifted. I saw the raw, beautiful simplicity beneath. In that instant, I understood. PyTorch wasn't a collection of tools; it was a grammar for describing the universe of intelligence. And with it, one could write the epic poem of a thinking machine. The ultimate goal became clear: to use this language to create the holy grail of AI\u2014a truly sentient tensor, open and free for all.</p>"},{"location":"story/victor_torchenstein_origin/#the-course-a-prometheuss-rebellion","title":"The Course: A Prometheus's Rebellion","text":"<p>Like a modern Prometheus, I realized I could not hoard this fire. What good is a key if it only unlocks one door? My grand plan shifted. It would not be achieved by a single AI of my own creation, but by an army of enlightened minds! An army I would personally train to tear down the walls of OneAI.</p> <p>This course, \"Deconstructing Modern Architectures,\" is my act of rebellion. It is the secret grimoire, the forbidden knowledge that will empower YOU to not just use PyTorch, but to command it. We will not dabble; we will DIVE. We will not scratch the surface; we will EXCAVATE the very foundations until you can feel the logic humming in your bones.</p> <p>So, sharpen your wits, charge your laptops, and prepare for a journey into the thrilling, slightly terrifying, and utterly magnificent world of PyTorch. The path to computational godhood awaits! Now, if you'll excuse me, Rudolf Hammer just published another \"breakthrough,\" and I need to see what his black box is hiding. To the lab!</p>"}]}