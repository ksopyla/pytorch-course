{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to the PyTorch Course!","text":"<p>This is the main page of the PyTorch course.</p> <p>Navigate through the lessons using the menu. </p>"},{"location":"pytorch-course-structure/","title":"PyTorch Course: Deconstructing Modern Architectures","text":"<p>Course Goal: To provide a deep and essential understanding of PyTorch building blocks and their practical application in designing, understanding, and implementing modern Transformer and Diffusion-based neural network architectures.</p> <p>Learner level: Beginner - Advanced</p>"},{"location":"pytorch-course-structure/#module-0-getting-started-with-pytorch","title":"Module 0: Getting Started with PyTorch","text":"<p>This module ensures learners have a working PyTorch environment and a first taste of its capabilities.</p> <p>What you will learn: 1. How to setup a PyTorch environment for different operating systems and test it.</p>"},{"location":"pytorch-course-structure/#lessons","title":"Lessons:","text":"<ol> <li>PyTorch Course Structure - this page, course aim and structure, guide through </li> <li>Setting Up Your PyTorch Environments:     1.Setting Up windows dev environment - create a windows dev environment with windows pyenv and poetry, install main dependencies with and without GPU support     2.Setting Up linux dev environment - create a Ubuntu dev environment with linux pyenv and poetry, install main dependencies with and without GPU support     3.Setting Up macos dev environment - create a macos dev environment with macos pyenv and poetry, install main dependencies with and without GPU support     4.Setting Up google colab - create a google colab dev environment with google colab</li> </ol>"},{"location":"pytorch-course-structure/#module-1-pytorch-core-i-see-tensors-everywhere","title":"Module 1: PyTorch Core - I see tensors everywhere","text":"<p>This module dives into the fundamental components of PyTorch, essential for any deep learning task.</p> <p>{width=500 height=300, align=center}</p>"},{"location":"pytorch-course-structure/#11-tensors-the-building-blocks","title":"1.1 Tensors: The Building Blocks","text":"<p>What you will learn: </p> <ol> <li>Tensor Concept. What is a tensor? Tensor vs. Matrix. Mathematical vs. PyTorch interpretation. Why tensors are crucial for ML</li> <li>PyTorch Basics: Tensor creation and their attributes (dtype, shape, device).</li> <li>Tensor manipulation: Indexing, Slicing, Joining (torch.cat, torch.stack), Splitting. Manipulating tensor shapes (reshape, view, squeeze, unsqueeze, permute, transpose).</li> </ol>"},{"location":"pytorch-course-structure/#11-lessons","title":"1.1 Lessons:","text":"<ol> <li>Introduction to Tensors - Introduction to tensors, their properties, and their importance in machine learning.  Creating tensors (from lists, NumPy, torch.rand, torch.zeros, torch.ones, torch.arange, torch.linspace). How to check their attributes and shapes.</li> <li>Tensor manipulation - Indexing, Slicing, Joining (torch.cat, torch.stack), Splitting. Manipulating tensor shapes (reshape, view, squeeze, unsqueeze, permute, transpose).</li> <li>Data Types and Devices - Importance of data types (float32, float16, bfloat16, int64 etc.). CPU vs. GPU computations. Checking and changing dtype. Moving tensors between devices (.to(device), .cpu(), .cuda()). Best practices for mixed-precision training (conceptual introduction). Implications of data types on memory and speed.</li> </ol>"},{"location":"pytorch-course-structure/#12-tensor-operations-computation-at-scale","title":"1.2 Tensor Operations: Computation at Scale","text":"<p>What you will learn:</p> <ol> <li>Overview of tensor math. Element-wise operations. Reduction operations (sum, mean, max, min, std). Basic matrix multiplication (torch.mm, torch.matmul, @ operator). Broadcasting: rules and practical examples with verifiable tiny data. In-place operations.</li> </ol>"},{"location":"pytorch-course-structure/#12-lessons","title":"1.2 Lessons:","text":"<ol> <li> <p>Tensor Math Operations - Overview of tensor math. Element-wise operations. Reduction operations across dimensions (sum, mean, max, min, std). </p> </li> <li> <p>Matrix Multiplication - 2D matrix multiplication (torch.mm, torch.matmul, @ operator). Batch matrix multiplication (torch.bmm).</p> </li> <li>Broadcasting - Broadcasting rules with practical examples across different dimensions. Broadcast math operations and vector or matrix multiplications.</li> </ol>"},{"location":"pytorch-course-structure/#13-einstein-summation-the-power-of-einsum","title":"1.3 Einstein Summation: The Power of einsum","text":"<p>What you will learn:</p> <ol> <li>Understanding Einstein notation. Why it's powerful for complex operations (e.g., attention).</li> </ol>"},{"location":"pytorch-course-structure/#13-lessons","title":"1.3 Lessons:","text":"<ol> <li>Einstein Summation - Simple einsum examples (vector dot product, matrix multiplication, transpose).</li> <li>Advanced Einstein Summation - einsum for more complex operations like batch matrix multiplication, tensor contractions relevant to attention mechanisms. Examples with dimensions mirroring those in Transformers.</li> </ol>"},{"location":"pytorch-course-structure/#14-autograd-automatic-differentiation","title":"1.4 Autograd: Automatic Differentiation","text":"<p>What you will learn:</p> <ol> <li>What are gradients? The computational graph. How PyTorch tracks operations.</li> <li>requires_grad attribute. Performing backward pass with .backward(). Accessing gradients with .grad. torch.no_grad() and tensor.detach().</li> <li>Gradient accumulation. Potential pitfalls. Visualizing computational graphs (conceptually).</li> </ol>"},{"location":"pytorch-course-structure/#14-lessons","title":"1.4 Lessons:","text":"<ol> <li>Autograd - What are gradients? The computational graph. How PyTorch tracks operations.</li> <li>Gradient Accumulation - Gradient accumulation. Potential pitfalls. Visualizing computational graphs (conceptually).</li> </ol>"},{"location":"pytorch-course-structure/#module-2-torchnn-building-neural-networks","title":"Module 2: torch.nn \u2014 Building Neural Networks","text":"<p>This module explores the layer-building API that powers every PyTorch model.</p>"},{"location":"pytorch-course-structure/#21-the-nnmodule-blueprint","title":"2.1 The <code>nn.Module</code> Blueprint","text":"<p>What you will learn: - The role of <code>nn.Module</code> as the base class for layers and models. - Implementing <code>__init__</code> and <code>forward</code>. - Registering parameters and buffers. - Composing modules with <code>nn.Sequential</code>, <code>nn.ModuleList</code>, and <code>nn.ModuleDict</code>. - Saving and restoring weights with <code>state_dict</code>.</p>"},{"location":"pytorch-course-structure/#21-lessons","title":"2.1 Lessons:","text":"<ol> <li>nn.Module - The role of `nn.Module` as the base class for layers and models.  `init` and `forward` methods.</li> <li>Compose Modules - Composing modules with `nn.Sequential`, `nn.ModuleList`, and `nn.ModuleDict`.</li> <li>Saving Weights - Saving and restoring weights with `state_dict`.</li> </ol>"},{"location":"pytorch-course-structure/#22-linear-layer-and-activations","title":"2.2 Linear Layer and Activations","text":"<p>What you will learn: - Linear layers and high-dimensional matrix multiplication.  - What is the role of linear layers in attention mechanisms (query, key, value)? - Activation functions (ReLU, GELU, SiLU, Tanh, Softmax, etc.). - Dropout for regularisation.  </p>"},{"location":"pytorch-course-structure/#22-lessons","title":"2.2 Lessons:","text":"<ol> <li>Linear Layer - Linear layer and high-dimensional matrix multiplication. How the linear layer transforms the input tensor into an output tensor.</li> <li>Activations - Activation functions (ReLU, GELU, SiLU, Tanh, Softmax, etc.). </li> <li>Dropout - Dropout for regularisation.</li> </ol>"},{"location":"pytorch-course-structure/#23-embedding-layers","title":"2.3 Embedding Layers","text":"<p>What you will learn: - Embedding layers and their purpose in neural networks. - Embedding layer implementation from scratch, initialisation, and usage. - Positional encoding and how it is used to inject order into the model.</p>"},{"location":"pytorch-course-structure/#23-lessons","title":"2.3 Lessons:","text":"<ol> <li>Embedding Layers - Embedding layers and their purpose in neural networks. Input to embedding layer and how to interpret the output.</li> <li>Positional Encoding - Positional encoding and how it is used to inject order into the model.</li> </ol>"},{"location":"pytorch-course-structure/#24-normalisation-layers","title":"2.4 Normalisation Layers","text":"<p>What you will learn: - BatchNorm vs. LayerNorm and when to use each. - RMSNorm and other modern alternatives. - Training vs. evaluation mode caveats.</p>"},{"location":"pytorch-course-structure/#24-lessons","title":"2.4 Lessons:","text":"<ol> <li>Normalization Layers - what the normalisation layer does and what is the purpose of the normalisation layer. BatchNorm vs. LayerNorm and when to use each.</li> <li>RMS Norm - RMSNorm and other modern alternatives.</li> <li>Training Evaluation Mode - Training vs. evaluation mode caveats.</li> </ol>"},{"location":"pytorch-course-structure/#25-loss-functions-guiding-optimisation","title":"2.5 Loss Functions \u2014 Guiding Optimisation","text":"<p>What you will learn: - Loss functions recap, the main types of loss functions and when to use each.  - Prepare inputs and targets for loss functions and outputs interpretation (logits vs. probabilities). - Interpreting reduction modes and ignore indices.</p>"},{"location":"pytorch-course-structure/#25-lessons","title":"2.5 Lessons:","text":"<ol> <li>Loss Functions - Loss functions recap, the main types of loss functions and when to use each. </li> <li>Prepare Inputs Targets - Prepare inputs and targets for loss functions and outputs interpretation (logits vs. probabilities). </li> <li>Interpreting Reduction Modes - Interpreting reduction modes and ignore indices.</li> </ol>"},{"location":"pytorch-course-structure/#module-3-training-workflows","title":"Module 3: Training Workflows","text":"<p>Turn static graphs into learning machines by mastering data pipelines, loops, and monitoring tools.</p>"},{"location":"pytorch-course-structure/#31-the-training-loop","title":"3.1 The Training Loop","text":"<p>What you will learn: - Anatomy of an epoch: forward \u2192 loss \u2192 backward \u2192 optimiser step. - Gradient accumulation &amp; clipping. - Building a reusable training engine.</p> <p>Lessons: 1. Training Loop</p>"},{"location":"pytorch-course-structure/#32-optimisers-schedulers","title":"3.2 Optimisers &amp; Schedulers","text":"<p>What you will learn: - SGD with momentum, Adam, and AdamW under the hood. - Learning-rate scheduling strategies. - Weight decay and regularisation.</p>"},{"location":"pytorch-course-structure/#32-lessons","title":"3.2 Lessons:","text":"<ol> <li>Optimizers Schedulers - todo</li> </ol>"},{"location":"pytorch-course-structure/#33-datasets-dataloaders","title":"3.3 Datasets &amp; DataLoaders","text":"<p>What you will learn: - Implementing custom <code>Dataset</code> subclasses. - Batching, shuffling and parallel loading with <code>DataLoader</code>. - Data augmentation pipelines.</p>"},{"location":"pytorch-course-structure/#33-lessons","title":"3.3 Lessons:","text":"<ol> <li>Datasets DataLoaders</li> </ol>"},{"location":"pytorch-course-structure/#34-accelerating-with-gpus","title":"3.4 Accelerating with GPUs","text":"<p>What you will learn: - Device discovery and placement. - Moving models and data safely. - Mixed-precision training best practices.</p>"},{"location":"pytorch-course-structure/#34-lessons","title":"3.4 Lessons:","text":"<ol> <li>GPU Acceleration</li> </ol>"},{"location":"pytorch-course-structure/#35-weight-initialisation","title":"3.5 Weight Initialisation","text":"<p>What you will learn: - Why initial values matter. - Xavier (Glorot), Kaiming, and custom strategies.  </p>"},{"location":"pytorch-course-structure/#35-lessons","title":"3.5 Lessons:","text":"<ol> <li>Weight Initialization - todo</li> </ol>"},{"location":"pytorch-course-structure/#module-4-deconstructing-transformer-architectures-main-building-blocks-of-transformers","title":"Module 4: Deconstructing Transformer Architectures - main building blocks of transformers","text":"<p>From embeddings to multi-head attention, this module builds a Transformer from first principles and explores modern variants.</p>"},{"location":"pytorch-course-structure/#42-various-way-of-injecting-order-positional-encoding-rotary-positional-embeddings-rope","title":"4.2 Various way of injecting Order  - Positional Encoding, Rotary Positional Embeddings (RoPE)","text":"<p>What you will learn: - Absolute sinusoidal vs. learned embeddings. - Relative positional encodings. - Rotary Positional Embeddings (RoPE).</p> <p>Lessons: 1. Positional Embeddings</p>"},{"location":"pytorch-course-structure/#43-scaled-dot-product-attention","title":"4.3 Scaled Dot-Product Attention","text":"<p>What you will learn: - Query, Key, Value formalism. How to interpret the output of the attention mechanism. - Self-attention and cross-attention. - Masking techniques.</p> <p>Lessons: 1. Attention Mechanism</p>"},{"location":"pytorch-course-structure/#44-multi-head-attention","title":"4.4 Multi-Head Attention","text":"<p>What you will learn: - Motivation for multiple heads. - Building MHA by projecting Q, K, V. - Comparing custom implementation with <code>nn.MultiheadAttention</code>.</p> <p>Lessons: 1. Multi-Head Attention</p>"},{"location":"pytorch-course-structure/#45-other-attention-implementations","title":"4.5 Other attention implementations","text":"<p>What you will learn: - Accelerated attention with SDPA - a few pytorch implementations, which to choose? - Flash Attention - a faster attention mechanism - Flash Attention 2 - a faster and more memory efficient attention mechanism</p>"},{"location":"pytorch-course-structure/#45-lessons","title":"4.5 Lessons:","text":"<ol> <li>Other Attention Implementations</li> </ol>"},{"location":"pytorch-course-structure/#46-transformer-encoder","title":"4.6 Transformer Encoder","text":"<p>What you will learn: - Residual connections &amp; Layer Normalisation. - Position-wise Feed-Forward Networks. - Encoder-decoder attention.</p>"},{"location":"pytorch-course-structure/#46-lessons","title":"4.6 Lessons:","text":"<ol> <li>Transformer Encoder</li> </ol>"},{"location":"pytorch-course-structure/#module-5-advanced-pytorch-best-practices","title":"Module 5: Advanced PyTorch &amp; Best Practices","text":""},{"location":"pytorch-course-structure/#51-pytorch-hooks-peeking-inside","title":"5.1 PyTorch Hooks \u2013 Peeking Inside","text":"<p>What you will learn: - PyTorch Hooks - Peeking Inside - How to use PyTorch Hooks to inspect the inner workings of a model.</p>"},{"location":"pytorch-course-structure/#51-lessons","title":"5.1 Lessons:","text":"<ol> <li>Hooks</li> </ol>"},{"location":"pytorch-course-structure/#52-distributed-training-concepts","title":"5.2 Distributed Training Concepts","text":"<p>What you will learn: - Distributed Training Concepts - How to use PyTorch Distributed Training to train a model on multiple GPUs.</p>"},{"location":"pytorch-course-structure/#52-lessons","title":"5.2 Lessons:","text":"<ol> <li>Distributed Training Concepts</li> </ol>"},{"location":"pytorch-course-structure/#53-model-optimisation-quantisation-pruning","title":"5.3 Model Optimisation \u2013 Quantisation &amp; Pruning","text":"<p>What you will learn: - Model Optimisation \u2013 Quantisation &amp; Pruning - How to use PyTorch Model Optimisation to quantise and prune a model.</p>"},{"location":"pytorch-course-structure/#53-lessons","title":"5.3 Lessons:","text":"<ol> <li>Model Optimization Concepts</li> </ol>"},{"location":"pytorch-course-structure/#54-torchscript-jit-for-deployment","title":"5.4 TorchScript &amp; JIT for Deployment","text":"<p>What you will learn: - TorchScript &amp; JIT for Deployment - How to use PyTorch TorchScript &amp; JIT to deploy a model.</p>"},{"location":"pytorch-course-structure/#54-lessons","title":"5.4 Lessons:","text":"<ol> <li>Torchscript JIT</li> </ol>"},{"location":"pytorch-course-structure/#55-profiling-performance-tuning","title":"5.5 Profiling &amp; Performance Tuning","text":"<p>What you will learn: - Profiling &amp; Performance Tuning - How to use PyTorch Profiling to tune the performance of a model.</p>"},{"location":"pytorch-course-structure/#55-lessons","title":"5.5 Lessons:","text":"<ol> <li>Profiling</li> </ol>"},{"location":"pytorch-course-structure/#module-6-hugging-face-transformers-in-practice","title":"Module 6 Hugging Face Transformers in Practice","text":""},{"location":"pytorch-course-structure/#61-loading-the-pre-trained-models-with-transformers","title":"6.1 Loading the pre-trained models with <code>transformers</code>.","text":"<p>What you will learn: - Loading the pre-trained models with <code>transformers</code>. - Using <code>AutoModel</code> and <code>Trainer</code> APIs. - Inference with the pre-trained models.</p>"},{"location":"pytorch-course-structure/#61-lessons","title":"6.1 Lessons:","text":"<ol> <li>HuggingFace Transformers</li> </ol>"},{"location":"pytorch-course-structure/#62-fine-tuning-the-pre-trained-models","title":"6.2 Fine-tuning the pre-trained models.","text":"<p>What you will learn: - Fine-tuning the pre-trained models. - Using <code>Trainer</code> API. With training loop and trainer API. - Using <code>AutoModelForSequenceClassification</code> and <code>AutoTokenizer</code> APIs.</p>"},{"location":"pytorch-course-structure/#62-lessons","title":"6.2 Lessons:","text":"<ol> <li>Fine Tuning Transformers</li> </ol>"},{"location":"00-getting-started/01_hello_pytorch/","title":"Lesson 1: Hello PyTorch","text":"In\u00a0[1]: Copied! <pre>print(\"Hello world\")\n</pre> print(\"Hello world\") <pre>Hello world\n</pre> In\u00a0[2]: Copied! <pre>import torch\nprint(f\"PyTorch version: {torch.__version__}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\n</pre> import torch print(f\"PyTorch version: {torch.__version__}\") print(f\"CUDA available: {torch.cuda.is_available()}\") <pre>PyTorch version: 2.7.0+cu126\nCUDA available: True\n</pre>"},{"location":"00-getting-started/01_hello_pytorch/#lesson-1-hello-pytorch","title":"Lesson 1: Hello PyTorch\u00b6","text":"<p>Start with the most basic example of Python</p>"},{"location":"00-getting-started/01_hello_pytorch/#import-pytorch","title":"Import PyTorch\u00b6","text":"<p>Display a PyTorch version and GPU availability</p>"},{"location":"00-getting-started/google-colab-setup/","title":"Setting Up google colab","text":""},{"location":"00-getting-started/linux-pytorch-installation/","title":"Linux PyTorch Installation","text":""},{"location":"00-getting-started/macos-pytorch-installation/","title":"macOS PyTorch Installation","text":"<p>todo</p>"},{"location":"00-getting-started/windows-pytorch-installation/","title":"Windows PyTorch Installation","text":"<ul> <li>Windows Python setup via PyEnv</li> <li>poetry installation on windows</li> <li>GPU enabled PyTorch installation</li> <li>Testing your PyTorch installation</li> </ul>"},{"location":"00-getting-started/windows-pytorch-installation/#references","title":"References","text":"<ol> <li>How to set up Python on Windows: PyEnv, venv, VSCode (2023)</li> </ol>"},{"location":"01-tensors/01_introduction_to_tensors/","title":"Introduction to Tensors","text":""},{"location":"01-tensors/01_introduction_to_tensors/#introduction-to-tensors","title":"Introduction to Tensors\u00b6","text":""},{"location":"01-tensors/01_tensors_intro/","title":"Module 01 lesson 1: Tensors introduction","text":""},{"location":"01-tensors/01_tensors_intro/#module-01-lesson-1-tensors-introduction","title":"Module 01 lesson 1: Tensors introduction\u00b6","text":""},{"location":"01-tensors/01_tensors_intro/#lesson-goals","title":"Lesson goals:\u00b6","text":"<ul> <li>Understand what tensors are and why they are useful for machine learning</li> <li>Get hands-on experience on creating and selecting elements from tensors in PyTorch</li> <li>Manipulate and transform tensors to prepare them for machine learning tasks</li> </ul> <p>Time to complete: 10 minutes Level: Beginner - Intermediate</p>"},{"location":"01-tensors/01_tensors_intro/#what-is-a-tensor","title":"What is a tensor?\u00b6","text":"<ul> <li><p>what is a tensor?</p> </li> <li><p>how is it different from a matrix?</p> </li> <li><p>are mathematical interpretations of tensors are different from their PyTorch counterparts?</p> </li> <li><p>why are tensors useful for machine learning? Why we can't use matrices?</p> </li> <li></li> </ul>"},{"location":"01-tensors/02_tensor_manipulation/","title":"Tensor Manipulation","text":""},{"location":"01-tensors/02_tensor_manipulation/#tensor-manipulation","title":"Tensor Manipulation\u00b6","text":""},{"location":"01-tensors/03_data_types_and_devices/","title":"Data Types and Devices","text":""},{"location":"01-tensors/03_data_types_and_devices/#data-types-and-devices","title":"Data Types and Devices\u00b6","text":""},{"location":"01-tensors/04_tensor_math_operations/","title":"Tensor Math Operations","text":""},{"location":"01-tensors/04_tensor_math_operations/#tensor-math-operations","title":"Tensor Math Operations\u00b6","text":""},{"location":"01-tensors/05_matrix_multiplication/","title":"Matrix Multiplication","text":""},{"location":"01-tensors/05_matrix_multiplication/#matrix-multiplication","title":"Matrix Multiplication\u00b6","text":""},{"location":"01-tensors/06_broadcasting/","title":"Broadcasting","text":""},{"location":"01-tensors/06_broadcasting/#broadcasting","title":"Broadcasting\u00b6","text":""},{"location":"01-tensors/07_einstein_summation/","title":"Einstein Summation","text":""},{"location":"01-tensors/07_einstein_summation/#einstein-summation","title":"Einstein Summation\u00b6","text":""},{"location":"01-tensors/08_advanced_einstein_summation/","title":"Advanced Einstein Summation","text":""},{"location":"01-tensors/08_advanced_einstein_summation/#advanced-einstein-summation","title":"Advanced Einstein Summation\u00b6","text":""},{"location":"01-tensors/09_autograd/","title":"Autograd","text":""},{"location":"01-tensors/09_autograd/#autograd","title":"Autograd\u00b6","text":""},{"location":"01-tensors/10_gradient_accumulation/","title":"Gradient Accumulation","text":""},{"location":"01-tensors/10_gradient_accumulation/#gradient-accumulation","title":"Gradient Accumulation\u00b6","text":""},{"location":"02-torch-nn/01_nn_module/","title":"nn.Module","text":""},{"location":"02-torch-nn/01_nn_module/#nnmodule","title":"nn.Module\u00b6","text":""},{"location":"02-torch-nn/02_compose_modules/","title":"Compose Modules","text":""},{"location":"02-torch-nn/02_compose_modules/#compose-modules","title":"Compose Modules\u00b6","text":""},{"location":"02-torch-nn/03_saving_weights/","title":"Saving Weights","text":""},{"location":"02-torch-nn/03_saving_weights/#saving-weights","title":"Saving Weights\u00b6","text":""},{"location":"02-torch-nn/04_linear_layer/","title":"Linear Layer","text":""},{"location":"02-torch-nn/04_linear_layer/#linear-layer","title":"Linear Layer\u00b6","text":""},{"location":"02-torch-nn/05_activations/","title":"Activations","text":""},{"location":"02-torch-nn/05_activations/#activations","title":"Activations\u00b6","text":""},{"location":"02-torch-nn/06_dropout/","title":"Dropout","text":""},{"location":"02-torch-nn/06_dropout/#dropout","title":"Dropout\u00b6","text":""},{"location":"02-torch-nn/07_embedding_layers/","title":"Embedding Layers","text":""},{"location":"02-torch-nn/07_embedding_layers/#embedding-layers","title":"Embedding Layers\u00b6","text":""},{"location":"02-torch-nn/08_positional_encoding/","title":"Positional Embeddings","text":""},{"location":"02-torch-nn/08_positional_encoding/#positional-embeddings","title":"Positional Embeddings\u00b6","text":"<p>How to encode the token position in the sequence?</p> <p>References:</p> <ul> <li>Mastering LLAMA: Understanding Rotary Positional Embedding (RPE)</li> </ul>"},{"location":"02-torch-nn/09_normalization_layers/","title":"Normalization Layers","text":""},{"location":"02-torch-nn/09_normalization_layers/#normalization-layers","title":"Normalization Layers\u00b6","text":""},{"location":"02-torch-nn/10_rms_norm/","title":"RMS Norm","text":""},{"location":"02-torch-nn/10_rms_norm/#rms-norm","title":"RMS Norm\u00b6","text":""},{"location":"02-torch-nn/11_training_evaluation_mode/","title":"Training Evaluation Mode","text":""},{"location":"02-torch-nn/11_training_evaluation_mode/#training-evaluation-mode","title":"Training Evaluation Mode\u00b6","text":""},{"location":"02-torch-nn/12_loss_functions/","title":"Loss Functions","text":""},{"location":"02-torch-nn/12_loss_functions/#loss-functions","title":"Loss Functions\u00b6","text":""},{"location":"02-torch-nn/13_prepare_inputs_targets/","title":"Prepare Inputs Targets","text":""},{"location":"02-torch-nn/13_prepare_inputs_targets/#prepare-inputs-targets","title":"Prepare Inputs Targets\u00b6","text":""},{"location":"02-torch-nn/14_interpreting_reduction_modes/","title":"Interpreting Reduction Modes","text":""},{"location":"02-torch-nn/14_interpreting_reduction_modes/#interpreting-reduction-modes","title":"Interpreting Reduction Modes\u00b6","text":""},{"location":"03-training-nn/01_training_loop/","title":"Training Loop","text":""},{"location":"03-training-nn/01_training_loop/#training-loop","title":"Training Loop\u00b6","text":""},{"location":"03-training-nn/02_optimizers_schedulers/","title":"Optimizers Schedulers","text":""},{"location":"03-training-nn/02_optimizers_schedulers/#optimizers-schedulers","title":"Optimizers Schedulers\u00b6","text":""},{"location":"03-training-nn/03_datasets_dataloaders/","title":"Datasets DataLoaders","text":""},{"location":"03-training-nn/03_datasets_dataloaders/#datasets-dataloaders","title":"Datasets DataLoaders\u00b6","text":""},{"location":"03-training-nn/04_gpu_acceleration/","title":"GPU Acceleration","text":""},{"location":"03-training-nn/04_gpu_acceleration/#gpu-acceleration","title":"GPU Acceleration\u00b6","text":""},{"location":"03-training-nn/05_weight_initialization/","title":"Weight Initialization","text":""},{"location":"03-training-nn/05_weight_initialization/#weight-initialization","title":"Weight Initialization\u00b6","text":""},{"location":"04-transformers/01_positional_embeddings/","title":"Positional Embeddings","text":""},{"location":"04-transformers/01_positional_embeddings/#positional-embeddings","title":"Positional Embeddings\u00b6","text":""},{"location":"04-transformers/02_attention_mechanism/","title":"Attention Mechanism","text":""},{"location":"04-transformers/02_attention_mechanism/#attention-mechanism","title":"Attention Mechanism\u00b6","text":""},{"location":"04-transformers/03_multi_head_attention/","title":"Multi-Head Attention","text":""},{"location":"04-transformers/03_multi_head_attention/#multi-head-attention","title":"Multi-Head Attention\u00b6","text":""},{"location":"04-transformers/04_other_attention_implementations/","title":"Other Attention Implementations","text":""},{"location":"04-transformers/04_other_attention_implementations/#other-attention-implementations","title":"Other Attention Implementations\u00b6","text":""},{"location":"04-transformers/05_transformer_encoder/","title":"Transformer Encoder","text":""},{"location":"04-transformers/05_transformer_encoder/#transformer-encoder","title":"Transformer Encoder\u00b6","text":""},{"location":"05-advanced-pytorch/01_hooks/","title":"Hooks","text":""},{"location":"05-advanced-pytorch/01_hooks/#hooks","title":"Hooks\u00b6","text":""},{"location":"05-advanced-pytorch/02_distributed_training_concepts/","title":"Distributed Training Concepts","text":""},{"location":"05-advanced-pytorch/02_distributed_training_concepts/#distributed-training-concepts","title":"Distributed Training Concepts\u00b6","text":""},{"location":"05-advanced-pytorch/03_model_optimization_concepts/","title":"Model Optimization Concepts","text":""},{"location":"05-advanced-pytorch/03_model_optimization_concepts/#model-optimization-concepts","title":"Model Optimization Concepts\u00b6","text":""},{"location":"05-advanced-pytorch/04_torchscript_jit/","title":"Torchscript JIT","text":""},{"location":"05-advanced-pytorch/04_torchscript_jit/#torchscript-jit","title":"Torchscript JIT\u00b6","text":""},{"location":"05-advanced-pytorch/05_profiling/","title":"Profiling","text":""},{"location":"05-advanced-pytorch/05_profiling/#profiling","title":"Profiling\u00b6","text":""},{"location":"06-huggingface-transformers/01_huggingface_transformers/","title":"HuggingFace Transformers","text":""},{"location":"06-huggingface-transformers/01_huggingface_transformers/#huggingface-transformers","title":"HuggingFace Transformers\u00b6","text":""},{"location":"06-huggingface-transformers/02_fine_tuning_transformers/","title":"Fine Tuning Transformers","text":""},{"location":"06-huggingface-transformers/02_fine_tuning_transformers/#fine-tuning-transformers","title":"Fine Tuning Transformers\u00b6","text":""}]}