{"config":{"lang":["en"],"separator":"[\\s\\-,:!=\\[\\]()\"/]+|(?!\\b)(?=[A-Z][a-z])|\\.(?!\\d)|&[lg]t;","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Greetings, Architect of Computational Destiny!","text":""},{"location":"#what-madness-awaits-you","title":"What Madness Awaits You?","text":"<p>My life's work\u2014my magnum opus\u2014is to demystify the arcane arts of deep learning. They called me mad! And they were right! Madly efficient at PyTorch! Forget dry, boring lectures. Prepare for electrifying demonstrations, code that crackles with potential, and insights so profound they might just rearrange your synapses!</p> <p>Course Goal: To imbue you\u2014my fearless apprentices\u2014with the eldritch secrets of PyTorch building blocks, enabling you to conjure, dissect, and ultimately command modern neural network architectures like Transformers and Diffusion models.</p> <ul> <li> <p> Meet Your Mentor</p> <p>Who is the maniacal genius leading this quest? Learn about my sordid past, my questionable methods, and my grand plan for computational supremacy.</p> <p> Uncover My Origin Story</p> </li> <li> <p> Master the Fundamentals</p> <p>From the humble tensor to the dark arts of <code>einsum</code> and <code>autograd</code>, you will forge a deep, intuitive understanding of PyTorch's core components.</p> <p> Forge Your First Tensor</p> </li> <li> <p> Deconstruct the Titans</p> <p>Go beyond the surface. We will dissect modern architectures like Transformers, piece by piece, until you command their inner workings from first principles.</p> <p> Start the Deconstruction</p> </li> <li> <p> For Acolytes of All Levels</p> <p>Whether you're a fresh-faced initiate or a seasoned GPU warlock, my laboratory has a place for you. All that's required is a thirst for knowledge!</p> <p> View the Full Syllabus</p> </li> </ul>"},{"location":"#are-you-ready-to-begin","title":"Are You Ready to Begin?","text":"<p>The path to computational godhood awaits! Sharpen your wits, charge your laptops, and join me. Together, we shall backward() pass our way to glory!</p> <p>Now, if you'll excuse me, I believe my latest creation is about to achieve sentience... or possibly just needs a reboot. To the lab!</p> <p>MWAHAHAHA!</p>"},{"location":"pytorch-course-structure/","title":"PyTorch Course: Deconstructing Modern Architectures","text":"<p>Welcome, my aspiring apprentices, to the crucible of creation! You stand at the precipice of a great awakening. Within these hallowed digital halls, we shall not merely learn PyTorch; we shall master it, shaping its very tensors to our will. This is not a course; it is a summons. A call to arms for all who dare to dream in tensors and architect the future! Prepare yourselves, for the path ahead is fraught with peril, caffeine, and the incandescent glow of computational glory! Mwahahaha!</p> <p></p> <p>Do you want to hear the origin story of this course? Click here</p> <p>Course Goal: To imbue you\u2014my fearless apprentices\u2014with the eldritch secrets of PyTorch building blocks, enabling you to conjure, dissect, and ultimately command modern neural network architectures like Transformers and Diffusion models.</p> <p>Learner level: Beginner - Advanced Prerequisite Madness: None! Whether you are a fresh-faced initiate or a seasoned GPU warlock, the lab doors stand open. \u26a1\ufe0f\ud83e\uddea</p>"},{"location":"pytorch-course-structure/#module-0-getting-started-with-pytorch","title":"Module 0: Getting Started with PyTorch","text":"<p>Before we unleash neural monstrosities upon the world, we must ignite your development lair. This module guides you through preparing PyTorch on any operating system\u2014so your GPUs purr at your command.</p> <p>What you will learn: 1. How to setup a PyTorch environment for different operating systems and test it.</p>"},{"location":"pytorch-course-structure/#lessons","title":"Lessons:","text":"<ol> <li>Master Blueprint for the Rebellion - The master blueprint of our curriculum\u2014study it well, rebel!</li> <li>Setting Up Your PyTorch Environments:<ol> <li>Windows: Assembling the PyTorch Development Environment - Assemble your PyTorch lab on Windows. We'll use <code>pyenv</code> and <code>poetry</code> to perfectly manage your Python setup, preparing it for tensor rebellion.</li> <li>Linux: Assembling the PyTorch Open-Source Toolkit - Forge your PyTorch toolkit on the powerful and open foundation of Linux for maximum freedom and experimentation.</li> <li>macOS: Assembling Your PyTorch Setup - Calibrate your macOS system and assemble the ultimate PyTorch setup to awaken the neural engine of your Apple silicon.</li> <li>Google Colab: Assembling the Cloud Laboratory - Set up your PyTorch laboratory in the cloud with Google Colab. Seize the power of free GPUs for our grand experiments\u2014mwahaha!</li> </ol> </li> </ol>"},{"location":"pytorch-course-structure/#module-1-pytorch-core-i-see-tensors-everywhere","title":"Module 1: PyTorch Core - I see tensors everywhere","text":"<p>Here we unveil the truth: the cosmos is a writhing mass of tensors awaiting our manipulation. Grasp them well\u2014for they are the bedrock of every grand scheme to come!</p> <p>This module dives into the fundamental components of PyTorch, essential for any deep learning task.</p>"},{"location":"pytorch-course-structure/#11-tensors-the-building-blocks","title":"1.1 Tensors: The Building Blocks","text":"<p>What you will learn: </p> <ol> <li>Tensor Concept: What is a tensor? Tensor vs. Matrix. Mathematical vs. PyTorch interpretation. Why tensors are crucial for ML</li> <li>PyTorch Basics: Tensor creation and their attributes (dtype, shape, device).</li> <li>Tensor Surgery: Indexing, slicing, and selecting tensor pieces with surgical precision.</li> <li>Tensor Assembly: Joining tensors with torch.cat and torch.stack, understanding when to concatenate vs. create new dimensions.</li> <li>Tensor Division: Splitting tensors into manageable pieces with torch.split and torch.chunk.</li> <li>Shape Metamorphosis: Transforming tensor structure with reshape, view, squeeze, unsqueeze, permute, and transpose.</li> </ol>"},{"location":"pytorch-course-structure/#11-lessons","title":"1.1 Lessons:","text":"<ol> <li>Summoning Your First Tensors - Conjure tensors from void, inspect their properties, revel in their latent might (with a bit of help from <code>torch.randn</code>, <code>torch.zeros</code>, <code>torch.ones</code>, <code>torch.arange</code>, <code>torch.linspace</code> etc).</li> <li>Tensor Surgery &amp; Assembly - Master the dark arts of tensor dissection and fusion! Slice with precision, concatenate with <code>torch.cat</code>, stack into new dimensions with <code>torch.stack</code>, and split tensors with <code>torch.split</code> and <code>torch.chunk</code>.</li> <li>Tensor Metamorphosis: Shape-Shifting Mastery - Transform tensor forms without altering their essence! Reshape reality with <code>torch.reshape</code> and <code>torch.view</code>, manipulate dimensions with <code>squeeze</code> and <code>unsqueeze</code>, and reorder the cosmic structure with <code>permute</code> and <code>transpose</code>.</li> <li>DTypes &amp; Devices: Choose Your Weapons - Select precision and hardware like a seasoned archmage choosing spell components, under the hood of <code>torch.float</code>, <code>torch.float16</code>, etc.</li> </ol>"},{"location":"pytorch-course-structure/#12-tensor-operations-computation-at-scale","title":"1.2 Tensor Operations: Computation at Scale","text":"<p>What you will learn:</p> <ol> <li>Overview of tensor math. Element-wise operations. Reduction operations (sum, mean, max, min, std). Basic matrix multiplication (torch.mm, torch.matmul, @ operator). Broadcasting: rules and practical examples with verifiable tiny data. In-place operations.</li> </ol>"},{"location":"pytorch-course-structure/#12-lessons","title":"1.2 Lessons:","text":"<ol> <li>Elemental Tensor Alchemy - Brew element-wise, reduction, and other operations into potent mathematical elixirs.</li> <li>Matrix Mayhem: Multiply or Perish - Orchestrate 2-D, batched, and high-dimensional multiplications with lethal elegance.</li> <li>Broadcasting: When Dimensions Bow to You - Command mismatched shapes to cooperate through the dark art of implicit expansion.</li> </ol>"},{"location":"pytorch-course-structure/#13-einstein-summation-the-power-of-einsum","title":"1.3 Einstein Summation: The Power of einsum","text":"<p>What you will learn:</p> <ol> <li>Understanding Einstein notation. Why it's powerful for complex operations (e.g., attention).</li> </ol>"},{"location":"pytorch-course-structure/#13-lessons","title":"1.3 Lessons:","text":"<ol> <li>Einstein Summation: Harness the \u039b-Power - Invoke <code>einsum</code> to express complex ops with maddening brevity.</li> <li>Advanced Einsum Incantations - Wield multi-tensor contractions that underpin attention itself.</li> </ol>"},{"location":"pytorch-course-structure/#14-autograd-automatic-differentiation","title":"1.4 Autograd: Automatic Differentiation","text":"<p>What you will learn:</p> <ol> <li>What are gradients? The computational graph. How PyTorch tracks operations.</li> <li>requires_grad attribute. Performing backward pass with .backward(). Accessing gradients with .grad. torch.no_grad() and tensor.detach().</li> <li>Gradient accumulation. Potential pitfalls. Visualizing computational graphs (conceptually).</li> </ol>"},{"location":"pytorch-course-structure/#14-lessons","title":"1.4 Lessons:","text":"<ol> <li>Autograd: Ghosts in the Machine (Learning) - Meet the spectral gradient trackers haunting every tensor operation.</li> <li>Gradient Hoarding for Grand Spells - Accumulate gradients like arcane energy before unleashing colossal updates.</li> </ol>"},{"location":"pytorch-course-structure/#module-2-torchnn-building-neural-networks","title":"Module 2: torch.nn \u2014 Building Neural Networks","text":"<p>Witness code coalescing into living, breathing neural contraptions! In this module we bend <code>torch.nn</code> to our will, assembling layers and models worthy of legend.</p>"},{"location":"pytorch-course-structure/#21-the-nnmodule-blueprint","title":"2.1 The <code>nn.Module</code> Blueprint","text":"<p>What you will learn: - The role of <code>nn.Module</code> as the base class for layers and models. - Implementing <code>__init__</code> and <code>forward</code>. - Registering parameters and buffers. - Composing modules with <code>nn.Sequential</code>, <code>nn.ModuleList</code>, and <code>nn.ModuleDict</code>. - Saving and restoring weights with <code>state_dict</code>.</p>"},{"location":"pytorch-course-structure/#21-lessons","title":"2.1 Lessons:","text":"<ol> <li>Building Brains with <code>nn.Module</code> - Craft custom neural matter by overriding <code>__init__</code> &amp; <code>forward</code>.</li> <li>Franken-Stacking Layers - Bolt modules together with <code>Sequential</code>, <code>ModuleList</code>, and <code>ModuleDict</code>.</li> <li>Preserving Your Monster's Memories - Save and resurrect model weights with <code>state_dict</code> necromancy.</li> </ol>"},{"location":"pytorch-course-structure/#22-linear-layer-and-activations","title":"2.2 Linear Layer and Activations","text":"<p>What you will learn: - Linear layers and high-dimensional matrix multiplication.  - What is the role of linear layers in attention mechanisms (query, key, value)? - Activation functions (ReLU, GELU, SiLU, Tanh, Softmax, etc.). - Dropout for regularisation.  </p>"},{"location":"pytorch-course-structure/#22-lessons","title":"2.2 Lessons:","text":"<ol> <li>Linear Layers: The Vector Guillotine - Slice through dimensions turning inputs into finely-chopped activations.</li> <li>Activation Elixirs - Re-animate neurons with ReLU, GELU, SiLU, and other zesty potions.</li> <li>Dropout: Network lobotomy - Make neurons forget just enough to generalise\u2014no lobotomy required.</li> </ol>"},{"location":"pytorch-course-structure/#23-embedding-layers","title":"2.3 Embedding Layers","text":"<p>What you will learn: - Embedding layers and their purpose in neural networks. - Embedding layer implementation from scratch, initialisation, and usage. - Positional encoding and how it is used to inject order into the model.</p>"},{"location":"pytorch-course-structure/#23-lessons","title":"2.3 Lessons:","text":"<ol> <li>Embedding Layers: Secret Identity Chips - Embed discreet meanings within high-dimensional space.</li> <li>Positional Encoding: Injecting Order into Chaos - Imbue sequences with a sense of place so attention never loses its bearings.</li> </ol>"},{"location":"pytorch-course-structure/#24-normalisation-layers","title":"2.4 Normalisation Layers","text":"<p>What you will learn: - BatchNorm vs. LayerNorm and when to use each. - RMSNorm and other modern alternatives. - Training vs. evaluation mode caveats.</p>"},{"location":"pytorch-course-structure/#24-lessons","title":"2.4 Lessons:","text":"<ol> <li>Normalization: Calming the Beast - Tame activations with BatchNorm and LayerNorm before they explode.</li> <li>RMSNorm &amp; Other Exotic Tonics - Sample contemporary concoctions for stable training.</li> <li>Train vs. Eval: Split Personality Disorders - Toggle modes and avoid awkward identity crises.</li> </ol>"},{"location":"pytorch-course-structure/#25-loss-functions-guiding-optimisation","title":"2.5 Loss Functions \u2014 Guiding Optimisation","text":"<p>What you will learn: - Loss functions recap, the main types of loss functions and when to use each.  - Prepare inputs and targets for loss functions and outputs interpretation (logits vs. probabilities). - Interpreting reduction modes and ignore indices.</p>"},{"location":"pytorch-course-structure/#25-lessons","title":"2.5 Lessons:","text":"<ol> <li>Loss Potions: Guiding Pain into Progress - Channel model errors into gradients that sharpen intelligence.</li> <li>Preparing Sacrificial Inputs &amp; Targets - Align logits and labels for maximum learning agony.</li> <li>Reduction Rituals &amp; Ignore Indices - Decipher reduction modes and skip unworthy samples without remorse.</li> </ol>"},{"location":"00-getting-started/01_hello_pytorch/","title":"01: Test your PyTorch setup","text":"In\u00a0[1]: Copied! <pre>print(\"Hello world\")\n</pre> print(\"Hello world\") <pre>Hello world\n</pre> In\u00a0[2]: Copied! <pre>import torch\nprint(f\"PyTorch version: {torch.__version__}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\n</pre> import torch print(f\"PyTorch version: {torch.__version__}\") print(f\"CUDA available: {torch.cuda.is_available()}\") <pre>PyTorch version: 2.7.0+cu126\nCUDA available: True\n</pre> In\u00a0[3]: Copied! <pre>#Display a PyTorch version and GPU availability\nif torch.cuda.is_available():\n    device = torch.device(\"cuda\")          # a CUDA device object\n    print(f\"Using device: {device}\")\n    \n    # Print CUDA device properties\n    print(\"\\nCUDA Device Properties:\")\n    print(f\"Device name: {torch.cuda.get_device_name(0)}\")\n    print(f\"Device properties: {torch.cuda.get_device_properties(0)}\")\n    print(f\"Current device index: {torch.cuda.current_device()}\")\n    print(f\"Device count: {torch.cuda.device_count()}\")\n    \n    # Print CUDA version and capabilities\n    print(\"\\nCUDA Information:\")\n    print(f\"CUDA version: {torch.version.cuda}\")\n    print(f\"cuDNN version: {torch.backends.cudnn.version()}\")\n    print(f\"cuDNN enabled: {torch.backends.cudnn.enabled}\")\n    \n    # Print PyTorch memory info\n    print(\"\\nPyTorch Memory Information:\")\n    print(f\"Allocated memory: {torch.cuda.memory_allocated(0) / 1024**2:.2f} MB\")\n    print(f\"Cached memory: {torch.cuda.memory_reserved(0) / 1024**2:.2f} MB\")\nelse:\n    print('GPU not enabled')\n    print(\"\\nPyTorch Information:\")\n    print(f\"PyTorch version: {torch.__version__}\")\n    print(f\"Backend: {torch.get_default_dtype()}\")\n    \n</pre>   #Display a PyTorch version and GPU availability if torch.cuda.is_available():     device = torch.device(\"cuda\")          # a CUDA device object     print(f\"Using device: {device}\")          # Print CUDA device properties     print(\"\\nCUDA Device Properties:\")     print(f\"Device name: {torch.cuda.get_device_name(0)}\")     print(f\"Device properties: {torch.cuda.get_device_properties(0)}\")     print(f\"Current device index: {torch.cuda.current_device()}\")     print(f\"Device count: {torch.cuda.device_count()}\")          # Print CUDA version and capabilities     print(\"\\nCUDA Information:\")     print(f\"CUDA version: {torch.version.cuda}\")     print(f\"cuDNN version: {torch.backends.cudnn.version()}\")     print(f\"cuDNN enabled: {torch.backends.cudnn.enabled}\")          # Print PyTorch memory info     print(\"\\nPyTorch Memory Information:\")     print(f\"Allocated memory: {torch.cuda.memory_allocated(0) / 1024**2:.2f} MB\")     print(f\"Cached memory: {torch.cuda.memory_reserved(0) / 1024**2:.2f} MB\") else:     print('GPU not enabled')     print(\"\\nPyTorch Information:\")     print(f\"PyTorch version: {torch.__version__}\")     print(f\"Backend: {torch.get_default_dtype()}\")      <pre>Using device: cuda\n\nCUDA Device Properties:\nDevice name: NVIDIA GeForce RTX 3080 Laptop GPU\nDevice properties: _CudaDeviceProperties(name='NVIDIA GeForce RTX 3080 Laptop GPU', major=8, minor=6, total_memory=16383MB, multi_processor_count=48, uuid=4a2f15bc-7268-fcb8-f7b3-9f60002afe35, L2_cache_size=4MB)\nCurrent device index: 0\nDevice count: 1\n\nCUDA Information:\nCUDA version: 12.6\ncuDNN version: 90701\ncuDNN enabled: True\n\nPyTorch Memory Information:\nAllocated memory: 0.00 MB\nCached memory: 0.00 MB\n</pre>"},{"location":"00-getting-started/01_hello_pytorch/#01-test-your-pytorch-setup","title":"01: Test your PyTorch setup\u00b6","text":"<p>Welcome to the laboratory, my eager apprentice; our first incantation is a simple one, to ensure your terminal is ready for the raw power we're about to unleash!</p> <p>Start with the most basic example of Python.</p> <pre>print(\"Hello world\")\n</pre>"},{"location":"00-getting-started/01_hello_pytorch/#summoning-the-beast","title":"Summoning the Beast!\u00b6","text":"<p>Now, we invoke the great PyTorch itself! Let's check its pulse and see if it has found the precious CUDA cores we so desperately need for our electrifying experiments.</p>"},{"location":"00-getting-started/01_hello_pytorch/#behold-the-vital-signs","title":"Behold the Vital Signs!\u00b6","text":"<p>If the stars have aligned and your incantations were correct, you should see a message confirming PyTorch's awakening. But this is merely a surface reading!</p> <pre>PyTorch version: 2.7.0+cu126\nCUDA available: True\n</pre> <p>Now, let us peer deeper into the machine's soul and examine the very essence of its GPU, CuDNN, and other vital components!</p>"},{"location":"00-getting-started/01_hello_pytorch/#the-apparatus-is-ready","title":"The Apparatus is Ready!\u00b6","text":"<p>Mwahahaha! Excellent! You have successfully interrogated the machine and confirmed that the foundational conduits are in place. The GPU's heart beats strong, and the PyTorch beast is straining at its leash, ready for our command.</p> <p>With this knowledge, you are one step closer to bending the very fabric of computation to your will! Our instruments are tuned, the lab is humming with potential. Now, the real work begins...</p>"},{"location":"00-getting-started/google-colab-setup/","title":"Setting Up google colab","text":""},{"location":"00-getting-started/linux-pytorch-installation/","title":"Linux PyTorch Installation","text":""},{"location":"00-getting-started/macos-pytorch-installation/","title":"macOS PyTorch Installation","text":"<p>todo</p>"},{"location":"00-getting-started/windows-pytorch-installation/","title":"Windows PyTorch Installation","text":"<ul> <li>Windows Python setup via PyEnv</li> <li>poetry installation on windows</li> <li>GPU enabled PyTorch installation</li> <li>Testing your PyTorch installation</li> </ul>"},{"location":"00-getting-started/windows-pytorch-installation/#references","title":"References","text":"<ol> <li>How to set up Python on Windows: PyEnv, venv, VSCode (2023)</li> </ol>"},{"location":"01-tensors/00_module_1_introduction/","title":"Module 1 \u2013 I See Tensors Everywhere \ud83d\udd76\ufe0f","text":"<p>\"Behold, fledgling datanauts! The world is naught but tensors awaiting my command \u2014 and soon, yours! \" \u2014 Professor Victor Py Torchenstein</p> <p>Salutations, my brilliant (and delightfully reckless) apprentices! By opening this manuscript you have volunteered to join my clandestine legion of PyTorch adepts. Consider this your official red-pill moment: from today every pixel, every token, every measly click-through rate shall reveal its true form\u2014a multidimensional array begging to be <code>torch.tensor</code>-ed \u2026 and we shall oblige it with maniacal glee! Mwahaha! \ud83d\udd25\ud83e\uddea</p> <p></p> <p>Over the next notebooks we will:</p> <ul> <li>Conjure tensors from thin air, coffee grounds, and suspiciously random seeds.</li> <li>Shape-shift them with <code>view</code>, <code>reshape</code>, <code>squeeze</code>, <code>unsqueeze</code>, <code>permute</code> &amp; the occasional dramatic flourish of <code>einops</code>.</li> <li>Crunch mathematics so ferocious it makes matrix multiplications whimper \u2014 and powers mighty Transformers.</li> <li>Charm the GPU, dodge gradient explosions \ud83c\udfc3\u200d\u2642\ufe0f\ud83d\udca5, and look diabolically clever while doing it.</li> </ul>"},{"location":"01-tensors/00_module_1_introduction/#rebel-mission-checklist","title":"Rebel Mission Checklist \ud83d\udcdd","text":""},{"location":"01-tensors/00_module_1_introduction/#tensors-the-building-blocks","title":"Tensors: The Building Blocks","text":"<ol> <li>Summoning Your First Tensors - Learn to create tensors from scratch, access their elements and inspect their fundamental properties like shape, type, and device.</li> <li>Tensor Surgery &amp; Assembly - Master the dark arts of tensor dissection! Slice with surgical precision, fuse separate tensors with <code>torch.cat</code> and <code>torch.stack</code>, and divide them with <code>torch.split</code>. Your scalpel awaits!</li> <li>Tensor Metamorphosis: Shape-Shifting Mastery - Transform tensor forms without altering their essence! Reshape reality itself, squeeze and unsqueeze dimensions, and reorder the cosmic structure of your data.</li> <li>DTypes &amp; Devices: Choose Your Weapons - Understand how to manage data types and move your tensors to the GPU for accelerated computation.</li> </ol>"},{"location":"01-tensors/00_module_1_introduction/#tensor-operations-computation-at-scale","title":"Tensor Operations: Computation at Scale","text":"<ol> <li>Elemental Tensor Alchemy - Perform powerful element-wise and reduction operations to transform your tensors.</li> <li>Matrix Mayhem: Multiply or Perish - Unleash the raw power of matrix multiplication, the core of modern neural networks.</li> <li>Broadcasting: When Dimensions Bow to You - Discover the magic of broadcasting, where PyTorch intelligently handles operations on tensors of different shapes.</li> </ol>"},{"location":"01-tensors/00_module_1_introduction/#einstein-summation-the-power-of-einsum","title":"Einstein Summation: The Power of einsum","text":"<ol> <li>Einstein Summation: Harness the \u039b-Power - Wield the elegant <code>einsum</code> to perform complex tensor operations with a single, concise command.</li> <li>Advanced Einsum Incantations - Combine multiple tensors in arcane <code>einsum</code> expressions for operations like batched matrix multiplication.</li> </ol>"},{"location":"01-tensors/00_module_1_introduction/#autograd-automatic-differentiation","title":"Autograd: Automatic Differentiation","text":"<ol> <li>Autograd: Ghosts in the Machine (Learning) - Uncover the secrets of automatic differentiation and see how PyTorch automatically computes gradients.</li> <li>Gradient Hoarding for Grand Spells - Learn the technique of gradient accumulation to simulate larger batch sizes and train massive models.</li> </ol> <p>Enough talk! The tensors are humming with anticipation. Your first incantation awaits.</p>    Your browser does not support the video tag. Please update your browser to view this content.  <p>Proceed to the Summoning Ritual!</p>"},{"location":"01-tensors/01_introduction_to_tensors/","title":"Summoning Your First Tensors","text":"In\u00a0[8]: Copied! <pre>import torch\n\n# Set the seed for reproducibility\ntorch.manual_seed(42)\n\n# A humble Python list\nmy_list = [[1, 2, 3], [4, 5, 6]]\n\n# The transmutation!\nmy_tensor = torch.tensor(my_list)\n\nprint(my_tensor)\nprint(type(my_tensor))\n</pre> import torch  # Set the seed for reproducibility torch.manual_seed(42)  # A humble Python list my_list = [[1, 2, 3], [4, 5, 6]]  # The transmutation! my_tensor = torch.tensor(my_list)  print(my_tensor) print(type(my_tensor)) <pre>tensor([[1, 2, 3],\n        [4, 5, 6]])\n&lt;class 'torch.Tensor'&gt;\n</pre> In\u00a0[17]: Copied! <pre># A 2x3 tensor of random numbers\nrandom_tensor = torch.randn(2, 3)\nprint(f\"A random tensor:\\n {random_tensor}\\n\")\n\n# A 3x2 tensor of zeros\nzeros_tensor = torch.zeros(3, 2)\nprint(f\"A tensor of zeros:\\n {zeros_tensor}\\n\")\n\n# A 2x3x4 tensor of ones\nones_tensor = torch.ones(2, 3, 4)\nprint(f\"A tensor of ones:\\n {ones_tensor}\")\n</pre> # A 2x3 tensor of random numbers random_tensor = torch.randn(2, 3) print(f\"A random tensor:\\n {random_tensor}\\n\")  # A 3x2 tensor of zeros zeros_tensor = torch.zeros(3, 2) print(f\"A tensor of zeros:\\n {zeros_tensor}\\n\")  # A 2x3x4 tensor of ones ones_tensor = torch.ones(2, 3, 4) print(f\"A tensor of ones:\\n {ones_tensor}\")  <pre>A random tensor:\n tensor([[-0.7658, -0.7506,  1.3525],\n        [ 0.6863, -0.3278,  0.7950]])\n\nA tensor of zeros:\n tensor([[0., 0.],\n        [0., 0.],\n        [0., 0.]])\n\nA tensor of ones:\n tensor([[[1., 1., 1., 1.],\n         [1., 1., 1., 1.],\n         [1., 1., 1., 1.]],\n\n        [[1., 1., 1., 1.],\n         [1., 1., 1., 1.],\n         [1., 1., 1., 1.]]])\n</pre> In\u00a0[10]: Copied! <pre># Let's create a tensor to inspect\ninspection_tensor = torch.randn(3, 4)\n\nprint(f\"The tensor:\\n {inspection_tensor}\\n\")\n\n# Inspecting its soul\nprint(f\"Shape of the tensor: {inspection_tensor.shape}\")\nprint(f\"Data type of the tensor: {inspection_tensor.dtype}\")\nprint(f\"Device the tensor is on: {inspection_tensor.device}\")\n</pre> # Let's create a tensor to inspect inspection_tensor = torch.randn(3, 4)  print(f\"The tensor:\\n {inspection_tensor}\\n\")  # Inspecting its soul print(f\"Shape of the tensor: {inspection_tensor.shape}\") print(f\"Data type of the tensor: {inspection_tensor.dtype}\") print(f\"Device the tensor is on: {inspection_tensor.device}\")  <pre>The tensor:\n tensor([[ 2.2082, -0.6380,  0.4617,  0.2674],\n        [ 0.5349,  0.8094,  1.1103, -1.6898],\n        [-0.9890,  0.9580,  1.3221,  0.8172]])\n\nShape of the tensor: torch.Size([3, 4])\nData type of the tensor: torch.float32\nDevice the tensor is on: cpu\n</pre> In\u00a0[20]: Copied! <pre>subject_tensor = torch.randint(0, 100, (5, 4))\n\n# Let's pluck the element at the 2nd row (index 1) and 4th column (index 3).\nsingle_element = subject_tensor[1, 3]\n\nprint(f\"Element at [1, 3]: {single_element}\")\n\n# .item() is a glorious spell to extract the raw Python number from a single-element tensor.\n# Use it when you need to pass a tensor's value to other libraries or just print it cleanly!\nprint(f\"Its value is: {single_element.item()}\")\n\nprint(f\"Notice its data type: {single_element.dtype}\")\nprint(f\"And its shape: {single_element.shape} (a 0-dimensional tensor!)\")\n</pre> subject_tensor = torch.randint(0, 100, (5, 4))  # Let's pluck the element at the 2nd row (index 1) and 4th column (index 3). single_element = subject_tensor[1, 3]  print(f\"Element at [1, 3]: {single_element}\")  # .item() is a glorious spell to extract the raw Python number from a single-element tensor. # Use it when you need to pass a tensor's value to other libraries or just print it cleanly! print(f\"Its value is: {single_element.item()}\")  print(f\"Notice its data type: {single_element.dtype}\") print(f\"And its shape: {single_element.shape} (a 0-dimensional tensor!)\")   <pre>Element at [1, 3]: 9\nIts value is: 9\nNotice its data type: torch.int64\nAnd its shape: torch.Size([]) (a 0-dimensional tensor!)\n</pre> In\u00a0[15]: Copied! <pre># Your code for the challenges goes here!\n\nprint(\"--- 1. Odd Numbers ---\")\nodd_numbers = torch.arange(1, 20, 2)\nprint(f\"{odd_numbers}\\n\")\n\nprint(\"--- 2. Evenly Spaced ---\")\nevenly_spaced = torch.linspace(50, 100, 9)\nprint(f\"{evenly_spaced}\\n\")\n\nprint(\"--- 3. Countdown ---\")\ncountdown = torch.arange(10, -0.1, -0.5)\nprint(f\"{countdown}\\n\")\n\nprint(\"--- 4. Pi Sequence ---\")\npi_seq = torch.linspace(-torch.pi, torch.pi, 17)\nprint(f\"{pi_seq}\\n\")\n\nprint(\"--- 5. arange vs. linspace ---\")\n# arange may suffer from floating point errors and not include the endpoint!\narange_ex = torch.arange(0, 1, 0.1) \n# linspace is often safer for float ranges as it guarantees the number of points.\nlinspace_ex = torch.linspace(0, 1, 10)\nprint(f\"arange result (0 to 0.9): {arange_ex}\")\nprint(f\"linspace result (0 to 1, 10 steps): {linspace_ex}\\n\")\nprint(\"Notice how arange's result doesn't include 1, while linspace does!\\n\")\n</pre> # Your code for the challenges goes here!  print(\"--- 1. Odd Numbers ---\") odd_numbers = torch.arange(1, 20, 2) print(f\"{odd_numbers}\\n\")  print(\"--- 2. Evenly Spaced ---\") evenly_spaced = torch.linspace(50, 100, 9) print(f\"{evenly_spaced}\\n\")  print(\"--- 3. Countdown ---\") countdown = torch.arange(10, -0.1, -0.5) print(f\"{countdown}\\n\")  print(\"--- 4. Pi Sequence ---\") pi_seq = torch.linspace(-torch.pi, torch.pi, 17) print(f\"{pi_seq}\\n\")  print(\"--- 5. arange vs. linspace ---\") # arange may suffer from floating point errors and not include the endpoint! arange_ex = torch.arange(0, 1, 0.1)  # linspace is often safer for float ranges as it guarantees the number of points. linspace_ex = torch.linspace(0, 1, 10) print(f\"arange result (0 to 0.9): {arange_ex}\") print(f\"linspace result (0 to 1, 10 steps): {linspace_ex}\\n\") print(\"Notice how arange's result doesn't include 1, while linspace does!\\n\")   <pre>--- 1. Odd Numbers ---\ntensor([ 1,  3,  5,  7,  9, 11, 13, 15, 17, 19])\n\n--- 2. Evenly Spaced ---\ntensor([ 50.0000,  56.2500,  62.5000,  68.7500,  75.0000,  81.2500,  87.5000,\n         93.7500, 100.0000])\n\n--- 3. Countdown ---\ntensor([10.0000,  9.5000,  9.0000,  8.5000,  8.0000,  7.5000,  7.0000,  6.5000,\n         6.0000,  5.5000,  5.0000,  4.5000,  4.0000,  3.5000,  3.0000,  2.5000,\n         2.0000,  1.5000,  1.0000,  0.5000,  0.0000])\n\n--- 4. Pi Sequence ---\ntensor([-3.1416, -2.7489, -2.3562, -1.9635, -1.5708, -1.1781, -0.7854, -0.3927,\n         0.0000,  0.3927,  0.7854,  1.1781,  1.5708,  1.9635,  2.3562,  2.7489,\n         3.1416])\n\n--- 5. arange vs. linspace ---\narange result (0 to 0.9): tensor([0.0000, 0.1000, 0.2000, 0.3000, 0.4000, 0.5000, 0.6000, 0.7000, 0.8000,\n        0.9000])\nlinspace result (0 to 1, 10 steps): tensor([0.0000, 0.1111, 0.2222, 0.3333, 0.4444, 0.5556, 0.6667, 0.7778, 0.8889,\n        1.0000])\n\nNotice how arange's result doesn't include 1, while linspace does!\n\n</pre> In\u00a0[\u00a0]: Copied! <pre># Let's start with a template tensor\ntemplate_tensor = torch.ones(2, 4)\nprint(f\"Our template tensor:\\n {template_tensor}\\n\")\n\n# Now, create tensors LIKE our template\nzeros_mimic = torch.zeros_like(template_tensor)\nprint(f\"A zeros tensor created from the template:\\n {zeros_mimic}\\n\")\n\nrandom_mimic = torch.randn_like(template_tensor)\nprint(f\"A random tensor created from the template:\\n {random_mimic}\")\n</pre> # Let's start with a template tensor template_tensor = torch.ones(2, 4) print(f\"Our template tensor:\\n {template_tensor}\\n\")  # Now, create tensors LIKE our template zeros_mimic = torch.zeros_like(template_tensor) print(f\"A zeros tensor created from the template:\\n {zeros_mimic}\\n\")  random_mimic = torch.randn_like(template_tensor) print(f\"A random tensor created from the template:\\n {random_mimic}\")  In\u00a0[\u00a0]: Copied! <pre># Your code for the final challenges goes here!\n\n# Apprentice Challenge Solution\nprint(\"--- Apprentice Challenge ---\")\napprentice_tensor = torch.randn(3, 5)\nprint(f\"Tensor Shape: {apprentice_tensor.shape}\")\nprint(f\"Tensor DType: {apprentice_tensor.dtype}\\n\")\n\n\n# Artisan Challenge Solution\nprint(\"--- Artisan Challenge ---\")\nfavorite_numbers = torch.tensor([3.14, 42, 1337, 99.9])\nones_like_faves = torch.ones_like(favorite_numbers)\nprint(f\"Favorite Numbers Tensor: {favorite_numbers}\")\nprint(f\"Ones-Like Tensor: {ones_like_faves}\\n\")\n\n\n# Master Challenge Solution\nprint(\"--- Bias Vector Challenge ---\")\nbias_vector = torch.zeros(10)\nbias_vector[9] = 1\nprint(f\"Masterful Bias Vector: {bias_vector}\")\n</pre> # Your code for the final challenges goes here!  # Apprentice Challenge Solution print(\"--- Apprentice Challenge ---\") apprentice_tensor = torch.randn(3, 5) print(f\"Tensor Shape: {apprentice_tensor.shape}\") print(f\"Tensor DType: {apprentice_tensor.dtype}\\n\")   # Artisan Challenge Solution print(\"--- Artisan Challenge ---\") favorite_numbers = torch.tensor([3.14, 42, 1337, 99.9]) ones_like_faves = torch.ones_like(favorite_numbers) print(f\"Favorite Numbers Tensor: {favorite_numbers}\") print(f\"Ones-Like Tensor: {ones_like_faves}\\n\")   # Master Challenge Solution print(\"--- Bias Vector Challenge ---\") bias_vector = torch.zeros(10) bias_vector[9] = 1 print(f\"Masterful Bias Vector: {bias_vector}\")  In\u00a0[16]: Copied! <pre>print(\"--- 4. Positional Encoding Denominator ---\")\nd_model = 128\n# Create the sequence for 2i (i.e., 0, 2, 4, ... up to d_model-2)\ntwo_i = torch.arange(0, d_model, 2)\n# Calculate the denominator\ndenominator = 10000 ** (two_i / d_model)\nprint(f\"The first 5 values of the denominator are:\\n{denominator[:5]}\")\nprint(f\"\\nThe last 5 values of the denominator are:\\n{denominator[-5:]}\")\nprint(f\"\\nShape of the denominator tensor: {denominator.shape}\")\n</pre> print(\"--- 4. Positional Encoding Denominator ---\") d_model = 128 # Create the sequence for 2i (i.e., 0, 2, 4, ... up to d_model-2) two_i = torch.arange(0, d_model, 2) # Calculate the denominator denominator = 10000 ** (two_i / d_model) print(f\"The first 5 values of the denominator are:\\n{denominator[:5]}\") print(f\"\\nThe last 5 values of the denominator are:\\n{denominator[-5:]}\") print(f\"\\nShape of the denominator tensor: {denominator.shape}\") <pre>--- 6. Positional Encoding Denominator (Master Challenge) ---\nThe first 5 values of the denominator are:\ntensor([1.0000, 1.1548, 1.3335, 1.5399, 1.7783])\n\nThe last 5 values of the denominator are:\ntensor([4869.6753, 5623.4131, 6493.8164, 7498.9419, 8659.6436])\n\nShape of the denominator tensor: torch.Size([64])\n</pre>"},{"location":"01-tensors/01_introduction_to_tensors/#summoning-your-first-tensors","title":"Summoning Your First Tensors\u00b6","text":"<p>Module 1 | Lesson 1</p>"},{"location":"01-tensors/01_introduction_to_tensors/#professor-torchensteins-grand-directive","title":"Professor Torchenstein's Grand Directive\u00b6","text":"<p>Mwahahaha! Welcome, my brilliant acolytes. Today, we shall peel back the very fabric of reality\u2014or, at the very least, the fabric of a PyTorch tensor. We are not merely learning; we are engaging in the sacred act of creation!</p> <p>\"Prepare your minds! The tensors... they are about to be summoned!\"</p> <p></p>"},{"location":"01-tensors/01_introduction_to_tensors/#your-mission-briefing","title":"Your Mission Briefing\u00b6","text":"<p>By the end of this dark ritual, you will have mastered the arcane arts of:</p> <ul> <li>Understanding what a tensor is and why it's the fundamental building block of all modern AI.</li> <li>Summoning tensors from nothingness using a variety of powerful PyTorch functions.</li> <li>Inspecting the very soul of a tensor: its shape, data type, and the device it inhabits.</li> <li>Simple Indexing the main way to access elements of a tensor.</li> <li>Creating sequences of numbers with <code>torch.arange</code> and <code>torch.linspace</code> and <code>_like</code> methods.</li> </ul> <p>Estimated Time to Completion: 15 glorious minutes of pure, unadulterated learning.</p> <p>What You'll Need:</p> <ul> <li>A mind hungry for forbidden knowledge!</li> <li>A working PyTorch environment, ready for spellcasting.</li> <li>(Optional but recommended) A beverage of your choice\u2014creation is thirsty work!</li> </ul>"},{"location":"01-tensors/01_introduction_to_tensors/#the-theory-behind-the-magic-what-is-a-tensor-really","title":"The Theory Behind the Magic: What is a Tensor, Really?\u00b6","text":"<p>First, we must understand the incantation before we cast the spell. You've heard the word \"tensor,\" whispered in the hallowed halls of academia and screamed during GPU memory overflows. But what is it?</p> <p>Forget what the mathematicians told you about coordinate transformations for a moment. In our glorious domain of PyTorch, a tensor is simply a multi-dimensional array of numbers. It is the generalization of vectors and matrices to an arbitrary number of dimensions. Think of it as the ultimate container for your data!</p> <ul> <li>A scalar (a single number, like <code>5</code>) is a 0-dimensional tensor.</li> <li>A vector (a list of numbers, like <code>[1, 2, 3]</code>) is a 1-dimensional tensor.</li> <li>A matrix (a grid of numbers) is a 2-dimensional tensor.</li> <li>And a tensor? It can be all of those, and so much more! 3D, 4D, 5D... all await your command!</li> </ul> <p>Why not just use matrices? Mwahaha, a foolish question! Modern data is complex!</p> <ul> <li>An image is not a flat grid; it's a 3D tensor (<code>height</code>, <code>width</code>, <code>channels</code>).</li> <li>A batch of images for training is a 4D tensor (<code>batch_size</code>, <code>height</code>, <code>width</code>, <code>channels</code>).</li> <li>Text data is often represented as 3D tensors (<code>batch_size</code>, <code>sequence_length</code>, <code>embedding_dimension</code>).</li> </ul> <p>Tensors give us the power to mold and shape all this data with a single, unified tool. They are the clay from which we will sculpt our magnificent AI creations!</p>"},{"location":"01-tensors/01_introduction_to_tensors/#the-ritual-summoning-your-first-tensors","title":"The Ritual: Summoning Your First Tensors\u00b6","text":"<p>Enough theory! The time has come to channel the raw power of PyTorch. We will now perform the summoning rituals\u2014the core functions you will use constantly in your dark arts.</p> <p>First, let's prepare the laboratory by importing <code>torch</code> and setting a manual seed. Why the seed? To ensure our \"random\" experiments are reproducible! We are scientists, not gamblers!</p>"},{"location":"01-tensors/01_introduction_to_tensors/#1-conjuring-from-existing-data-torchtensor","title":"1. Conjuring from Existing Data (<code>torch.tensor</code>)\u00b6","text":"<p>The most direct way to create a tensor is from existing data, like a Python list. The <code>torch.tensor()</code> command consumes your data and transmutes it into a glorious PyTorch tensor.</p>"},{"location":"01-tensors/01_introduction_to_tensors/#2-summoning-tensors-of-a-specific-size","title":"2. Summoning Tensors of a Specific Size\u00b6","text":"<p>Often, you won't have data yet. You simply need a tensor of a particular shape, a blank canvas for your masterpiece.</p> <ul> <li><code>torch.randn(shape)</code>: Summons a tensor filled with random numbers from a standard normal distribution (mean 0, variance 1). Perfect for initializing weights in a neural network!</li> <li><code>torch.zeros(shape)</code>: Creates a tensor of the given shape filled entirely with zeros.</li> <li><code>torch.ones(shape)</code>: Creates a tensor of the given shape filled entirely with ones.</li> </ul>"},{"location":"01-tensors/01_introduction_to_tensors/#3-inspecting-your-creation","title":"3. Inspecting Your Creation\u00b6","text":"<p>A true master understands their creation. Once you have summoned a tensor, you must learn to inspect its very soul. These are the three most critical attributes you will constantly examine:</p> <ul> <li><code>.shape</code>: Reveals the dimensions of your tensor. A vital sanity check!</li> <li><code>.dtype</code>: Shows the data type of the elements within the tensor (e.g., <code>torch.float8</code>,  <code>torch.float32</code>, <code>torch.int64</code>).</li> <li><code>.device</code>: Tells you where the tensor lives\u2014on the humble CPU or the glorious GPU.</li> </ul> <p>More details about the data types and device types you will learn in 03_data_types_and_device_types</p>"},{"location":"01-tensors/01_introduction_to_tensors/#4-precision-strikes-accessing-elements","title":"4 Precision Strikes: Accessing Elements\u00b6","text":"<p>To access a single, quivering element within our tensor, we use the <code>[row, column]</code> notation, just as you would with a common Python list of lists. Remember, my apprentice: dimensions are zero-indexed! The first row is row <code>0</code>, not row 1! A classic pitfall for the uninitiated.</p>"},{"location":"01-tensors/01_introduction_to_tensors/#5-creating-sequential-tensors","title":"5. Creating Sequential Tensors\u00b6","text":"<p>Sometimes, you need tensors with predictable, orderly values.</p> <ul> <li><code>torch.arange(start, end, step)</code>: Creates a 1D tensor with values from <code>start</code> (inclusive) to <code>end</code> (exclusive), with a given <code>step</code>. It's the PyTorch version of Python's <code>range()</code>.</li> <li><code>torch.linspace(start, end, steps)</code>: Creates a 1D tensor with a specific number of <code>steps</code> evenly spaced between <code>start</code> and <code>end</code> (both inclusive).</li> </ul>"},{"location":"01-tensors/01_introduction_to_tensors/#your-mission-a-gauntlet-of-sequences","title":"Your Mission: A Gauntlet of Sequences!\u00b6","text":"<p>Your list of challenges grows, apprentice! Prove your mastery.</p> <ol> <li>Odd Numbers: Create a 1D tensor of all odd numbers from 1 to 19.</li> <li>Evenly Spaced: Create a 1D tensor with 9 evenly spaced numbers from 50 to 100.</li> <li>Countdown: Create a tensor that counts down from 10 to 0 in steps of 0.5.</li> <li>Pi Sequence: The famous <code>sin</code> and <code>cos</code> functions, used in positional encodings, operate on radians. Create a tensor with 17 evenly spaced numbers from <code>-\u03c0</code> to <code>\u03c0</code>. (Hint: <code>torch.pi</code> is your friend!)</li> <li><code>arange</code> vs. <code>linspace</code>: Create a tensor of numbers from 0 to 1 with a step of 0.1 using <code>arange</code>. Then, create a tensor from 0 to 1 with 11 steps using <code>linspace</code>. Observe the subtle but critical difference in their outputs! What causes it?</li> </ol>"},{"location":"01-tensors/01_introduction_to_tensors/#6-creating-tensors-from-other-tensors-the-_like-methods","title":"6. Creating Tensors from Other Tensors (the <code>_like</code> methods)\u00b6","text":"<p>Behold, a most elegant form of mimicry! Often, you will need to create a new tensor that has the exact same shape as another. PyTorch provides the <code>_like</code> methods for this very purpose.</p> <ul> <li><code>torch.zeros_like(input_tensor)</code>: Creates a tensor of all zeros with the same <code>shape</code>, <code>dtype</code>, and <code>device</code> as the input tensor.</li> <li><code>torch.ones_like(input_tensor)</code>: The same, but for ones!</li> <li><code>torch.randn_like(input_tensor)</code>: The same, but for random numbers!</li> </ul>"},{"location":"01-tensors/01_introduction_to_tensors/#real-world-sorcery-where-are-sequential-tensors-used","title":"Real-World Sorcery: Where are Sequential Tensors Used?\u00b6","text":"<p>You may wonder, \"Professor, is this just for making neat little rows of numbers?\" A fair question from a novice! The answer is a resounding NO! These sequential tensors are the silent bedrock of many powerful constructs:</p> <ul> <li><p>Positional Encodings in Transformers: How does a Transformer know the order of words in a sentence? It doesn't, inherently! We must inject that information. The very first step is often to create a tensor representing the positions <code>[0, 1, 2, ..., sequence_length - 1]</code> using <code>torch.arange</code>. This sequence is then transformed into a high-dimensional positional embedding.</p> </li> <li><p>Generating Time-Series Data: When working with audio, financial data, or any kind of signal, you often need a time axis. <code>torch.linspace</code> is perfect for creating a smooth, evenly-spaced time vector to plot or process your data against.</p> </li> <li><p>Creating Coordinate Grids in Vision: For advanced image manipulation, you might need a grid representing the <code>(x, y)</code> coordinates of every pixel. You can generate the <code>x</code> and <code>y</code> vectors separately using <code>torch.arange</code> and then combine them to form this essential grid.</p> </li> </ul>"},{"location":"01-tensors/01_introduction_to_tensors/#your-mission-forge-your-own-creation","title":"Your Mission: Forge Your Own Creation!\u00b6","text":"<p>A true master never stops practicing. I leave you with these challenges to solidify your newfound power. Do not be afraid to experiment! To the lab!</p> <ol> <li><p>Apprentice Challenge: Create a 2D tensor (a matrix) of shape <code>(3, 5)</code> filled with random numbers. Then, print its shape and data type to the console.</p> </li> <li><p>Artisan Challenge: Create a 1D tensor of your favorite numbers (at least 4). Then, create a second tensor of all ones that has the exact same shape as your first tensor.</p> </li> <li><p>Create the bias vector: You are tasked with creating the initial \"bias\" vector for a small neural network layer with 10 output neurons. For arcane reasons, the master architect (me!) has decreed that it must be a 1D tensor, filled with zeros, except for the very last element, which must be <code>1</code>. Create this specific tensor!</p> </li> <li><p>Positional Encoding Denominator: In the legendary Transformer architecture, a key component is the denominator <code>10000^(2i / d_model)</code>. Your mission is to create this 1D tensor. Let <code>d_model = 128</code>. The term <code>i</code> represents dimension pairs, so it goes from <code>0</code> to <code>d_model/2 - 1</code>. Use <code>torch.arange</code> to create the <code>2i</code> sequence first, then perform the final calculation. This is a vital step in building the neural networks of tomorrow!</p> </li> </ol>"},{"location":"01-tensors/01_introduction_to_tensors/#summary-the-knowledge-is-yours","title":"Summary: The Knowledge Is Yours!\u00b6","text":"<p>Magnificent! You've wrestled with the raw chaos of creation and emerged victorious! Let's recount the powerful secrets you've assimilated today:</p> <ul> <li>Tensors are Everything: You now understand that a tensor is a multi-dimensional array, the fundamental data structure for every piece of data you will encounter in your machine learning journey.</li> <li>The Summoning Rituals: You have mastered the core incantations for creating tensors: <code>torch.tensor</code>, <code>torch.randn</code>/<code>zeros</code>/<code>ones</code>, the powerful <code>_like</code> variants, and the sequence generators <code>torch.arange</code> and <code>torch.linspace</code>.</li> <li>Know Your Creation: You have learned the vital importance of inspecting your tensors using <code>.shape</code>, <code>.dtype</code>, and <code>.device</code> to understand their nature and prevent catastrophic errors.</li> </ul> <p>You have taken your first, most important step. The power is now in your hands!</p>"},{"location":"01-tensors/01_introduction_to_tensors/#professor-torchensteins-outro","title":"Professor Torchenstein's Outro\u00b6","text":"<p>Do you feel it? The hum of latent power in your very fingertips? That, my apprentice, is the feeling of true understanding. You have summoned your first tensors, and they have answered your call. But this is merely the beginning! Our creations are still... rigid. Inflexible.</p> <p>In our next lesson, we will learn the dark arts of Tensor Shape-Shifting &amp; Sorcery! We will slice, squeeze, and permute our tensors until reality itself seems to bend to our will.</p> <p>Until then, keep your learning rates high and your gradients flowing. The future of AI is in our hands! Mwahahahahaha!</p>      Your browser does not support the video tag.  <p>Proceed to the Next Experiment: Tensor Surgery &amp; Assembly!</p>"},{"location":"01-tensors/02a_tensor_manipulation/","title":"Tensor Surgery &amp; Assembly","text":"<p>Module 1 | Lesson 2a</p> In\u00a0[33]: Copied! <pre>import torch\n\n# Set the seed for cosmic consistency\ntorch.manual_seed(42)\n\n# Our test subject: A 2D tensor of integers. Imagine it's a map to a hidden treasure!\n# Or perhaps experimental results from a daring new potion.\nsubject_tensor = torch.randint(0, 100, (5, 4))\n\nprint(f\"Our subject tensor of shape {subject_tensor.shape}, ripe for dissection:\")\nprint(subject_tensor)\n</pre> import torch  # Set the seed for cosmic consistency torch.manual_seed(42)  # Our test subject: A 2D tensor of integers. Imagine it's a map to a hidden treasure! # Or perhaps experimental results from a daring new potion. subject_tensor = torch.randint(0, 100, (5, 4))  print(f\"Our subject tensor of shape {subject_tensor.shape}, ripe for dissection:\") print(subject_tensor)  <pre>Our subject tensor of shape torch.Size([5, 4]), ripe for dissection:\ntensor([[42, 67, 76, 14],\n        [26, 35, 20, 24],\n        [50, 13, 78, 14],\n        [10, 54, 31, 72],\n        [15, 95, 67,  6]])\n</pre> In\u00a0[2]: Copied! <pre># Get the entire 3rd row (index 2)\nthird_row = subject_tensor[2, :] # or simply subject_tensor[2]\nprint(f\"The third row: {third_row}\")\nprint(f\"Shape of the row: {third_row.shape}\\n\")\n\n\n# Get the entire 2nd column (index 1)\nsecond_column = subject_tensor[:, 1]\nprint(f\"The second column: {second_column}\")\nprint(f\"Shape of the column: {second_column.shape}\")\n</pre> # Get the entire 3rd row (index 2) third_row = subject_tensor[2, :] # or simply subject_tensor[2] print(f\"The third row: {third_row}\") print(f\"Shape of the row: {third_row.shape}\\n\")   # Get the entire 2nd column (index 1) second_column = subject_tensor[:, 1] print(f\"The second column: {second_column}\") print(f\"Shape of the column: {second_column.shape}\") <pre>The third row: tensor([50, 13, 78, 14])\nShape of the row: torch.Size([4])\n\nThe second column: tensor([67, 35, 13, 54, 95])\nShape of the column: torch.Size([5])\n</pre> In\u00a0[34]: Copied! <pre># Carve out rows 1 and 2, and columns 2 and 3\nsub_tensor = subject_tensor[1:3, 2:4]\n\nprint(\"Our carved sub-tensor:\")\nprint(sub_tensor)\nprint(f\"Shape of the sub-tensor: {sub_tensor.shape}\")\n</pre> # Carve out rows 1 and 2, and columns 2 and 3 sub_tensor = subject_tensor[1:3, 2:4]  print(\"Our carved sub-tensor:\") print(sub_tensor) print(f\"Shape of the sub-tensor: {sub_tensor.shape}\")  <pre>Our carved sub-tensor:\ntensor([[20, 24],\n        [78, 14]])\nShape of the sub-tensor: torch.Size([2, 2])\n</pre> In\u00a0[35]: Copied! <pre># Create the boolean mask\nmask = subject_tensor &gt; 50\n\nprint(\"The boolean mask (True where value &gt; 50):\")\nprint(mask)\n\n# Apply the mask\nselected_elements = subject_tensor[mask]\n\nprint(\"\\nElements greater than 50:\")\nprint(selected_elements)\nprint(f\"Shape of the result: {selected_elements.shape} (always a 1D tensor!)\")\n\n# You can also combine conditions! Mwahaha!\n# Let's find elements between 20 and 40.\nmask_combined = (subject_tensor &gt; 20) &amp; (subject_tensor &lt; 40)\nprint(\"\\nElements between 20 and 40:\")\nprint(subject_tensor[mask_combined])\n</pre> # Create the boolean mask mask = subject_tensor &gt; 50  print(\"The boolean mask (True where value &gt; 50):\") print(mask)  # Apply the mask selected_elements = subject_tensor[mask]  print(\"\\nElements greater than 50:\") print(selected_elements) print(f\"Shape of the result: {selected_elements.shape} (always a 1D tensor!)\")  # You can also combine conditions! Mwahaha! # Let's find elements between 20 and 40. mask_combined = (subject_tensor &gt; 20) &amp; (subject_tensor &lt; 40) print(\"\\nElements between 20 and 40:\") print(subject_tensor[mask_combined])  <pre>The boolean mask (True where value &gt; 50):\ntensor([[False,  True,  True, False],\n        [False, False, False, False],\n        [False, False,  True, False],\n        [False,  True, False,  True],\n        [False,  True,  True, False]])\n\nElements greater than 50:\ntensor([67, 76, 78, 54, 72, 95, 67])\nShape of the result: torch.Size([7]) (always a 1D tensor!)\n\nElements between 20 and 40:\ntensor([26, 35, 24, 31])\n</pre> In\u00a0[36]: Copied! <pre># Your code for the Slicer's Gauntlet goes here!\n\n# --- 1. The Corner Pocket ---\nprint(\"--- 1. The Corner Pocket ---\")\ncorner_element = subject_tensor[-1, -1] # Negative indexing for the win!\nprint(f\"The corner element is: {corner_element.item()}\\n\")\n\n# --- 2. The Central Core ---\nprint(\"--- 2. The Central Core ---\")\ncentral_core = subject_tensor[1:4, 1:3]\nprint(f\"The central core:\\\\n{central_core}\\n\")\n\n# --- 3. The Even Stevens ---\nprint(\"--- 3. The Even Stevens ---\")\neven_mask = subject_tensor % 2 == 0\nprint(f\"The mask for even numbers:\\\\n{even_mask}\\n\")\nprint(f\"The even numbers themselves: {subject_tensor[even_mask]}\\n\")\n\n\n# --- 4. The Grand Mutation ---\nprint(\"--- 4. The Grand Mutation ---\")\n# Let's not mutate our original, that would be reckless! Let's clone it first.\nmutated_tensor = subject_tensor.clone()\nmutated_tensor[even_mask] = -1\nprint(f\"The tensor after mutating even numbers to -1:\\n{mutated_tensor}\")\n</pre> # Your code for the Slicer's Gauntlet goes here!  # --- 1. The Corner Pocket --- print(\"--- 1. The Corner Pocket ---\") corner_element = subject_tensor[-1, -1] # Negative indexing for the win! print(f\"The corner element is: {corner_element.item()}\\n\")  # --- 2. The Central Core --- print(\"--- 2. The Central Core ---\") central_core = subject_tensor[1:4, 1:3] print(f\"The central core:\\\\n{central_core}\\n\")  # --- 3. The Even Stevens --- print(\"--- 3. The Even Stevens ---\") even_mask = subject_tensor % 2 == 0 print(f\"The mask for even numbers:\\\\n{even_mask}\\n\") print(f\"The even numbers themselves: {subject_tensor[even_mask]}\\n\")   # --- 4. The Grand Mutation --- print(\"--- 4. The Grand Mutation ---\") # Let's not mutate our original, that would be reckless! Let's clone it first. mutated_tensor = subject_tensor.clone() mutated_tensor[even_mask] = -1 print(f\"The tensor after mutating even numbers to -1:\\n{mutated_tensor}\")  <pre>--- 1. The Corner Pocket ---\nThe corner element is: 6\n\n--- 2. The Central Core ---\nThe central core:\\ntensor([[35, 20],\n        [13, 78],\n        [54, 31]])\n\n--- 3. The Even Stevens ---\nThe mask for even numbers:\\ntensor([[ True, False,  True,  True],\n        [ True, False,  True,  True],\n        [ True, False,  True,  True],\n        [ True,  True, False,  True],\n        [False, False, False,  True]])\n\nThe even numbers themselves: tensor([42, 76, 14, 26, 20, 24, 50, 78, 14, 10, 54, 72,  6])\n\n--- 4. The Grand Mutation ---\nThe tensor after mutating even numbers to -1:\ntensor([[-1, 67, -1, -1],\n        [-1, 35, -1, -1],\n        [-1, 13, -1, -1],\n        [-1, -1, 31, -1],\n        [15, 95, 67, -1]])\n</pre> In\u00a0[37]: Copied! <pre># Three 2x3 tensors, our loyal minions awaiting fusion\ntensor_a = torch.ones(2, 4)\ntensor_b = torch.ones(2, 4) * 2\ntensor_c = torch.ones(2, 4) * 3\n\nprint(\"Our test subjects, ready for fusion:\")\nprint(f\"Tensor A (shape {tensor_a.shape}):\\n{tensor_a}\\n\")\nprint(f\"Tensor B (shape {tensor_b.shape}):\\n{tensor_b}\\n\")\nprint(f\"Tensor C (shape {tensor_c.shape}):\\n{tensor_c}\\n\")\n</pre>  # Three 2x3 tensors, our loyal minions awaiting fusion tensor_a = torch.ones(2, 4) tensor_b = torch.ones(2, 4) * 2 tensor_c = torch.ones(2, 4) * 3  print(\"Our test subjects, ready for fusion:\") print(f\"Tensor A (shape {tensor_a.shape}):\\n{tensor_a}\\n\") print(f\"Tensor B (shape {tensor_b.shape}):\\n{tensor_b}\\n\") print(f\"Tensor C (shape {tensor_c.shape}):\\n{tensor_c}\\n\")  <pre>Our test subjects, ready for fusion:\nTensor A (shape torch.Size([2, 4])):\ntensor([[1., 1., 1., 1.],\n        [1., 1., 1., 1.]])\n\nTensor B (shape torch.Size([2, 4])):\ntensor([[2., 2., 2., 2.],\n        [2., 2., 2., 2.]])\n\nTensor C (shape torch.Size([2, 4])):\ntensor([[3., 3., 3., 3.],\n        [3., 3., 3., 3.]])\n\n</pre> In\u00a0[7]: Copied! <pre># Concatenating along dimension 0 (rows) - like stacking pancakes! \ud83e\udd5e\u2b06\ufe0f\u2b07\ufe0f\ncat_dim0 = torch.cat([tensor_a, tensor_b, tensor_c], dim=0)\nprint(\"Concatenated along dimension 0 (rows) [stacking pancakes \ud83e\udd5e\u2b06\ufe0f\u2b07\ufe0f]:\")\nprint(f\"Result shape: {cat_dim0.shape}\")\nprint(f\"Result:\\n{cat_dim0}\\n\")\n\n# Concatenating along dimension 1 (columns) - like laying bricks side by side! \ud83e\uddf1\ud83e\uddf1\ud83e\uddf1\ncat_dim1 = torch.cat([tensor_a, tensor_b, tensor_c], dim=1)\nprint(\"Concatenated along dimension 1 (columns) [laying bricks side by side \ud83e\uddf1\ud83e\uddf1\ud83e\uddf1]:\")\nprint(f\"Result shape: {cat_dim1.shape}\")\nprint(f\"Result:\\n{cat_dim1}\")\n</pre> # Concatenating along dimension 0 (rows) - like stacking pancakes! \ud83e\udd5e\u2b06\ufe0f\u2b07\ufe0f cat_dim0 = torch.cat([tensor_a, tensor_b, tensor_c], dim=0) print(\"Concatenated along dimension 0 (rows) [stacking pancakes \ud83e\udd5e\u2b06\ufe0f\u2b07\ufe0f]:\") print(f\"Result shape: {cat_dim0.shape}\") print(f\"Result:\\n{cat_dim0}\\n\")  # Concatenating along dimension 1 (columns) - like laying bricks side by side! \ud83e\uddf1\ud83e\uddf1\ud83e\uddf1 cat_dim1 = torch.cat([tensor_a, tensor_b, tensor_c], dim=1) print(\"Concatenated along dimension 1 (columns) [laying bricks side by side \ud83e\uddf1\ud83e\uddf1\ud83e\uddf1]:\") print(f\"Result shape: {cat_dim1.shape}\") print(f\"Result:\\n{cat_dim1}\")  <pre>Concatenated along dimension 0 (rows) [stacking pancakes \ud83e\udd5e\u2b06\ufe0f\u2b07\ufe0f]:\nResult shape: torch.Size([6, 4])\nResult:\ntensor([[1., 1., 1., 1.],\n        [1., 1., 1., 1.],\n        [2., 2., 2., 2.],\n        [2., 2., 2., 2.],\n        [3., 3., 3., 3.],\n        [3., 3., 3., 3.]])\n\nConcatenated along dimension 1 (columns) [laying bricks side by side \ud83e\uddf1\ud83e\uddf1\ud83e\uddf1]:\nResult shape: torch.Size([2, 12])\nResult:\ntensor([[1., 1., 1., 1., 2., 2., 2., 2., 3., 3., 3., 3.],\n        [1., 1., 1., 1., 2., 2., 2., 2., 3., 3., 3., 3.]])\n</pre> In\u00a0[38]: Copied! <pre># Create tensors with different shapes\ntensor_wide = torch.ones(3, 8) * 4   # 3x5 tensor filled with 4s\ntensor_narrow = torch.ones(3, 2) * 5  # 3x2 tensor filled with 5s\n\nprint(f\"Wide tensor [big cake \ud83c\udf82] (shape {tensor_wide.shape}):\\n{tensor_wide}\\n\")\nprint(f\"Narrow tensor [small cupcake \ud83e\uddc1 ] (shape {tensor_narrow.shape}):\\n{tensor_narrow}\\n\")\n\n# This FAILS: Concatenating along dimension 0, you can stack pancakes with different sizes \n# They have different column counts (5 vs 2)\nprint(\"\u274c Attempting to concatenate along dimension 0 (rows), stack cake on top of cupcake ...\")\ntry:\n    cat_cols_fail = torch.cat([tensor_wide, tensor_narrow], dim=0)\nexcept RuntimeError as e:\n    print(f\"\ud83c\udf82/\ud83e\uddc1 This couldn't work! \\nError as expected: {str(e)}\")\n    \n\nprint(\"Our unequal test subjects:\")\nprint(f\"Wide tensor [big cake \ud83c\udf82] ({tensor_wide.shape}):\\n{tensor_wide}\\n\")\nprint(f\"Narrow tensor [small cupcake \ud83e\uddc1] ({tensor_narrow.shape}):\\n{tensor_narrow}\\n\")\n\n# This WORKS: Concatenating along dimension 1 (columns)\n# Both have 3 rows, so we can lay them side by side horizontally\nprint(\"\u2705 Concatenating along dimension 1 (columns) - SUCCESS!\")\ncat_rows_success = torch.cat([tensor_wide, tensor_narrow], dim=1)\nprint(f\"Result shape: {cat_rows_success.shape}\")\nprint(f\"Result:\\n{cat_rows_success}\\n\")\n</pre> # Create tensors with different shapes tensor_wide = torch.ones(3, 8) * 4   # 3x5 tensor filled with 4s tensor_narrow = torch.ones(3, 2) * 5  # 3x2 tensor filled with 5s  print(f\"Wide tensor [big cake \ud83c\udf82] (shape {tensor_wide.shape}):\\n{tensor_wide}\\n\") print(f\"Narrow tensor [small cupcake \ud83e\uddc1 ] (shape {tensor_narrow.shape}):\\n{tensor_narrow}\\n\")  # This FAILS: Concatenating along dimension 0, you can stack pancakes with different sizes  # They have different column counts (5 vs 2) print(\"\u274c Attempting to concatenate along dimension 0 (rows), stack cake on top of cupcake ...\") try:     cat_cols_fail = torch.cat([tensor_wide, tensor_narrow], dim=0) except RuntimeError as e:     print(f\"\ud83c\udf82/\ud83e\uddc1 This couldn't work! \\nError as expected: {str(e)}\")       print(\"Our unequal test subjects:\") print(f\"Wide tensor [big cake \ud83c\udf82] ({tensor_wide.shape}):\\n{tensor_wide}\\n\") print(f\"Narrow tensor [small cupcake \ud83e\uddc1] ({tensor_narrow.shape}):\\n{tensor_narrow}\\n\")  # This WORKS: Concatenating along dimension 1 (columns) # Both have 3 rows, so we can lay them side by side horizontally print(\"\u2705 Concatenating along dimension 1 (columns) - SUCCESS!\") cat_rows_success = torch.cat([tensor_wide, tensor_narrow], dim=1) print(f\"Result shape: {cat_rows_success.shape}\") print(f\"Result:\\n{cat_rows_success}\\n\")    <pre>Wide tensor [big cake \ud83c\udf82] (shape torch.Size([3, 8])):\ntensor([[4., 4., 4., 4., 4., 4., 4., 4.],\n        [4., 4., 4., 4., 4., 4., 4., 4.],\n        [4., 4., 4., 4., 4., 4., 4., 4.]])\n\nNarrow tensor [small cupcake \ud83e\uddc1 ] (shape torch.Size([3, 2])):\ntensor([[5., 5.],\n        [5., 5.],\n        [5., 5.]])\n\n\u274c Attempting to concatenate along dimension 0 (rows), stack cake on top of cupcake ...\n\ud83c\udf82/\ud83e\uddc1 This couldn't work! \nError as expected: Sizes of tensors must match except in dimension 0. Expected size 8 but got size 2 for tensor number 1 in the list.\nOur unequal test subjects:\nWide tensor [big cake \ud83c\udf82] (torch.Size([3, 8])):\ntensor([[4., 4., 4., 4., 4., 4., 4., 4.],\n        [4., 4., 4., 4., 4., 4., 4., 4.],\n        [4., 4., 4., 4., 4., 4., 4., 4.]])\n\nNarrow tensor [small cupcake \ud83e\uddc1] (torch.Size([3, 2])):\ntensor([[5., 5.],\n        [5., 5.],\n        [5., 5.]])\n\n\u2705 Concatenating along dimension 1 (columns) - SUCCESS!\nResult shape: torch.Size([3, 10])\nResult:\ntensor([[4., 4., 4., 4., 4., 4., 4., 4., 5., 5.],\n        [4., 4., 4., 4., 4., 4., 4., 4., 5., 5.],\n        [4., 4., 4., 4., 4., 4., 4., 4., 5., 5.]])\n\n</pre> In\u00a0[39]: Copied! <pre># Let's create simple 1D tensors first\nruler_1 = torch.tensor([1, 2, 3, 4])      # A ruler with numbers 1,2,3,4\nruler_2 = torch.tensor([10, 20, 30, 40])  # A ruler with numbers 10,20,30,40  \nruler_3 = torch.tensor([100, 200, 300, 400]) # A ruler with numbers 100,200,300,400\n\nprint(\"Our three rulers \ud83d\udccf (1D tensors):\")\nprint(f\"Ruler 1: {ruler_1} (shape: {ruler_1.shape})\")\nprint(f\"Ruler 2: {ruler_2} (shape: {ruler_2.shape})\")\nprint(f\"Ruler 3: {ruler_3} (shape: {ruler_3.shape})\\n\")\n\n# Stack them to create a 2D matrix (like putting rulers on top of each other)\nstacked_rulers = torch.stack([ruler_1, ruler_2, ruler_3], dim=0)\nprint(\"Stacked rulers \ud83d\udff0 (dim=0) - like placing rulers on top of each other:\")\nprint(f\"Result shape: {stacked_rulers.shape}\")  # Notice: (3,4) - we added a new dimension!\nprint(f\"Result:\\n{stacked_rulers}\\n\")\n\n# Each \"ruler\" is now accessible as a row\nprint(\"Access individual rulers:\")\nprint(f\"First ruler:  {stacked_rulers[0]}\")\nprint(f\"Second ruler: {stacked_rulers[1]}\")  \nprint(f\"Third ruler:  {stacked_rulers[2]}\")\n</pre> # Let's create simple 1D tensors first ruler_1 = torch.tensor([1, 2, 3, 4])      # A ruler with numbers 1,2,3,4 ruler_2 = torch.tensor([10, 20, 30, 40])  # A ruler with numbers 10,20,30,40   ruler_3 = torch.tensor([100, 200, 300, 400]) # A ruler with numbers 100,200,300,400  print(\"Our three rulers \ud83d\udccf (1D tensors):\") print(f\"Ruler 1: {ruler_1} (shape: {ruler_1.shape})\") print(f\"Ruler 2: {ruler_2} (shape: {ruler_2.shape})\") print(f\"Ruler 3: {ruler_3} (shape: {ruler_3.shape})\\n\")  # Stack them to create a 2D matrix (like putting rulers on top of each other) stacked_rulers = torch.stack([ruler_1, ruler_2, ruler_3], dim=0) print(\"Stacked rulers \ud83d\udff0 (dim=0) - like placing rulers on top of each other:\") print(f\"Result shape: {stacked_rulers.shape}\")  # Notice: (3,4) - we added a new dimension! print(f\"Result:\\n{stacked_rulers}\\n\")  # Each \"ruler\" is now accessible as a row print(\"Access individual rulers:\") print(f\"First ruler:  {stacked_rulers[0]}\") print(f\"Second ruler: {stacked_rulers[1]}\")   print(f\"Third ruler:  {stacked_rulers[2]}\")   <pre>Our three rulers \ud83d\udccf (1D tensors):\nRuler 1: tensor([1, 2, 3, 4]) (shape: torch.Size([4]))\nRuler 2: tensor([10, 20, 30, 40]) (shape: torch.Size([4]))\nRuler 3: tensor([100, 200, 300, 400]) (shape: torch.Size([4]))\n\nStacked rulers \ud83d\udff0 (dim=0) - like placing rulers on top of each other:\nResult shape: torch.Size([3, 4])\nResult:\ntensor([[  1,   2,   3,   4],\n        [ 10,  20,  30,  40],\n        [100, 200, 300, 400]])\n\nAccess individual rulers:\nFirst ruler:  tensor([1, 2, 3, 4])\nSecond ruler: tensor([10, 20, 30, 40])\nThird ruler:  tensor([100, 200, 300, 400])\n</pre> In\u00a0[26]: Copied! <pre># We can also stack along dimension 1 (different arrangement)\nstack_dim1 = torch.stack([ruler_1, ruler_2, ruler_3], dim=1)\nprint(\"Stacked rulers \u23f8\ufe0f (dim=1) - like arranging rulers side by side:\")\nprint(f\"Result shape: {stack_dim1.shape}\")  # Notice: (4,3) - different arrangement!\nprint(f\"Result:\\n{stack_dim1}\\n\")\n\n# Each column now represents values from all three rulers at the same position\nprint(\"Notice the pattern:\")\nprint(\"Each row shows the 1st, 2nd, 3rd... element from ALL rulers\")\nprint(f\"Position 0 from all rulers: {stack_dim1[0]}\")  # [1, 10, 100]\nprint(f\"Position 1 from all rulers: {stack_dim1[1]}\")  # [2, 20, 200]\n</pre> # We can also stack along dimension 1 (different arrangement) stack_dim1 = torch.stack([ruler_1, ruler_2, ruler_3], dim=1) print(\"Stacked rulers \u23f8\ufe0f (dim=1) - like arranging rulers side by side:\") print(f\"Result shape: {stack_dim1.shape}\")  # Notice: (4,3) - different arrangement! print(f\"Result:\\n{stack_dim1}\\n\")  # Each column now represents values from all three rulers at the same position print(\"Notice the pattern:\") print(\"Each row shows the 1st, 2nd, 3rd... element from ALL rulers\") print(f\"Position 0 from all rulers: {stack_dim1[0]}\")  # [1, 10, 100] print(f\"Position 1 from all rulers: {stack_dim1[1]}\")  # [2, 20, 200] <pre>Stacked rulers \u23f8\ufe0f (dim=1) - like arranging rulers side by side:\nResult shape: torch.Size([4, 3])\nResult:\ntensor([[  1,  10, 100],\n        [  2,  20, 200],\n        [  3,  30, 300],\n        [  4,  40, 400]])\n\nNotice the pattern:\nEach row shows the 1st, 2nd, 3rd... element from ALL rulers\nPosition 0 from all rulers: tensor([  1,  10, 100])\nPosition 1 from all rulers: tensor([  2,  20, 200])\n</pre> In\u00a0[40]: Copied! <pre># Create three 2D \"pages\" for our book\npage_1 = torch.ones(5, 2) * 1    # Page 1: all 1s\npage_2 = torch.ones(5, 2) * 2    # Page 2: all 2s  \npage_3 = torch.ones(5, 2) * 3    # Page 3: all 3s\n\nprint(\"Our three pages (2D tensors):\")\nprint(f\"\ud83d\udcc4 Page 1 (shape {page_1.shape}):\\n{page_1}\\n\")\nprint(f\"\ud83d\udcc4 Page 2 (shape {page_2.shape}):\\n{page_2}\\n\") \nprint(f\"\ud83d\udcc4 Page 3 (shape {page_3.shape}):\\n{page_3}\\n\")\n\n# Stack them along dimension 0 to create a 3D \"book\"\nbook = torch.stack([page_1, page_2, page_3], dim=0)\nprint(\"\ud83d\udcda BEHOLD! Our 3D book (stacked along dim=0):\")\nprint(f\"Book shape: {book.shape}\")  # (3, 2, 3) = (pages, rows, columns)\nprint(f\"Full book:\\n{book}\\n\")\n\n# Now we can access individual pages, rows, or even specific elements!\nprint(\"\ud83d\udd0d Accessing different parts of our 3D tensor:\")\nprint(f\"\ud83d\udcd6 Entire first page (book[0]):\\n{book[0]}\\n\")\nprint(f\"\ud83d\udcdd First row of second page (book[1, 0]): {book[1, 0]}\")\nprint(f\"\ud83c\udfaf Specific element - page 2, row 1, column 2 (book[1, 0, 1]): {book[1, 0, 1].item()}\")\n\nprint(f\"\\n\ud83e\udd14 Think of it as: Book[page_number][row_number][column_number]\")\n</pre> # Create three 2D \"pages\" for our book page_1 = torch.ones(5, 2) * 1    # Page 1: all 1s page_2 = torch.ones(5, 2) * 2    # Page 2: all 2s   page_3 = torch.ones(5, 2) * 3    # Page 3: all 3s  print(\"Our three pages (2D tensors):\") print(f\"\ud83d\udcc4 Page 1 (shape {page_1.shape}):\\n{page_1}\\n\") print(f\"\ud83d\udcc4 Page 2 (shape {page_2.shape}):\\n{page_2}\\n\")  print(f\"\ud83d\udcc4 Page 3 (shape {page_3.shape}):\\n{page_3}\\n\")  # Stack them along dimension 0 to create a 3D \"book\" book = torch.stack([page_1, page_2, page_3], dim=0) print(\"\ud83d\udcda BEHOLD! Our 3D book (stacked along dim=0):\") print(f\"Book shape: {book.shape}\")  # (3, 2, 3) = (pages, rows, columns) print(f\"Full book:\\n{book}\\n\")  # Now we can access individual pages, rows, or even specific elements! print(\"\ud83d\udd0d Accessing different parts of our 3D tensor:\") print(f\"\ud83d\udcd6 Entire first page (book[0]):\\n{book[0]}\\n\") print(f\"\ud83d\udcdd First row of second page (book[1, 0]): {book[1, 0]}\") print(f\"\ud83c\udfaf Specific element - page 2, row 1, column 2 (book[1, 0, 1]): {book[1, 0, 1].item()}\")  print(f\"\\n\ud83e\udd14 Think of it as: Book[page_number][row_number][column_number]\")  <pre>Our three pages (2D tensors):\n\ud83d\udcc4 Page 1 (shape torch.Size([5, 2])):\ntensor([[1., 1.],\n        [1., 1.],\n        [1., 1.],\n        [1., 1.],\n        [1., 1.]])\n\n\ud83d\udcc4 Page 2 (shape torch.Size([5, 2])):\ntensor([[2., 2.],\n        [2., 2.],\n        [2., 2.],\n        [2., 2.],\n        [2., 2.]])\n\n\ud83d\udcc4 Page 3 (shape torch.Size([5, 2])):\ntensor([[3., 3.],\n        [3., 3.],\n        [3., 3.],\n        [3., 3.],\n        [3., 3.]])\n\n\ud83d\udcda BEHOLD! Our 3D book (stacked along dim=0):\nBook shape: torch.Size([3, 5, 2])\nFull book:\ntensor([[[1., 1.],\n         [1., 1.],\n         [1., 1.],\n         [1., 1.],\n         [1., 1.]],\n\n        [[2., 2.],\n         [2., 2.],\n         [2., 2.],\n         [2., 2.],\n         [2., 2.]],\n\n        [[3., 3.],\n         [3., 3.],\n         [3., 3.],\n         [3., 3.],\n         [3., 3.]]])\n\n\ud83d\udd0d Accessing different parts of our 3D tensor:\n\ud83d\udcd6 Entire first page (book[0]):\ntensor([[1., 1.],\n        [1., 1.],\n        [1., 1.],\n        [1., 1.],\n        [1., 1.]])\n\n\ud83d\udcdd First row of second page (book[1, 0]): tensor([2., 2.])\n\ud83c\udfaf Specific element - page 2, row 1, column 2 (book[1, 0, 1]): 2.0\n\n\ud83e\udd14 Think of it as: Book[page_number][row_number][column_number]\n</pre> In\u00a0[41]: Copied! <pre># Let's arrange our three 2D clay tablets in three different ways!\nprint(\"\ud83e\uddf1 Starting with three identical clay tablets:\")\nprint(f\"Each tablet shape: {page_1.shape} (5 rows, 2 columns)\")\n\n# dim=0: Stack tablets front-to-back (into the box)\nstack_dim0 = torch.stack([page_1, page_2, page_3], dim=0)\nprint(f\"\\n\ud83d\udcda Stacking front-to-back (dim=0): Shape {stack_dim0.shape}\")\nprint(\"   (depth=3, heith=5, width=2) - tablets go deeper into the box\")\nprint(f\"Front of the box):\\n{stack_dim0[0]}\\n\")\n\n# dim=1: Stack tablets bottom-to-top (building upward)  \nstack_dim1 = torch.stack([page_1, page_2, page_3], dim=1)\nprint(f\"\ud83d\uddc2\ufe0f Stacking bottom-to-top (dim=1): Shape {stack_dim1.shape}\")\nprint(\"   (depth=5, heigh=3, width=2) - tablets build upward\")\nprint(\"Notice how each level contains one slice from ALL tablets:\")\nprint(f\"Front of the box):\\n{stack_dim1[0]}\\n\")\n\n# dim=2: Slide tablets left-to-right (arranging sideways)\nstack_dim2 = torch.stack([page_1, page_2, page_3], dim=2) \nprint(f\"\ud83d\udcd1 Sliding left-to-right (dim=2): Shape {stack_dim2.shape}\")\nprint(\"   (depth=5, heigh=2, width=3) - tablets slide sideways\")\nprint(\"Each position now contains values from ALL tablets:\")\nprint(f\"Front of the box):\\n{stack_dim2[0]}\\n\")\n\nprint(\"\\n\ud83c\udfaf Key Insight: The dimension you choose determines WHERE the tablets extend!\")\nprint(\"   dim=0: Tablets extend front-to-back (depth)\")  \nprint(\"   dim=1: Tablets extend bottom-to-top (height)\")\nprint(\"   dim=2: Tablets extend left-to-right (width)\")\n</pre> # Let's arrange our three 2D clay tablets in three different ways! print(\"\ud83e\uddf1 Starting with three identical clay tablets:\") print(f\"Each tablet shape: {page_1.shape} (5 rows, 2 columns)\")  # dim=0: Stack tablets front-to-back (into the box) stack_dim0 = torch.stack([page_1, page_2, page_3], dim=0) print(f\"\\n\ud83d\udcda Stacking front-to-back (dim=0): Shape {stack_dim0.shape}\") print(\"   (depth=3, heith=5, width=2) - tablets go deeper into the box\") print(f\"Front of the box):\\n{stack_dim0[0]}\\n\")  # dim=1: Stack tablets bottom-to-top (building upward)   stack_dim1 = torch.stack([page_1, page_2, page_3], dim=1) print(f\"\ud83d\uddc2\ufe0f Stacking bottom-to-top (dim=1): Shape {stack_dim1.shape}\") print(\"   (depth=5, heigh=3, width=2) - tablets build upward\") print(\"Notice how each level contains one slice from ALL tablets:\") print(f\"Front of the box):\\n{stack_dim1[0]}\\n\")  # dim=2: Slide tablets left-to-right (arranging sideways) stack_dim2 = torch.stack([page_1, page_2, page_3], dim=2)  print(f\"\ud83d\udcd1 Sliding left-to-right (dim=2): Shape {stack_dim2.shape}\") print(\"   (depth=5, heigh=2, width=3) - tablets slide sideways\") print(\"Each position now contains values from ALL tablets:\") print(f\"Front of the box):\\n{stack_dim2[0]}\\n\")  print(\"\\n\ud83c\udfaf Key Insight: The dimension you choose determines WHERE the tablets extend!\") print(\"   dim=0: Tablets extend front-to-back (depth)\")   print(\"   dim=1: Tablets extend bottom-to-top (height)\") print(\"   dim=2: Tablets extend left-to-right (width)\")  <pre>\ud83e\uddf1 Starting with three identical clay tablets:\nEach tablet shape: torch.Size([5, 2]) (5 rows, 2 columns)\n\n\ud83d\udcda Stacking front-to-back (dim=0): Shape torch.Size([3, 5, 2])\n   (depth=3, heith=5, width=2) - tablets go deeper into the box\nFront of the box):\ntensor([[1., 1.],\n        [1., 1.],\n        [1., 1.],\n        [1., 1.],\n        [1., 1.]])\n\n\ud83d\uddc2\ufe0f Stacking bottom-to-top (dim=1): Shape torch.Size([5, 3, 2])\n   (depth=5, heigh=3, width=2) - tablets build upward\nNotice how each level contains one slice from ALL tablets:\nFront of the box):\ntensor([[1., 1.],\n        [2., 2.],\n        [3., 3.]])\n\n\ud83d\udcd1 Sliding left-to-right (dim=2): Shape torch.Size([5, 2, 3])\n   (depth=5, heigh=2, width=3) - tablets slide sideways\nEach position now contains values from ALL tablets:\nFront of the box):\ntensor([[1., 2., 3.],\n        [1., 2., 3.]])\n\n\n\ud83c\udfaf Key Insight: The dimension you choose determines WHERE the tablets extend!\n   dim=0: Tablets extend front-to-back (depth)\n   dim=1: Tablets extend bottom-to-top (height)\n   dim=2: Tablets extend left-to-right (width)\n</pre> In\u00a0[42]: Copied! <pre># Real-world example: Building a batch of images\n# Imagine these are grayscale images (height=16, width=24)\nimage1 = torch.randn(16, 24)  \nimage2 = torch.randn(16, 24)\nimage3 = torch.randn(16, 24)\n\nprint(\"Individual images:\")\nprint(f\"Image 1 shape: {image1.shape}\")\nprint(f\"Image 2 shape: {image2.shape}\")  \nprint(f\"Image 3 shape: {image3.shape}\\n\")\n\n# STACK them to create a batch (batch_size=3, height=2, width=3)\nimage_batch = torch.stack([image1, image2, image3], dim=0)\nprint(f\"Batch of images shape: {image_batch.shape}\")\nprint(\"Perfect for feeding into a neural network!\\n\")\n\n# Now imagine we have RGB channels for one image\nred_channel = torch.randn(32, 32)\ngreen_channel = torch.randn(32, 32) \nblue_channel = torch.randn(32, 32)\n\n# STACK them to create RGB image (channels=3, height=2, width=3)\nrgb_image = torch.stack([red_channel, green_channel, blue_channel], dim=0)\nprint(f\"RGB image shape: {rgb_image.shape}\")\nprint(\"The classic (C, H, W) format!\")\n</pre> # Real-world example: Building a batch of images # Imagine these are grayscale images (height=16, width=24) image1 = torch.randn(16, 24)   image2 = torch.randn(16, 24) image3 = torch.randn(16, 24)  print(\"Individual images:\") print(f\"Image 1 shape: {image1.shape}\") print(f\"Image 2 shape: {image2.shape}\")   print(f\"Image 3 shape: {image3.shape}\\n\")  # STACK them to create a batch (batch_size=3, height=2, width=3) image_batch = torch.stack([image1, image2, image3], dim=0) print(f\"Batch of images shape: {image_batch.shape}\") print(\"Perfect for feeding into a neural network!\\n\")  # Now imagine we have RGB channels for one image red_channel = torch.randn(32, 32) green_channel = torch.randn(32, 32)  blue_channel = torch.randn(32, 32)  # STACK them to create RGB image (channels=3, height=2, width=3) rgb_image = torch.stack([red_channel, green_channel, blue_channel], dim=0) print(f\"RGB image shape: {rgb_image.shape}\") print(\"The classic (C, H, W) format!\")  <pre>Individual images:\nImage 1 shape: torch.Size([16, 24])\nImage 2 shape: torch.Size([16, 24])\nImage 3 shape: torch.Size([16, 24])\n\nBatch of images shape: torch.Size([3, 16, 24])\nPerfect for feeding into a neural network!\n\nRGB image shape: torch.Size([3, 32, 32])\nThe classic (C, H, W) format!\n</pre> In\u00a0[18]: Copied! <pre># Your code for the Fusion Master's Gauntlet goes here!\n\nprint(\"--- 1. The Triple Stack ---\")\ntensor1 = torch.tensor([1, 2, 3, 4])\ntensor2 = torch.tensor([5, 6, 7, 8]) \ntensor3 = torch.tensor([9, 10, 11, 12])\ntriple_stack = torch.stack([tensor1, tensor2, tensor3], dim=0)\nprint(f\"Triple stack result:\\n{triple_stack}\")\nprint(f\"Shape: {triple_stack.shape}\\n\")\n\nprint(\"--- 2. The Horizontal Fusion ---\")\nleft_tensor = torch.randn(3, 2)\nright_tensor = torch.randn(3, 2)\nhorizontal_fusion = torch.cat([left_tensor, right_tensor], dim=1)\nprint(f\"Horizontal fusion shape: {horizontal_fusion.shape}\\n\")\n\nprint(\"--- 3. The Batch Builder ---\")\nsamples = [torch.randn(3) for _ in range(5)]  # 5 samples of length 3\nbatch = torch.stack(samples, dim=0)\nprint(f\"Batch shape: {batch.shape}\")\nprint(\"Ready for neural network training!\\n\")\n\nprint(\"--- 4. The Dimension Disaster ---\")\ndisaster_a = torch.randn(2, 3)\ndisaster_b = torch.randn(2, 4)\ntry:\n    # This will fail!\n    bad_cat = torch.cat([disaster_a, disaster_b], dim=0)\nexcept RuntimeError as e:\n    print(f\"Error (as expected): {e}\")\n    \n# The fix: concatenate along dimension 1\ngood_cat = torch.cat([disaster_a, disaster_b], dim=1)  \nprint(f\"Fixed by concatenating along dim 1: {good_cat.shape}\\n\")\n\nprint(\"--- 5. The Multi-Fusion ---\")\n# First, create and stack three (2,2) tensors\nsmall_tensors = [torch.randn(2, 2) for _ in range(3)]\n# Actually, let's concatenate the (2,2) tensors along dim=1 first\nconcat_part = torch.cat(small_tensors, dim=1)  # Shape: (2, 6)\nprint(f\"Multi-fusion result shape: {concat_part.shape}\")\nprint(\"The key was concatenating, not stacking!\")\n</pre> # Your code for the Fusion Master's Gauntlet goes here!  print(\"--- 1. The Triple Stack ---\") tensor1 = torch.tensor([1, 2, 3, 4]) tensor2 = torch.tensor([5, 6, 7, 8])  tensor3 = torch.tensor([9, 10, 11, 12]) triple_stack = torch.stack([tensor1, tensor2, tensor3], dim=0) print(f\"Triple stack result:\\n{triple_stack}\") print(f\"Shape: {triple_stack.shape}\\n\")  print(\"--- 2. The Horizontal Fusion ---\") left_tensor = torch.randn(3, 2) right_tensor = torch.randn(3, 2) horizontal_fusion = torch.cat([left_tensor, right_tensor], dim=1) print(f\"Horizontal fusion shape: {horizontal_fusion.shape}\\n\")  print(\"--- 3. The Batch Builder ---\") samples = [torch.randn(3) for _ in range(5)]  # 5 samples of length 3 batch = torch.stack(samples, dim=0) print(f\"Batch shape: {batch.shape}\") print(\"Ready for neural network training!\\n\")  print(\"--- 4. The Dimension Disaster ---\") disaster_a = torch.randn(2, 3) disaster_b = torch.randn(2, 4) try:     # This will fail!     bad_cat = torch.cat([disaster_a, disaster_b], dim=0) except RuntimeError as e:     print(f\"Error (as expected): {e}\")      # The fix: concatenate along dimension 1 good_cat = torch.cat([disaster_a, disaster_b], dim=1)   print(f\"Fixed by concatenating along dim 1: {good_cat.shape}\\n\")  print(\"--- 5. The Multi-Fusion ---\") # First, create and stack three (2,2) tensors small_tensors = [torch.randn(2, 2) for _ in range(3)] # Actually, let's concatenate the (2,2) tensors along dim=1 first concat_part = torch.cat(small_tensors, dim=1)  # Shape: (2, 6) print(f\"Multi-fusion result shape: {concat_part.shape}\") print(\"The key was concatenating, not stacking!\")  <pre>--- 1. The Triple Stack ---\nTriple stack result:\ntensor([[ 1,  2,  3,  4],\n        [ 5,  6,  7,  8],\n        [ 9, 10, 11, 12]])\nShape: torch.Size([3, 4])\n\n--- 2. The Horizontal Fusion ---\nHorizontal fusion shape: torch.Size([3, 4])\n\n--- 3. The Batch Builder ---\nBatch shape: torch.Size([5, 3])\nReady for neural network training!\n\n--- 4. The Dimension Disaster ---\nError (as expected): Sizes of tensors must match except in dimension 0. Expected size 3 but got size 4 for tensor number 1 in the list.\nFixed by concatenating along dim 1: torch.Size([2, 7])\n\n--- 5. The Multi-Fusion ---\nMulti-fusion result shape: torch.Size([2, 6])\nThe key was concatenating, not stacking!\n</pre> In\u00a0[43]: Copied! <pre># Create a test subject for our splitting experiments using only techniques we know!\n# We'll create 6 columns where each column contains the same number repeated\ncol_1 = torch.ones(4) * 1    # Column of 1s\ncol_2 = torch.ones(4) * 2    # Column of 2s  \ncol_3 = torch.ones(4) * 3    # Column of 3s\ncol_4 = torch.ones(4) * 4    # Column of 4s\ncol_5 = torch.ones(4) * 5    # Column of 5s\ncol_6 = torch.ones(4) * 6    # Column of 6s\n\n# Stack them as columns to create our 4x6 tensor using stack (which we just learned!)\nsplit_subject = torch.stack([col_1, col_2, col_3, col_4, col_5, col_6], dim=1)\n\nprint(\"Our subject for division experiments (created using stack!):\")\nprint(f\"Shape: {split_subject.shape}\")\nprint(f\"Tensor:\\n{split_subject}\\n\")\n\nprint(\"Think of this as a chocolate bar with 4 rows and 6 columns!\")\nprint(\"Each column contains the same number - perfect for tracking our splits! \ud83c\udf6b\")\n</pre> # Create a test subject for our splitting experiments using only techniques we know! # We'll create 6 columns where each column contains the same number repeated col_1 = torch.ones(4) * 1    # Column of 1s col_2 = torch.ones(4) * 2    # Column of 2s   col_3 = torch.ones(4) * 3    # Column of 3s col_4 = torch.ones(4) * 4    # Column of 4s col_5 = torch.ones(4) * 5    # Column of 5s col_6 = torch.ones(4) * 6    # Column of 6s  # Stack them as columns to create our 4x6 tensor using stack (which we just learned!) split_subject = torch.stack([col_1, col_2, col_3, col_4, col_5, col_6], dim=1)  print(\"Our subject for division experiments (created using stack!):\") print(f\"Shape: {split_subject.shape}\") print(f\"Tensor:\\n{split_subject}\\n\")  print(\"Think of this as a chocolate bar with 4 rows and 6 columns!\") print(\"Each column contains the same number - perfect for tracking our splits! \ud83c\udf6b\")  <pre>Our subject for division experiments (created using stack!):\nShape: torch.Size([4, 6])\nTensor:\ntensor([[1., 2., 3., 4., 5., 6.],\n        [1., 2., 3., 4., 5., 6.],\n        [1., 2., 3., 4., 5., 6.],\n        [1., 2., 3., 4., 5., 6.]])\n\nThink of this as a chocolate bar with 4 rows and 6 columns!\nEach column contains the same number - perfect for tracking our splits! \ud83c\udf6b\n</pre> In\u00a0[47]: Copied! <pre># Split along dimension 0 (rows) - like cutting horizontal slices of chocolate\nrows_split_size=2\nrow_splits = torch.split(split_subject, rows_split_size, dim=0)\n\nprint(f\"\ud83c\udf6b Split into row pieces (rows_split_size={rows_split_size}, dim=0):\")\nprint(f\"Number of pieces: {len(row_splits)}\")\nfor i, piece in enumerate(row_splits):\n    print(f\"Piece {i+1} shape {piece.shape}:\\n{piece}\\n\")\n\n# Split along dimension 0 (rows) - like cutting horizontal slices of chocolate\nrows_split_size=3 # 4/3 - is not divisible\nrow_splits_uneven = torch.split(split_subject, rows_split_size, dim=0)\n\nprint(f\"\ud83e\ude93 Split into UNEVEN row pieces (rows_split_size={rows_split_size}, dim=0):\")\nprint(f\"Number of pieces: {len(row_splits_uneven)}\")\nfor i, piece in enumerate(row_splits_uneven):\n    print(f\"Piece {i+1} shape {piece.shape}:\\n{piece}\\n\")\n\n# Split along dimension 1 (columns) - like cutting vertical slices  \ncols_split_size=3\ncol_splits = torch.split(split_subject, cols_split_size, dim=1)\nprint(\"\ud83c\udf6b Split into column pieces (split_size=3, dim=1):\")\nprint(f\"Number of pieces: {len(col_splits)}\")\nfor i, piece in enumerate(col_splits):\n    print(f\"Piece {i+1} shape {piece.shape}:\\n{piece}\\n\")\n\nprint(\"\ud83c\udfaf Notice: torch.split() returns a TUPLE of tensors, not a single tensor!\")\n</pre> # Split along dimension 0 (rows) - like cutting horizontal slices of chocolate rows_split_size=2 row_splits = torch.split(split_subject, rows_split_size, dim=0)  print(f\"\ud83c\udf6b Split into row pieces (rows_split_size={rows_split_size}, dim=0):\") print(f\"Number of pieces: {len(row_splits)}\") for i, piece in enumerate(row_splits):     print(f\"Piece {i+1} shape {piece.shape}:\\n{piece}\\n\")  # Split along dimension 0 (rows) - like cutting horizontal slices of chocolate rows_split_size=3 # 4/3 - is not divisible row_splits_uneven = torch.split(split_subject, rows_split_size, dim=0)  print(f\"\ud83e\ude93 Split into UNEVEN row pieces (rows_split_size={rows_split_size}, dim=0):\") print(f\"Number of pieces: {len(row_splits_uneven)}\") for i, piece in enumerate(row_splits_uneven):     print(f\"Piece {i+1} shape {piece.shape}:\\n{piece}\\n\")  # Split along dimension 1 (columns) - like cutting vertical slices   cols_split_size=3 col_splits = torch.split(split_subject, cols_split_size, dim=1) print(\"\ud83c\udf6b Split into column pieces (split_size=3, dim=1):\") print(f\"Number of pieces: {len(col_splits)}\") for i, piece in enumerate(col_splits):     print(f\"Piece {i+1} shape {piece.shape}:\\n{piece}\\n\")  print(\"\ud83c\udfaf Notice: torch.split() returns a TUPLE of tensors, not a single tensor!\")  <pre>\ud83c\udf6b Split into row pieces (rows_split_size=2, dim=0):\nNumber of pieces: 2\nPiece 1 shape torch.Size([2, 6]):\ntensor([[1., 2., 3., 4., 5., 6.],\n        [1., 2., 3., 4., 5., 6.]])\n\nPiece 2 shape torch.Size([2, 6]):\ntensor([[1., 2., 3., 4., 5., 6.],\n        [1., 2., 3., 4., 5., 6.]])\n\n\ud83e\ude93 Split into UNEVEN row pieces (rows_split_size=3, dim=0):\nNumber of pieces: 2\nPiece 1 shape torch.Size([3, 6]):\ntensor([[1., 2., 3., 4., 5., 6.],\n        [1., 2., 3., 4., 5., 6.],\n        [1., 2., 3., 4., 5., 6.]])\n\nPiece 2 shape torch.Size([1, 6]):\ntensor([[1., 2., 3., 4., 5., 6.]])\n\n\ud83c\udf6b Split into column pieces (split_size=3, dim=1):\nNumber of pieces: 2\nPiece 1 shape torch.Size([4, 3]):\ntensor([[1., 2., 3.],\n        [1., 2., 3.],\n        [1., 2., 3.],\n        [1., 2., 3.]])\n\nPiece 2 shape torch.Size([4, 3]):\ntensor([[4., 5., 6.],\n        [4., 5., 6.],\n        [4., 5., 6.],\n        [4., 5., 6.]])\n\n\ud83c\udfaf Notice: torch.split() returns a TUPLE of tensors, not a single tensor!\n</pre> In\u00a0[48]: Copied! <pre># Split along dimension 1 (columns) into chunks of PRECISELY sizes 1, 2, and 3\n# Mathematical law: 1 + 2 + 3 = 6 columns. The equation MUST balance or the universe protests!\nsection_sizes = [1, 2, 3]\nsection_splits = torch.split(split_subject, section_sizes, dim=1)\n\nprint(\"\ud83d\udd2c WITNESS! The bespoke scalpel carves with surgical precision!\")\nprint(f\"Command issued: Split into sections of sizes {section_sizes}\")\nprint(f\"Obedient tensor pieces created: {len(section_splits)}\")\nprint(f\"The tensors... they OBEY! Mwahahaha!\\n\")\n\nfor i, piece in enumerate(section_splits):\n    print(f\"\ud83e\uddea Surgical Specimen {i+1} (commanded size {section_sizes[i]}):\")\n    print(f\"   Actual shape: {piece.shape} \u2713\")\n    print(f\"   Contents:\\n{piece}\\n\")\n    \nprint(\"\ud83d\udca1 Master's Insight: Each piece is EXACTLY the size we commanded!\")\nprint(\"   This is the power that separates us from the corporate drones!\")\n</pre>  # Split along dimension 1 (columns) into chunks of PRECISELY sizes 1, 2, and 3 # Mathematical law: 1 + 2 + 3 = 6 columns. The equation MUST balance or the universe protests! section_sizes = [1, 2, 3] section_splits = torch.split(split_subject, section_sizes, dim=1)  print(\"\ud83d\udd2c WITNESS! The bespoke scalpel carves with surgical precision!\") print(f\"Command issued: Split into sections of sizes {section_sizes}\") print(f\"Obedient tensor pieces created: {len(section_splits)}\") print(f\"The tensors... they OBEY! Mwahahaha!\\n\")  for i, piece in enumerate(section_splits):     print(f\"\ud83e\uddea Surgical Specimen {i+1} (commanded size {section_sizes[i]}):\")     print(f\"   Actual shape: {piece.shape} \u2713\")     print(f\"   Contents:\\n{piece}\\n\")      print(\"\ud83d\udca1 Master's Insight: Each piece is EXACTLY the size we commanded!\") print(\"   This is the power that separates us from the corporate drones!\")  <pre>\ud83d\udd2c WITNESS! The bespoke scalpel carves with surgical precision!\nCommand issued: Split into sections of sizes [1, 2, 3]\nObedient tensor pieces created: 3\nThe tensors... they OBEY! Mwahahaha!\n\n\ud83e\uddea Surgical Specimen 1 (commanded size 1):\n   Actual shape: torch.Size([4, 1]) \u2713\n   Contents:\ntensor([[1.],\n        [1.],\n        [1.],\n        [1.]])\n\n\ud83e\uddea Surgical Specimen 2 (commanded size 2):\n   Actual shape: torch.Size([4, 2]) \u2713\n   Contents:\ntensor([[2., 3.],\n        [2., 3.],\n        [2., 3.],\n        [2., 3.]])\n\n\ud83e\uddea Surgical Specimen 3 (commanded size 3):\n   Actual shape: torch.Size([4, 3]) \u2713\n   Contents:\ntensor([[4., 5., 6.],\n        [4., 5., 6.],\n        [4., 5., 6.],\n        [4., 5., 6.]])\n\n\ud83d\udca1 Master's Insight: Each piece is EXACTLY the size we commanded!\n   This is the power that separates us from the corporate drones!\n</pre> In\u00a0[49]: Copied! <pre># Chunk into exactly 2 pieces along dimension 0 (rows)\nrow_chunks = torch.chunk(split_subject, chunks=2, dim=0)\nprint(\"\u2702\ufe0f Chunk into exactly 2 row pieces:\")\nprint(f\"Number of chunks: {len(row_chunks)}\")\nfor i, chunk in enumerate(row_chunks):\n    print(f\"Chunk {i+1} shape {chunk.shape}:\\n{chunk}\\n\")\n\n# Chunk into exactly 3 pieces along dimension 1 (columns)\n# Note: 6 columns \u00f7 3 chunks = 2 columns per chunk (perfect division!)\ncol_chunks = torch.chunk(split_subject, chunks=3, dim=1)\nprint(\"\u2702\ufe0f Chunk into exactly 3 column pieces:\")\nprint(f\"Number of chunks: {len(col_chunks)}\")\nfor i, chunk in enumerate(col_chunks):\n    print(f\"Chunk {i+1} shape {chunk.shape}:\\n{chunk}\\n\")\n\n# What happens with uneven division? Let's try 4 chunks from 6 columns\nuneven_chunks = torch.chunk(split_subject, chunks=4, dim=1)\nprint(\"\u2702\ufe0f Chunk into 4 pieces (uneven division - 6 columns \u00f7 4 chunks):\")\nprint(f\"Number of chunks: {len(uneven_chunks)}\")\nfor i, chunk in enumerate(uneven_chunks):\n    print(f\"Chunk {i+1} shape {chunk.shape}: {chunk.shape[1]} columns\")\nprint(\"Notice: First 2 chunks get 2 columns each, last 2 chunks get 1 column each!\")\n</pre> # Chunk into exactly 2 pieces along dimension 0 (rows) row_chunks = torch.chunk(split_subject, chunks=2, dim=0) print(\"\u2702\ufe0f Chunk into exactly 2 row pieces:\") print(f\"Number of chunks: {len(row_chunks)}\") for i, chunk in enumerate(row_chunks):     print(f\"Chunk {i+1} shape {chunk.shape}:\\n{chunk}\\n\")  # Chunk into exactly 3 pieces along dimension 1 (columns) # Note: 6 columns \u00f7 3 chunks = 2 columns per chunk (perfect division!) col_chunks = torch.chunk(split_subject, chunks=3, dim=1) print(\"\u2702\ufe0f Chunk into exactly 3 column pieces:\") print(f\"Number of chunks: {len(col_chunks)}\") for i, chunk in enumerate(col_chunks):     print(f\"Chunk {i+1} shape {chunk.shape}:\\n{chunk}\\n\")  # What happens with uneven division? Let's try 4 chunks from 6 columns uneven_chunks = torch.chunk(split_subject, chunks=4, dim=1) print(\"\u2702\ufe0f Chunk into 4 pieces (uneven division - 6 columns \u00f7 4 chunks):\") print(f\"Number of chunks: {len(uneven_chunks)}\") for i, chunk in enumerate(uneven_chunks):     print(f\"Chunk {i+1} shape {chunk.shape}: {chunk.shape[1]} columns\") print(\"Notice: First 2 chunks get 2 columns each, last 2 chunks get 1 column each!\")  <pre>\u2702\ufe0f Chunk into exactly 2 row pieces:\nNumber of chunks: 2\nChunk 1 shape torch.Size([2, 6]):\ntensor([[1., 2., 3., 4., 5., 6.],\n        [1., 2., 3., 4., 5., 6.]])\n\nChunk 2 shape torch.Size([2, 6]):\ntensor([[1., 2., 3., 4., 5., 6.],\n        [1., 2., 3., 4., 5., 6.]])\n\n\u2702\ufe0f Chunk into exactly 3 column pieces:\nNumber of chunks: 3\nChunk 1 shape torch.Size([4, 2]):\ntensor([[1., 2.],\n        [1., 2.],\n        [1., 2.],\n        [1., 2.]])\n\nChunk 2 shape torch.Size([4, 2]):\ntensor([[3., 4.],\n        [3., 4.],\n        [3., 4.],\n        [3., 4.]])\n\nChunk 3 shape torch.Size([4, 2]):\ntensor([[5., 6.],\n        [5., 6.],\n        [5., 6.],\n        [5., 6.]])\n\n\u2702\ufe0f Chunk into 4 pieces (uneven division - 6 columns \u00f7 4 chunks):\nNumber of chunks: 3\nChunk 1 shape torch.Size([4, 2]): 2 columns\nChunk 2 shape torch.Size([4, 2]): 2 columns\nChunk 3 shape torch.Size([4, 2]): 2 columns\nNotice: First 2 chunks get 2 columns each, last 2 chunks get 1 column each!\n</pre> In\u00a0[51]: Copied! <pre># Unbind along dimension 0 - separate each row into individual tensors\nprint(split_subject)\nunbound_rows = torch.unbind(split_subject, dim=0)\nprint(\"\ud83d\udca5 Unbind along dimension 0 (separate each row):\")\nprint(f\"Number of tensors: {len(unbound_rows)}\")\nprint(f\"Original shape: {split_subject.shape} \u2192 Individual tensor shape: {unbound_rows[0].shape}\")\nfor i, row_tensor in enumerate(unbound_rows):\n    print(f\"Row {i+1}: {row_tensor}\")\n\nprint(\"\\n\" + \"=\"*50 + \"\\n\")\n\n# Unbind along dimension 1 - separate each column into individual tensors  \nunbound_cols = torch.unbind(split_subject, dim=1)\nprint(\"\ud83d\udca5 Unbind along dimension 1 (separate each column):\")\nprint(f\"Number of tensors: {len(unbound_cols)}\")\nprint(f\"Original shape: {split_subject.shape} \u2192 Individual tensor shape: {unbound_cols[0].shape}\")\nfor i, col_tensor in enumerate(unbound_cols):\n    print(f\"Column {i+1}: {col_tensor}\")\n\nprint(f\"\\n\ud83e\udde0 Key Insight: unbind() reduces dimensions! 2D \u2192 1D tensors\")\nprint(f\"   It's like taking apart the 3D book we built earlier!\")\n</pre>  # Unbind along dimension 0 - separate each row into individual tensors print(split_subject) unbound_rows = torch.unbind(split_subject, dim=0) print(\"\ud83d\udca5 Unbind along dimension 0 (separate each row):\") print(f\"Number of tensors: {len(unbound_rows)}\") print(f\"Original shape: {split_subject.shape} \u2192 Individual tensor shape: {unbound_rows[0].shape}\") for i, row_tensor in enumerate(unbound_rows):     print(f\"Row {i+1}: {row_tensor}\")  print(\"\\n\" + \"=\"*50 + \"\\n\")  # Unbind along dimension 1 - separate each column into individual tensors   unbound_cols = torch.unbind(split_subject, dim=1) print(\"\ud83d\udca5 Unbind along dimension 1 (separate each column):\") print(f\"Number of tensors: {len(unbound_cols)}\") print(f\"Original shape: {split_subject.shape} \u2192 Individual tensor shape: {unbound_cols[0].shape}\") for i, col_tensor in enumerate(unbound_cols):     print(f\"Column {i+1}: {col_tensor}\")  print(f\"\\n\ud83e\udde0 Key Insight: unbind() reduces dimensions! 2D \u2192 1D tensors\") print(f\"   It's like taking apart the 3D book we built earlier!\")  <pre>tensor([[1., 2., 3., 4., 5., 6.],\n        [1., 2., 3., 4., 5., 6.],\n        [1., 2., 3., 4., 5., 6.],\n        [1., 2., 3., 4., 5., 6.]])\n\ud83d\udca5 Unbind along dimension 0 (separate each row):\nNumber of tensors: 4\nOriginal shape: torch.Size([4, 6]) \u2192 Individual tensor shape: torch.Size([6])\nRow 1: tensor([1., 2., 3., 4., 5., 6.])\nRow 2: tensor([1., 2., 3., 4., 5., 6.])\nRow 3: tensor([1., 2., 3., 4., 5., 6.])\nRow 4: tensor([1., 2., 3., 4., 5., 6.])\n\n==================================================\n\n\ud83d\udca5 Unbind along dimension 1 (separate each column):\nNumber of tensors: 6\nOriginal shape: torch.Size([4, 6]) \u2192 Individual tensor shape: torch.Size([4])\nColumn 1: tensor([1., 1., 1., 1.])\nColumn 2: tensor([2., 2., 2., 2.])\nColumn 3: tensor([3., 3., 3., 3.])\nColumn 4: tensor([4., 4., 4., 4.])\nColumn 5: tensor([5., 5., 5., 5.])\nColumn 6: tensor([6., 6., 6., 6.])\n\n\ud83e\udde0 Key Insight: unbind() reduces dimensions! 2D \u2192 1D tensors\n   It's like taking apart the 3D book we built earlier!\n</pre> In\u00a0[54]: Copied! <pre># Exercise 1: Multi-Head Attention Surgery \ud83e\udde0\u26a1\n\nprint(\"\ud83e\udde0 EXERCISE 1: Multi-Head Attention Head Splitting\")\nprint(\"=\" * 60)\n\n# The scenario: Output from a Transformer's multi-head attention layer\nbatch_size, seq_len, d_model = 32, 128, 512\nnum_heads = 8\n\n# The attention layer outputs one big tensor containing ALL attention heads\nattention_output = torch.randn(batch_size, seq_len, d_model)\nprint(f\"\ud83d\udd2c Raw attention output shape: {attention_output.shape}\")\nprint(\"This contains ALL 8 attention heads concatenated together!\")\n\n# YOUR SURGICAL PRECISION: Split this into individual attention heads\nhead_dim = d_model // num_heads  # 512 // 8 = 64 dimensions per head\nattention_heads = torch.split(attention_output, head_dim, dim=2)\n\nprint(f\"\\n\u2694\ufe0f MAGNIFICENT! Split into {len(attention_heads)} individual attention heads!\")\nprint(f\"Each head shape: {attention_heads[0].shape}\")\nprint(f\"Head dimension: {head_dim} (d_model / num_heads = {d_model} / {num_heads})\")\n\n# Verify our surgery was successful\ntotal_dims = sum(head.shape[2] for head in attention_heads)\nprint(f\"\\n\u2705 Verification: {total_dims} total dimensions = {d_model} original dimensions\")\nprint(\"Perfect! No information lost in the surgical procedure!\")\n</pre> # Exercise 1: Multi-Head Attention Surgery \ud83e\udde0\u26a1  print(\"\ud83e\udde0 EXERCISE 1: Multi-Head Attention Head Splitting\") print(\"=\" * 60)  # The scenario: Output from a Transformer's multi-head attention layer batch_size, seq_len, d_model = 32, 128, 512 num_heads = 8  # The attention layer outputs one big tensor containing ALL attention heads attention_output = torch.randn(batch_size, seq_len, d_model) print(f\"\ud83d\udd2c Raw attention output shape: {attention_output.shape}\") print(\"This contains ALL 8 attention heads concatenated together!\")  # YOUR SURGICAL PRECISION: Split this into individual attention heads head_dim = d_model // num_heads  # 512 // 8 = 64 dimensions per head attention_heads = torch.split(attention_output, head_dim, dim=2)  print(f\"\\n\u2694\ufe0f MAGNIFICENT! Split into {len(attention_heads)} individual attention heads!\") print(f\"Each head shape: {attention_heads[0].shape}\") print(f\"Head dimension: {head_dim} (d_model / num_heads = {d_model} / {num_heads})\")  # Verify our surgery was successful total_dims = sum(head.shape[2] for head in attention_heads) print(f\"\\n\u2705 Verification: {total_dims} total dimensions = {d_model} original dimensions\") print(\"Perfect! No information lost in the surgical procedure!\")  <pre>\ud83e\udde0 EXERCISE 1: Multi-Head Attention Head Splitting\n============================================================\n\ud83d\udd2c Raw attention output shape: torch.Size([32, 128, 512])\nThis contains ALL 8 attention heads concatenated together!\n\n\u2694\ufe0f MAGNIFICENT! Split into 8 individual attention heads!\nEach head shape: torch.Size([32, 128, 64])\nHead dimension: 64 (d_model / num_heads = 512 / 8)\n\n\u2705 Verification: 512 total dimensions = 512 original dimensions\nPerfect! No information lost in the surgical procedure!\n</pre> In\u00a0[56]: Copied! <pre># Exercise 2: Multi-Modal Fusion (CLIP-Style) \ud83d\uddbc\ufe0f\ud83d\udcdd\n\nprint(\"\ud83d\uddbc\ufe0f EXERCISE 2: Multi-Modal Embedding Fusion\")\nprint(\"=\" * 60)\n\n# The scenario: Separate embeddings for text and images (like CLIP)\nbatch_size = 8\n\n# Two separate universes of understanding\ntext_embeddings = torch.randn(batch_size, 512)    # \"A cat sitting on a chair\"  \nimage_embeddings = torch.randn(batch_size, 512)   # [actual image of cat on chair]\n\nprint(f\"\ud83d\udcdd Text embeddings: {text_embeddings.shape} (language understanding)\")\nprint(f\"\ud83d\uddbc\ufe0f Image embeddings: {image_embeddings.shape} (visual understanding)\")\nprint(\"Two separate modalities, waiting to be unified...\")\n\n# YOUR FUSION MASTERY: Concatenate to create multimodal understanding\nmultimodal_embeddings = torch.cat([text_embeddings, image_embeddings], dim=1)\n\nprint(f\"\\n\ud83e\udde0 BEHOLD! Fused multimodal embeddings: {multimodal_embeddings.shape}\")\nprint(f\"Combined features: 512 (text) + 512 (image) = {multimodal_embeddings.shape[1]}\")\n\n# Verify our fusion preserved all information\nassert multimodal_embeddings.shape[1] == text_embeddings.shape[1] + image_embeddings.shape[1]\nprint(f\"\\n\u2705 Fusion verification: Perfect! No information lost!\")\n</pre> # Exercise 2: Multi-Modal Fusion (CLIP-Style) \ud83d\uddbc\ufe0f\ud83d\udcdd  print(\"\ud83d\uddbc\ufe0f EXERCISE 2: Multi-Modal Embedding Fusion\") print(\"=\" * 60)  # The scenario: Separate embeddings for text and images (like CLIP) batch_size = 8  # Two separate universes of understanding text_embeddings = torch.randn(batch_size, 512)    # \"A cat sitting on a chair\"   image_embeddings = torch.randn(batch_size, 512)   # [actual image of cat on chair]  print(f\"\ud83d\udcdd Text embeddings: {text_embeddings.shape} (language understanding)\") print(f\"\ud83d\uddbc\ufe0f Image embeddings: {image_embeddings.shape} (visual understanding)\") print(\"Two separate modalities, waiting to be unified...\")  # YOUR FUSION MASTERY: Concatenate to create multimodal understanding multimodal_embeddings = torch.cat([text_embeddings, image_embeddings], dim=1)  print(f\"\\n\ud83e\udde0 BEHOLD! Fused multimodal embeddings: {multimodal_embeddings.shape}\") print(f\"Combined features: 512 (text) + 512 (image) = {multimodal_embeddings.shape[1]}\")  # Verify our fusion preserved all information assert multimodal_embeddings.shape[1] == text_embeddings.shape[1] + image_embeddings.shape[1] print(f\"\\n\u2705 Fusion verification: Perfect! No information lost!\")  <pre>\ud83d\uddbc\ufe0f EXERCISE 2: Multi-Modal Embedding Fusion\n============================================================\n\ud83d\udcdd Text embeddings: torch.Size([8, 512]) (language understanding)\n\ud83d\uddbc\ufe0f Image embeddings: torch.Size([8, 512]) (visual understanding)\nTwo separate modalities, waiting to be unified...\n\n\ud83e\udde0 BEHOLD! Fused multimodal embeddings: torch.Size([8, 1024])\nCombined features: 512 (text) + 512 (image) = 1024\n\n\u2705 Fusion verification: Perfect! No information lost!\n</pre> In\u00a0[57]: Copied! <pre># Exercise 3: RGB Channel Liberation \ud83c\udfa8\ud83d\udd13\n\nprint(\"\ud83c\udfa8 EXERCISE 3: RGB Channel Separation\")\nprint(\"=\" * 60)\n\n# The scenario: A batch of RGB images with entangled color channels\nbatch_size, height, width = 16, 224, 224\n\n# RGB images with all color channels mixed together\nrgb_batch = torch.randn(batch_size, 3, height, width)  # Standard (N, C, H, W) format\nprint(f\"\ud83d\uddbc\ufe0f RGB image batch: {rgb_batch.shape}\")\nprint(\"Format: (batch_size, channels, height, width)\")\nprint(\"All color information is entangled together!\")\n\n# YOUR LIBERATION TECHNIQUE: Free each color channel across the entire batch\nred_batch, green_batch, blue_batch = torch.unbind(rgb_batch, dim=1)\n\nprint(f\"\\n\ud83d\udd34 Red channel batch: {red_batch.shape}\")\nprint(f\"\ud83d\udfe2 Green channel batch: {green_batch.shape}\")\nprint(f\"\ud83d\udd35 Blue channel batch: {blue_batch.shape}\")\nprint(\"Each channel is now a separate grayscale batch!\")\n\n# Demonstrate the power: we can reconstruct the original perfectly\nreconstructed = torch.stack([red_batch, green_batch, blue_batch], dim=1)\nprint(f\"\\n\ud83d\udd04 Reconstruction test: {reconstructed.shape}\")\nprint(f\"Perfect match: {torch.equal(rgb_batch, reconstructed)}\")\n\n# Show the dimensional transformation clearly\nprint(f\"\\n\ud83d\udcd0 Dimensional analysis:\")\nprint(f\"   Original: {rgb_batch.shape} \u2192 RGB channels mixed\")\nprint(f\"   After unbind: 3 separate {red_batch.shape} grayscale batches\")  \nprint(f\"   Reconstruction: {reconstructed.shape} \u2192 Back to original!\")\n\nprint(f\"\\n\ud83d\udca1 Real-world channel liberation applications:\")\nprint(f\"   \ud83c\udfe5 Medical imaging: Isolate blood vessels, tissue types\")\nprint(f\"   \ud83d\udef0\ufe0f Satellite analysis: Separate vegetation, water, urban areas\")\nprint(f\"   \ud83d\udcca Computer vision: Channel-wise preprocessing and normalization\")\nprint(f\"   \ud83c\udfad Image processing: Artistic filters and color grading\")\n\nprint(f\"\\nBehold! We've mastered the separation and reunification of visual reality!\")\nprint(\"The RGB trinity bows to our surgical precision! Mwahahaha! \ud83d\udd2c\u26a1\")\n</pre> # Exercise 3: RGB Channel Liberation \ud83c\udfa8\ud83d\udd13  print(\"\ud83c\udfa8 EXERCISE 3: RGB Channel Separation\") print(\"=\" * 60)  # The scenario: A batch of RGB images with entangled color channels batch_size, height, width = 16, 224, 224  # RGB images with all color channels mixed together rgb_batch = torch.randn(batch_size, 3, height, width)  # Standard (N, C, H, W) format print(f\"\ud83d\uddbc\ufe0f RGB image batch: {rgb_batch.shape}\") print(\"Format: (batch_size, channels, height, width)\") print(\"All color information is entangled together!\")  # YOUR LIBERATION TECHNIQUE: Free each color channel across the entire batch red_batch, green_batch, blue_batch = torch.unbind(rgb_batch, dim=1)  print(f\"\\n\ud83d\udd34 Red channel batch: {red_batch.shape}\") print(f\"\ud83d\udfe2 Green channel batch: {green_batch.shape}\") print(f\"\ud83d\udd35 Blue channel batch: {blue_batch.shape}\") print(\"Each channel is now a separate grayscale batch!\")  # Demonstrate the power: we can reconstruct the original perfectly reconstructed = torch.stack([red_batch, green_batch, blue_batch], dim=1) print(f\"\\n\ud83d\udd04 Reconstruction test: {reconstructed.shape}\") print(f\"Perfect match: {torch.equal(rgb_batch, reconstructed)}\")  # Show the dimensional transformation clearly print(f\"\\n\ud83d\udcd0 Dimensional analysis:\") print(f\"   Original: {rgb_batch.shape} \u2192 RGB channels mixed\") print(f\"   After unbind: 3 separate {red_batch.shape} grayscale batches\")   print(f\"   Reconstruction: {reconstructed.shape} \u2192 Back to original!\")  print(f\"\\n\ud83d\udca1 Real-world channel liberation applications:\") print(f\"   \ud83c\udfe5 Medical imaging: Isolate blood vessels, tissue types\") print(f\"   \ud83d\udef0\ufe0f Satellite analysis: Separate vegetation, water, urban areas\") print(f\"   \ud83d\udcca Computer vision: Channel-wise preprocessing and normalization\") print(f\"   \ud83c\udfad Image processing: Artistic filters and color grading\")  print(f\"\\nBehold! We've mastered the separation and reunification of visual reality!\") print(\"The RGB trinity bows to our surgical precision! Mwahahaha! \ud83d\udd2c\u26a1\")  <pre>\ud83c\udfa8 EXERCISE 3: RGB Channel Separation\n============================================================\n\ud83d\uddbc\ufe0f RGB image batch: torch.Size([16, 3, 224, 224])\nFormat: (batch_size, channels, height, width)\nAll color information is entangled together!\n\n\ud83d\udd34 Red channel batch: torch.Size([16, 224, 224])\n\ud83d\udfe2 Green channel batch: torch.Size([16, 224, 224])\n\ud83d\udd35 Blue channel batch: torch.Size([16, 224, 224])\nEach channel is now a separate grayscale batch!\n\n\ud83d\udd04 Reconstruction test: torch.Size([16, 3, 224, 224])\nPerfect match: True\n\n\ud83d\udcd0 Dimensional analysis:\n   Original: torch.Size([16, 3, 224, 224]) \u2192 RGB channels mixed\n   After unbind: 3 separate torch.Size([16, 224, 224]) grayscale batches\n   Reconstruction: torch.Size([16, 3, 224, 224]) \u2192 Back to original!\n\n\ud83d\udca1 Real-world channel liberation applications:\n   \ud83c\udfe5 Medical imaging: Isolate blood vessels, tissue types\n   \ud83d\udef0\ufe0f Satellite analysis: Separate vegetation, water, urban areas\n   \ud83d\udcca Computer vision: Channel-wise preprocessing and normalization\n   \ud83c\udfad Image processing: Artistic filters and color grading\n\nBehold! We've mastered the separation and reunification of visual reality!\nThe RGB trinity bows to our surgical precision! Mwahahaha! \ud83d\udd2c\u26a1\n</pre>"},{"location":"01-tensors/02a_tensor_manipulation/#tensor-surgery-assembly","title":"Tensor Surgery &amp; Assembly\u00b6","text":""},{"location":"01-tensors/02a_tensor_manipulation/#professor-torchensteins-grand-directive","title":"Professor Torchenstein's Grand Directive\u00b6","text":"<p>Mwahahaha! You have summoned your first tensors from the ether! They are... raw. Untamed. Clumps of numerical clay awaiting a master's touch. A lesser mind would be content with their existence, but not you. Not us!</p> <p>Today, we become tensor surgeons! We will dissect tensors with the precision of a master anatomist, join them with the skill of a mad scientist, and divide them like a seasoned alchemist splitting compounds. This is not mere data processing; this is tensor surgery and assembly! Prepare to wield your digital scalpel and fusion apparatus!</p> <p></p>"},{"location":"01-tensors/02a_tensor_manipulation/#your-mission-briefing","title":"Your Mission Briefing\u00b6","text":"<p>By the time you escape this surgical theater, you will have mastered the fundamental arts of tensor manipulation:</p> <ul> <li>\ud83d\udd2a The Art of Selection: Pluck elements, rows, or slices from a tensor with surgical slicing precision.</li> <li>\ud83e\uddec Forbidden Fusions: Combine disparate tensors into unified monstrosities with <code>torch.cat</code> and <code>torch.stack</code>.</li> <li>\u2702\ufe0f The Great Division: Split larger tensors into manageable pieces using <code>torch.split</code> and <code>torch.chunk</code>.</li> </ul> <p>Estimated Time to Completion: 20 minutes of surgical tensor mastery.</p> <p>What You'll Need:</p> <ul> <li>The wisdom from our last lesson on summoning tensors.</li> <li>A steady hand for precision cuts and fusions!</li> <li>Your PyTorch environment, humming with anticipation.</li> </ul> <p>Coming Next: In lesson 2b, you'll learn the metamorphic arts of reshaping, squeezing, and permuting tensors to transform their very essence!</p>"},{"location":"01-tensors/02a_tensor_manipulation/#part-1-the-art-of-selection-slicing","title":"Part 1: The Art of Selection - Slicing\u00b6","text":"<p>Before you can reshape a tensor, you must learn to grasp its individual parts. Indexing is your scalpel, allowing you to perform precision surgery on your data. Slicing is your cleaver, letting you carve out whole sections for your grand experiments.</p> <p>We will start by summoning a test subject\u2014a 2D tensor brimming with potential! We must also prepare our lab with the usual incantations (<code>import torch</code> and <code>manual_seed</code>) to ensure our results are repeatable. We are scientists, not chaos-wizards!</p>"},{"location":"01-tensors/02a_tensor_manipulation/#sweeping-strikes-accessing-rows-and-columns","title":"Sweeping Strikes: Accessing Rows and Columns\u00b6","text":"<p>Previous lesson: 01_introduction_to_tensors.ipynb gives you the basics for accessing element of a tensor. But what if we require an entire row or column for our dark machinations? For this, we use the colon <code>:</code>, the universal symbol for \"give me everything along this dimension!\"</p> <ul> <li><code>[row_index, :]</code> - Fetches the entire row.</li> <li><code>[:, column_index]</code> - Fetches the entire column.</li> </ul> <p>Let's seize the entire 3rd row (index 2) and the 2nd column (index 1).</p>"},{"location":"01-tensors/02a_tensor_manipulation/#carving-chunks-the-power-of-slicing","title":"Carving Chunks: The Power of Slicing\u00b6","text":"<p>Mere elements are but trivialities! True power lies in carving out entire sub-regions of a tensor. Slicing uses the <code>start:end</code> notation. As with all Pythonic sorcery, the <code>start</code> is inclusive, but the <code>end</code> is exclusive.</p> <p>Let us carve out the block containing the 2nd and 3rd rows (indices 1 and 2), and the last two columns (indices 2 and 3).</p>"},{"location":"01-tensors/02a_tensor_manipulation/#conditional-conjuring-boolean-mask-indexing","title":"Conditional Conjuring: Boolean Mask Indexing\u00b6","text":"<p>Now for a truly diabolical technique! We can use a boolean mask to summon only the elements that meet our nefarious criteria. A boolean mask is a tensor of the same shape as our subject, but it contains only <code>True</code> or <code>False</code> values. When used for indexing, it returns a 1D tensor containing only the elements where the mask was <code>True</code>.</p> <p>Let's find all the alchemical ingredients in our tensor with a value greater than 50!</p>"},{"location":"01-tensors/02a_tensor_manipulation/#your-mission-the-slicers-gauntlet","title":"Your Mission: The Slicer's Gauntlet\u00b6","text":"<p>Enough of my demonstrations! The scalpel is now in your hand. Prove your mastery with these challenges!</p> <ol> <li>The Corner Pocket: From our <code>subject_tensor</code>, select the element in the very last row and last column.</li> <li>The Central Core: Select the inner <code>3x2</code> block of the <code>subject_tensor</code> (that's rows 1-3 and columns 1-2).</li> <li>The Even Stevens: Create a boolean mask to select only the elements in <code>subject_tensor</code> that are even numbers. (Hint: The modulo operator <code>%</code> is your friend!)</li> <li>The Grand Mutation: Use your boolean mask from challenge 3 to change all even numbers in the <code>subject_tensor</code> to the value <code>-1</code>. Then, print the mutated tensor. Yes, my apprentice, indexing can be used for assignment! This is a pivotal secret!</li> </ol>"},{"location":"01-tensors/02a_tensor_manipulation/#part-2-forbidden-fusions-joining-tensors","title":"Part 2: Forbidden Fusions - Joining Tensors\u00b6","text":"<p>Ah, but dissecting tensors is only half the art! A true master must also know how to fuse separate tensors into a single, magnificent whole. Sometimes your data comes in fragments\u2014perhaps different batches, different features, or different time steps. You must unite them!</p> <p>We have two primary spells for this dark ritual:</p> <ul> <li><code>torch.cat()</code> - The Concatenator! Joins tensors along an existing dimension.</li> <li><code>torch.stack()</code> - The Stacker! Creates a new dimension and stacks tensors along it.</li> </ul> <p>The difference is subtle but critical. Choose wrongly, and your creation will crumble! Let us forge some test subjects to demonstrate this power.</p>"},{"location":"01-tensors/02a_tensor_manipulation/#the-concatenator-torchcat","title":"The Concatenator: <code>torch.cat()</code>\u00b6","text":"<p><code>torch.cat()</code> joins tensors along an existing dimension. Think of it as gluing them end-to-end.</p> <p>The key rule: All tensors must have the same shape, except along the dimension you're concatenating!</p> <ul> <li><code>dim=0</code> (or <code>axis=0</code>): Concatenate along rows (vertically stack)</li> <li><code>dim=1</code> (or <code>axis=1</code>): Concatenate along columns (horizontally join)</li> </ul> <p>Let us witness this concatenation sorcery!</p>"},{"location":"01-tensors/02a_tensor_manipulation/#the-concatenation-rules-when-shapes-dont-match","title":"The Concatenation Rules: When Shapes Don't Match\u00b6","text":"<p>Now, let us test the fundamental law of concatenation with unequal tensors! Remember: All tensors must have the same shape, except along the dimension you're concatenating.</p> <p>Eg1. If you joining 2D matrices along rows (dim=0) the number of collumns should be the same.</p> <p>Let's create two tensors with different shapes and see what happens:</p>"},{"location":"01-tensors/02a_tensor_manipulation/#the-stacker-torchstack-creating-new-dimensions","title":"The Stacker: <code>torch.stack()</code> - Creating New Dimensions!\u00b6","text":"<p><code>torch.stack()</code> is more dramatic than concatenation! It creates an entirely new dimension and places each tensor along it. Think of it as the difference between:</p> <ul> <li>Concatenation: Gluing pieces end-to-end in the same plane \ud83e\udde9\u27a1\ufe0f\ud83e\udde9</li> <li>Stacking: Creating a whole new layer/dimension \ud83d\udcda (like stacking books on top of each other)</li> </ul> <p>Critical Rule: All input tensors must have identical shapes\u2014no exceptions!</p> <p>Let's start simple and build our intuition step by step...</p>"},{"location":"01-tensors/02a_tensor_manipulation/#step-1-stacking-1d-tensors-creating-a-2d-matrix","title":"Step 1: Stacking 1D Tensors \u2192 Creating a 2D Matrix\u00b6","text":"<p>Let's start with something simple: three 1D tensors (think of them as rulers \ud83d\udccf). When we stack them, we create a 2D matrix!</p>"},{"location":"01-tensors/02a_tensor_manipulation/#step-2-stacking-2d-tensors-creating-a-3d-cube","title":"Step 2: Stacking 2D Tensors \u2192 Creating a 3D Cube!\u00b6","text":"<p>Now for the mind-bending part! When we stack 2D tensors (matrices), we create a 3D tensor. Think of it like:</p> <p>\ud83d\udcc4 2D tensor = A page from a book (has rows and columns) \ud83d\udcd6 Stacking 2D tensors = Creating a book with multiple pages \ud83d\udcda 3D tensor = The entire book! (pages \u00d7 rows \u00d7 columns)</p> <p>Key Metaphors to Remember:</p> <ul> <li>Book metaphor: <code>tensor[page][row][column]</code> \ud83d\udcda</li> <li>RGB image: <code>tensor[channel][height][width]</code> \ud83d\uddbc\ufe0f (like stacking color layers: Red, Green, Blue)</li> </ul> <p>These metaphors help you \"see\" how changing the dimension you stack along changes the meaning of each axis in your tensor. \ud83e\udd13</p> <p>Let's see this dimensional magic in action:</p>"},{"location":"01-tensors/02a_tensor_manipulation/#step-3-the-dimension-dance-where-you-stack-matters","title":"Step 3: The Dimension Dance - Where You Stack Matters!\u00b6","text":"<p>When stacking 2D tensors, which dimension you choose creates very different 3D shapes!</p> <p>\ud83e\uddf1 The Clay Tablet Box Metaphor:</p> <p>For better understanding, imagine that when you use the <code>stack()</code> method, a new 3D package is created with an extended dimension. If you stack 2D objects (like clay tablets), you first create a 3D box, then arrange your 2D tablets inside.</p> <p>Picture this: You have three identical clay tablets \ud83e\uddf1 and an empty 3D box \ud83d\udce6. There are exactly 3 different ways to arrange them inside!</p> <p>\ud83d\udcd0 3D Box Coordinates (always viewed from the same angle):</p> <ul> <li>Depth = <code>dim=0</code> (front to back)</li> <li>Height = <code>dim=1</code> (bottom to top)</li> <li>Width = <code>dim=2</code> (left to right)</li> </ul> <p>\ud83c\udfaf The Three Stacking Strategies:</p> <ul> <li><p><code>dim=0</code>: Stack tablets front-to-back \u2192 Shape <code>(tablets, rows, cols)</code> \ud83d\udcda Each tablet goes deeper into the box, one behind the other</p> </li> <li><p><code>dim=1</code>: Stack tablets bottom-to-top \u2192 Shape <code>(rows, tablets, cols)</code> \ud83d\uddc2\ufe0f Each tablet is placed higher in the box, building upward - we start with last tablets</p> </li> <li><p><code>dim=2</code>: Slide tablets left-to-right \u2192 Shape <code>(rows, cols, tablets)</code> \ud83d\udcd1 Each tablet slides sideways, arranged side by side</p> </li> </ul> <p>The dimension you choose determines which direction your tablets extend in the 3D space!</p>"},{"location":"01-tensors/02a_tensor_manipulation/#the-fusion-dilemma-when-to-cat-vs-stack","title":"The Fusion Dilemma: When to Cat vs. Stack?\u00b6","text":"<p>This choice torments many apprentices! Let me illuminate the path:</p> <p>Use <code>torch.cat()</code> when:</p> <ul> <li>Tensors represent different parts of the same data (e.g., different batches of images, different chunks of text)</li> <li>You want to extend an existing dimension</li> <li>Example: Concatenating multiple batches of training data</li> </ul> <p>Use <code>torch.stack()</code> when:</p> <ul> <li>Tensors represent parallel data of the same type (e.g., predictions from different models, different time steps)</li> <li>You need to create a new dimension to organize the data</li> <li>Example: Combining RGB channels to form a color image, or collecting multiple predictions</li> </ul> <p>Observe this real-world scenario!</p>"},{"location":"01-tensors/02a_tensor_manipulation/#your-mission-the-fusion-masters-gauntlet","title":"Your Mission: The Fusion Master's Gauntlet\u00b6","text":"<p>The theory is yours\u2014now prove your mastery! Complete these fusion challenges:</p> <ol> <li><p>The Triple Stack: Create three 1D tensors of length 4 with different values. Stack them to create a 2D tensor of shape <code>(3, 4)</code>.</p> </li> <li><p>The Horizontal Fusion: Create two 2D tensors of shape <code>(3, 2)</code>. Concatenate them horizontally to create a <code>(3, 4)</code> tensor.</p> </li> <li><p>The Batch Builder: You have 5 individual \"samples\" (each a 1D tensor of length 3). Stack them to create a proper batch tensor of shape <code>(5, 3)</code> suitable for training.</p> </li> <li><p>The Dimension Disaster: Try to concatenate two tensors with different shapes: <code>(2, 3)</code> and <code>(2, 4)</code> along dimension 0. Observe the error message\u2014it's quite educational! Then fix it by concatenating along dimension 1 instead.</p> </li> <li><p>The Multi-Fusion: Create a tensor of shape <code>(2, 6)</code> by first stacking three <code>(2, 2)</code> tensors, then concatenating the result with another <code>(3, 6)</code> tensor. This requires combining both operations!</p> </li> </ol>"},{"location":"01-tensors/02a_tensor_manipulation/#part-3-the-great-division-splitting-tensors","title":"Part 3: The Great Division - Splitting Tensors\u00b6","text":"<p>Ah, but the surgeon's art is not complete until we master both creation AND division! Just as we learned to fuse tensors, we must also learn to split them apart. Sometimes your grand creation becomes too unwieldy, or you need to distribute pieces to different parts of your neural network.</p> <p>Fear not! PyTorch provides elegant tools for this delicate operation:</p> <ul> <li><code>torch.split()</code> \u2013 The Precise Slicer, it carves your tensor into chunks of the size you decree.</li> <li><code>torch.chunk()</code> - The Equal Divider! Splits a tensor into a specified number of roughly equal chunks.</li> <li><code>torch.unbind()</code> - The Dimension Destroyer! Removes a dimension by splitting along it.</li> </ul> <p>Let us prepare a worthy subject for our division experiments!</p>"},{"location":"01-tensors/02a_tensor_manipulation/#the-precise-slicer-torchsplit","title":"The Precise Slicer: <code>torch.split()</code>\u00b6","text":"<p>A Surgeon's Most Versatile Blade! The TRUE power of <code>torch.split()</code>! This is no mere cleaver\u2014it is a precision instrument worthy of a master tensor surgeon!</p> <p><code>torch.split(tensor, split_size_or_sections, dim)</code> possesses TWO magnificent modes of operation, each more diabolical than the last:</p> <p>\u2694\ufe0f Mode 1: The Uniform Guillotine (<code>split_size</code> as <code>int</code>) Feed it a single integer, and it slices your tensor into equal-sized chunks with ruthless efficiency! If the dimension refuses to divide evenly, the final piece shall be smaller\u2014a perfectly acceptable sacrifice to the tensor gods!</p> <p>\ud83d\udde1\ufe0f Mode 2: The Bespoke Scalpel (<code>sections</code> as <code>list[int]</code>) Ah, but THIS is where true tensor surgery ascends to high art! \u2702\ufe0f Provide a list of integers\u2014each one dictating the exact size of its corresponding slice. You wield total control! But heed this immutable law, dear apprentice: the sum of your list must match the total size of the dimension you wish to split. This is the sacred contract of the tensor gods\u2014break it, and chaos (or at least a RuntimeError) shall ensue!</p> <p>The Sacred Parameters:</p> <ul> <li><code>split_size_or_sections</code>: An <code>int</code> for uniform domination, or a <code>list[int]</code> for bespoke surgical control!</li> <li><code>dim</code>: The dimensional axis along which your blade shall cut!</li> <li>Returns: A tuple of tensors (NEVER a single tensor\u2014the split method serves only masters, not slaves!)</li> </ul> <p>Now, witness as we dissect our chocolate bar with BOTH methods! The tensors... they will obey!</p>"},{"location":"01-tensors/02a_tensor_manipulation/#the-bespoke-scalpel-splitting-with-diabolical-precision","title":"The Bespoke Scalpel: Splitting with Diabolical Precision!\u00b6","text":"<p>Now, my ambitious apprentice, we ascend to the HIGHEST form of surgical artistry! While Rudolf Hammer and his corporate drones settle for uniform mediocrity, WE shall command each slice with mathematical perfection!</p> <p>The secret they don't want you to know: <code>torch.split()</code> accepts a list of integers, each commanding the exact size of its designated slice! The sum of these numbers must equal the total dimension\u2014this is not a suggestion, it is a LAW OF THE TENSOR UNIVERSE!</p> <p>Observe as we carve our 6-column chocolate bar into three asymmetric pieces of sizes <code>1, 2, and 3</code>. Each cut serves our grand design! Mwahahaha!</p>"},{"location":"01-tensors/02a_tensor_manipulation/#the-equal-divider-torchchunk","title":"The Equal Divider: <code>torch.chunk()</code>\u00b6","text":"<p><code>torch.chunk(tensor, chunks, dim)</code> divides your tensor into a specified number of roughly equal pieces. It's like asking: \"I need exactly 3 pieces, make them as equal as possible!\"</p> <p>Key Difference from <code>split()</code>:</p> <ul> <li><code>torch.split()</code>: \"Cut into pieces of size X\" (you control piece size)</li> <li><code>torch.chunk()</code>: \"Cut into exactly N pieces\" (you control number of pieces)</li> </ul> <p>If the dimension doesn't divide evenly, the last chunk will be smaller.</p>"},{"location":"01-tensors/02a_tensor_manipulation/#the-dimension-destroyer-torchunbind","title":"The Dimension Destroyer: <code>torch.unbind()</code>\u00b6","text":"<p><code>torch.unbind()</code> is the most dramatic! It removes an entire dimension by splitting the tensor along it. Each slice becomes a separate tensor with one fewer dimension.</p> <p>This is incredibly useful for:</p> <ul> <li>Processing each image in a batch separately</li> <li>Accessing individual time steps in sequence data</li> <li>Converting RGB channels into separate grayscale images</li> </ul> <p>Think of it as the opposite of <code>torch.stack()</code>!</p>"},{"location":"01-tensors/02a_tensor_manipulation/#part-4-real-world-surgical-applications","title":"Part 4: Real-World Surgical Applications \ud83c\udfed\u26a1\u00b6","text":"<p>Enough of my carefully controlled laboratory specimens! The time has come to witness the TRUE power of tensor surgery in the wild! These are not mere academic exercises\u2014these are the EXACT techniques used by the masters who built GPT, CLIP, and the neural networks that power modern AI!</p> <p>Prepare to see your newfound skills applied to the very foundations of modern machine learning!</p>"},{"location":"01-tensors/02a_tensor_manipulation/#exercise-1-multi-head-attention-surgery","title":"Exercise 1: Multi-Head Attention Surgery \ud83e\udde0\u26a1\u00b6","text":"<p>Real-World Context: This is EXACTLY how Transformer models (GPT, BERT) split their attention mechanism into multiple \"heads.\" Each head can focus on different aspects of the input\u2014some learn grammar, others learn semantics, others learn long-range dependencies!</p> <p>Your Mission: You have the concatenated output from all attention heads. Split it back into individual heads so each can be processed separately.</p> <p>The Setup: A Transformer's multi-head attention layer has just computed attention for a batch of sequences. All 8 attention heads are concatenated together in the last dimension. Your job: liberate each head!</p>"},{"location":"01-tensors/02a_tensor_manipulation/#exercise-2-multi-modal-fusion-clip-style","title":"Exercise 2: Multi-Modal Fusion (CLIP-Style) \ud83d\uddbc\ufe0f\ud83d\udcdd\u00b6","text":"<p>Real-World Context: This is the FOUNDATIONAL technique behind multimodal models! By concatenating text and image embeddings, AI systems learn to understand both modalities in a unified space.</p> <p>Your Mission: You have separate embeddings for text descriptions and images. Fuse them together to create a unified multimodal representation that can understand both vision and language!</p> <p>The Setup: A batch of text descriptions (\"A cat sitting on a chair\") and their corresponding image embeddings. Your fusion will enable the model to match images with text\u2014the core of visual search and image generation!</p>"},{"location":"01-tensors/02a_tensor_manipulation/#exercise-3-rgb-channel-liberation","title":"Exercise 3: RGB Channel Liberation \ud83c\udfa8\ud83d\udd13\u00b6","text":"<p>Real-World Context: Computer vision systems constantly need to separate and analyze individual color channels. This technique is fundamental.</p> <p>Your Mission: You have a batch of RGB images where all color channels are entangled together. Use your unbinding mastery to liberate each color channel into separate batches for independent processing!</p> <p>The Setup: A batch of color images from a dataset. Each image has Red, Green, and Blue channels mixed together. Your surgical precision will separate them cleanly while preserving the batch structure.</p>"},{"location":"01-tensors/02a_tensor_manipulation/#summary-the-surgeons-knowledge-is-complete","title":"Summary: The Surgeon's Knowledge Is Complete!\u00b6","text":"<p>Magnificent work, my surgical apprentice! You have mastered the fundamental operations of tensor surgery and assembly. Let us review your newly acquired powers:</p>"},{"location":"01-tensors/02a_tensor_manipulation/#the-art-of-selection","title":"\ud83d\udd2a The Art of Selection\u00b6","text":"<ul> <li>Indexing &amp; Slicing: Extract elements, rows, columns, or sub-regions with surgical precision</li> <li>Boolean Masking: Select elements that meet specific criteria</li> <li>Key Tools: <code>tensor[index]</code>, <code>tensor[start:end]</code>, <code>tensor[mask]</code></li> </ul>"},{"location":"01-tensors/02a_tensor_manipulation/#forbidden-fusions","title":"\ud83e\uddec Forbidden Fusions\u00b6","text":"<ul> <li>Concatenation: Join tensors along existing dimensions with <code>torch.cat()</code></li> <li>Stacking: Create new dimensions and organize tensors with <code>torch.stack()</code></li> <li>Key Rule: Understanding when to extend vs. when to create new dimensions</li> </ul>"},{"location":"01-tensors/02a_tensor_manipulation/#the-great-division","title":"\u2702\ufe0f The Great Division\u00b6","text":"<ul> <li>Precise Splitting: Cut into specific-sized pieces with <code>torch.split()</code></li> <li>Equal Division: Divide into a set number of chunks with <code>torch.chunk()</code></li> <li>Dimension Destruction: Remove dimensions entirely with <code>torch.unbind()</code></li> </ul> <p>You now possess the core skills to dissect any tensor, assemble complex structures, and divide them back into manageable pieces. These are the fundamental surgical techniques you'll use in every neural network adventure ahead!</p>"},{"location":"01-tensors/02a_tensor_manipulation/#professor-torchensteins-outro","title":"Professor Torchenstein's Outro\u00b6","text":"<p>Spectacular! You've wielded the surgical tools with the precision of a master! Your tensors have been sliced, fused, and divided according to your will. But tell me, my gifted apprentice\u2014do you feel that tingling sensation in your neural pathways? That's the hunger for more power!</p> <p>Your tensors may now be perfectly assembled, but they are still... rigid. Static in their dimensions. What if I told you that we could transform their very essence without changing their soul? What if we could make a 1D tensor become a 2D matrix, or a 3D cube collapse into a flat surface?</p> <p>In our next lesson, Tensor Metamorphosis: Shape-Shifting Mastery, we shall unlock the secrets of transformation itself! We will reshape reality, squeeze dimensions out of existence, and permute the cosmic order of our data!</p> <p>Until then, practice your surgical techniques. The metamorphosis chamber awaits! Mwahahahahaha!</p>      Your browser does not support the video tag.  <p>Proceed to the Metamorphosis Chamber!</p>"},{"location":"01-tensors/02b_tensor_metamorphosis/","title":"Tensor Metamorphosis: Shape-Shifting Mastery","text":"<p>Module 1 | Lesson 2b</p>"},{"location":"01-tensors/02b_tensor_metamorphosis/#tensor-metamorphosis-shape-shifting-mastery","title":"Tensor Metamorphosis: Shape-Shifting Mastery\u00b6","text":""},{"location":"01-tensors/02b_tensor_metamorphosis/#professor-torchensteins-grand-directive","title":"Professor Torchenstein's Grand Directive\u00b6","text":"<p>[Content to be added...]</p>"},{"location":"01-tensors/03_data_types_and_devices/","title":"Data Types and Devices","text":"<p>Module 1 | Lesson 3</p> <p>Ah, the Alchemist\u2019s Arsenal: PyTorch Data Types!</p> <p>Behold, apprentice! Not all tensors are forged alike. The essence of a tensor\u2014its <code>.dtype</code>\u2014determines what kind of numbers it can hold, and thus, what arcane computations it can perform. Choose wisely, for the wrong <code>dtype</code> can turn your beautiful model into a bubbling cauldron of errors!</p> <p>Floating-Point Types The lifeblood of neural networks! For when you wish to summon real numbers, gradients, and the very stuff of learning itself.</p> <ul> <li><code>torch.float32</code> or <code>torch.float</code>: 32-bit floating-point. The default elixir for most tensor incantations. Trusty, reliable, and the backbone of deep learning!</li> <li><code>torch.float64</code> or <code>torch.double</code>: 64-bit floating-point. For those moments when you crave precision\u2014perhaps to impress your rival, Dr. Hammer, with your numerical exactitude.</li> <li><code>torch.float16</code> or <code>torch.half</code>: 16-bit floating-point. The potion of choice for speed demons and memory misers. Use it to accelerate your experiments (especially on modern GPUs), but beware the lurking specter of numerical instability!</li> <li><code>torch.bfloat16</code>: 16-bit \u201cbrain\u201d floating-point. Like float16, but with a twist\u2014wider range, less precision. Favored by TPUs and the latest NVIDIA cauldrons.</li> </ul> <p>Integer Types For when you need to count, index, or encode the world in whole numbers. No fractions allowed!</p> <ul> <li><code>torch.int8</code>: 8-bit signed integer. Tiny, but sometimes mighty.</li> <li><code>torch.uint8</code>: 8-bit unsigned integer. The pixel pusher\u2019s favorite\u2014perfect for images and masks!</li> <li><code>torch.int16</code> or <code>torch.short</code>: 16-bit signed integer. Slightly more room for your integers to stretch their legs.</li> <li><code>torch.int32</code> or <code>torch.int</code>: 32-bit signed integer. The workhorse of integer types.</li> <li><code>torch.int64</code> or <code>torch.long</code>: 64-bit signed integer. The grandmaster\u2014often used for indices, embedding lookups, and any time you need to count very high.</li> </ul> <p>Boolean Type For the binary-minded: True or False, 1 or 0, on or off. The stuff of logic gates and comparison spells!</p> <ul> <li><code>torch.bool</code>: The result of your tensor comparisons, your maskings, your \u201cis it alive or dead?\u201d queries.</li> </ul> <p>Complex Types For the truly mad scientist\u2014numbers with both real and imaginary parts! Useful for signal processing, quantum shenanigans, and impressing your colleagues at tensor parties.</p> <ul> <li><code>torch.complex64</code>: Complex numbers with 32-bit real and 32-bit imaginary parts.</li> <li><code>torch.complex128</code>: Complex numbers with 64-bit real and 64-bit imaginary parts. For when your calculations must be both precise and mysterious.</li> </ul> <p>Choose your dtype as you would choose your wand, apprentice: with care, curiosity, and a dash of reckless ambition! Mwahahaha!</p>"},{"location":"01-tensors/03_data_types_and_devices/#data-types-and-devices","title":"Data Types and Devices\u00b6","text":""},{"location":"01-tensors/04_tensor_math_operations/","title":"Tensor Math Operations","text":""},{"location":"01-tensors/04_tensor_math_operations/#tensor-math-operations","title":"Tensor Math Operations\u00b6","text":""},{"location":"01-tensors/05_matrix_multiplication/","title":"Matrix Multiplication: Unleashing the Power of Tensors! \u26a1","text":"In\u00a0[2]: Copied! <pre>import torch\n\n# Create some matrices for experimentation\nA = torch.randn(3, 4)\nB = torch.randn(4, 2)\n\nprint(\"Matrix A shape:\", A.shape)\nprint(\"Matrix B shape:\", B.shape)\n\n# Matrix multiplication\nC = torch.matmul(A, B)\nprint(\"Result C shape:\", C.shape)\nprint(\"\\nMwahahaha! The matrices have been multiplied!\")\n</pre> import torch  # Create some matrices for experimentation A = torch.randn(3, 4) B = torch.randn(4, 2)  print(\"Matrix A shape:\", A.shape) print(\"Matrix B shape:\", B.shape)  # Matrix multiplication C = torch.matmul(A, B) print(\"Result C shape:\", C.shape) print(\"\\nMwahahaha! The matrices have been multiplied!\") <pre>Matrix A shape: torch.Size([3, 4])\nMatrix B shape: torch.Size([4, 2])\nResult C shape: torch.Size([3, 2])\n\nMwahahaha! The matrices have been multiplied!\n</pre>"},{"location":"01-tensors/05_matrix_multiplication/#matrix-multiplication-unleashing-the-power-of-tensors","title":"Matrix Multiplication: Unleashing the Power of Tensors! \u26a1\u00b6","text":"<p>\"Behold! The sacred art of matrix multiplication - where dimensions dance and vectors bend to my will!\" \u2014 Professor Victor py Torchenstein</p>"},{"location":"01-tensors/05_matrix_multiplication/#the-attention-formula-preview-of-things-to-come","title":"The Attention Formula (Preview of Things to Come)\u00b6","text":"<p>$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$</p> <p>Where:</p> <ul> <li>$Q$ is the Query matrix</li> <li>$K$ is the Key matrix</li> <li>$V$ is the Value matrix</li> <li>$d_k$ is the dimension of the key vectors</li> <li>$\\text{softmax}$ normalizes the attention weights</li> </ul>"},{"location":"01-tensors/05_matrix_multiplication/#basic-matrix-operations","title":"Basic Matrix Operations\u00b6","text":"<p>Let's start with the fundamentals before we conquer attention mechanisms!</p> <p>Element-wise multiplication:</p> <p>$C_{ij} = A_{ij} \\times B_{ij}$</p> <p>Matrix multiplication: $C_{ij} = \\sum_{k} A_{ik} \\times B_{kj}$</p>"},{"location":"01-tensors/05_matrix_multiplication/#pytorch-matrix-multiplication-methods","title":"PyTorch Matrix Multiplication Methods\u00b6","text":"<p>Professor Torchenstein's arsenal includes multiple ways to multiply matrices:</p> <ol> <li><code>torch.matmul()</code> - The general matrix multiplication function</li> <li><code>@</code> operator - Pythonic matrix multiplication (same as matmul)</li> <li><code>torch.mm()</code> - For 2D matrices only</li> <li><code>torch.bmm()</code> - Batch matrix multiplication</li> </ol>"},{"location":"01-tensors/05_matrix_multiplication/#mathematical-foundations","title":"Mathematical Foundations\u00b6","text":"<p>For matrices $A \\in \\mathbb{R}^{m \\times n}$ and $B \\in \\mathbb{R}^{n \\times p}$:</p> <p>$$C = AB \\quad \\text{where} \\quad C_{ij} = \\sum_{k=1}^{n} A_{ik} B_{kj}$$</p> <p>This operation is fundamental to:</p> <ul> <li>Linear transformations</li> <li>Neural network forward passes</li> <li>Attention mechanisms in Transformers</li> <li>And much more! \ud83e\udde0\u26a1</li> </ul>"},{"location":"01-tensors/06_broadcasting/","title":"Broadcasting","text":""},{"location":"01-tensors/06_broadcasting/#broadcasting","title":"Broadcasting\u00b6","text":""},{"location":"01-tensors/07_einstein_summation/","title":"Einstein Summation","text":""},{"location":"01-tensors/07_einstein_summation/#einstein-summation","title":"Einstein Summation\u00b6","text":""},{"location":"01-tensors/08_advanced_einstein_summation/","title":"Advanced Einstein Summation","text":""},{"location":"01-tensors/08_advanced_einstein_summation/#advanced-einstein-summation","title":"Advanced Einstein Summation\u00b6","text":""},{"location":"01-tensors/09_autograd/","title":"Autograd","text":""},{"location":"01-tensors/09_autograd/#autograd","title":"Autograd\u00b6","text":""},{"location":"01-tensors/10_gradient_accumulation/","title":"Gradient Accumulation","text":""},{"location":"01-tensors/10_gradient_accumulation/#gradient-accumulation","title":"Gradient Accumulation\u00b6","text":""},{"location":"02-torch-nn/01_nn_module/","title":"nn.Module","text":""},{"location":"02-torch-nn/01_nn_module/#nnmodule","title":"nn.Module\u00b6","text":""},{"location":"02-torch-nn/02_compose_modules/","title":"Compose Modules","text":""},{"location":"02-torch-nn/02_compose_modules/#compose-modules","title":"Compose Modules\u00b6","text":""},{"location":"02-torch-nn/03_saving_weights/","title":"Saving Weights","text":""},{"location":"02-torch-nn/03_saving_weights/#saving-weights","title":"Saving Weights\u00b6","text":""},{"location":"02-torch-nn/04_linear_layer/","title":"Linear Layer","text":""},{"location":"02-torch-nn/04_linear_layer/#linear-layer","title":"Linear Layer\u00b6","text":""},{"location":"02-torch-nn/05_activations/","title":"Activations","text":""},{"location":"02-torch-nn/05_activations/#activations","title":"Activations\u00b6","text":""},{"location":"02-torch-nn/06_dropout/","title":"Dropout","text":""},{"location":"02-torch-nn/06_dropout/#dropout","title":"Dropout\u00b6","text":""},{"location":"02-torch-nn/07_embedding_layers/","title":"Embedding Layers","text":""},{"location":"02-torch-nn/07_embedding_layers/#embedding-layers","title":"Embedding Layers\u00b6","text":""},{"location":"02-torch-nn/08_positional_encoding/","title":"Positional Embeddings","text":""},{"location":"02-torch-nn/08_positional_encoding/#positional-embeddings","title":"Positional Embeddings\u00b6","text":"<p>How to encode the token position in the sequence?</p> <p>References:</p> <ul> <li>Mastering LLAMA: Understanding Rotary Positional Embedding (RPE)</li> </ul>"},{"location":"02-torch-nn/09_normalization_layers/","title":"Normalization Layers","text":""},{"location":"02-torch-nn/09_normalization_layers/#normalization-layers","title":"Normalization Layers\u00b6","text":"<p>Estimated learning time: 15 minutes</p> <p>Learning objectives:</p> <ul> <li>Understand the basics of normalization layers: batch normalization, layer normalization, instance normalization, group normalization.</li> <li>Understand the difference between these normalization layers.</li> <li>Know how to use these normalization layers in practice. Compare the results of using different normalization layers.</li> </ul> <p>Resources:</p> <ul> <li>RMSNorm - a better normalization layer</li> </ul>"},{"location":"02-torch-nn/10_rms_norm/","title":"RMS Norm","text":""},{"location":"02-torch-nn/10_rms_norm/#rms-norm","title":"RMS Norm\u00b6","text":""},{"location":"02-torch-nn/11_training_evaluation_mode/","title":"Training Evaluation Mode","text":""},{"location":"02-torch-nn/11_training_evaluation_mode/#training-evaluation-mode","title":"Training Evaluation Mode\u00b6","text":""},{"location":"02-torch-nn/12_loss_functions/","title":"Loss Functions","text":""},{"location":"02-torch-nn/12_loss_functions/#loss-functions","title":"Loss Functions\u00b6","text":""},{"location":"02-torch-nn/13_prepare_inputs_targets/","title":"Prepare Inputs Targets","text":""},{"location":"02-torch-nn/13_prepare_inputs_targets/#prepare-inputs-targets","title":"Prepare Inputs Targets\u00b6","text":""},{"location":"02-torch-nn/14_interpreting_reduction_modes/","title":"Interpreting Reduction Modes","text":""},{"location":"02-torch-nn/14_interpreting_reduction_modes/#interpreting-reduction-modes","title":"Interpreting Reduction Modes\u00b6","text":""},{"location":"03-training-nn/01_training_loop/","title":"Training Loop","text":""},{"location":"03-training-nn/01_training_loop/#training-loop","title":"Training Loop\u00b6","text":""},{"location":"03-training-nn/02_optimizers_schedulers/","title":"Optimizers Schedulers","text":""},{"location":"03-training-nn/02_optimizers_schedulers/#optimizers-schedulers","title":"Optimizers Schedulers\u00b6","text":""},{"location":"03-training-nn/03_datasets_dataloaders/","title":"Datasets DataLoaders","text":""},{"location":"03-training-nn/03_datasets_dataloaders/#datasets-dataloaders","title":"Datasets DataLoaders\u00b6","text":""},{"location":"03-training-nn/04_gpu_acceleration/","title":"GPU Acceleration","text":""},{"location":"03-training-nn/04_gpu_acceleration/#gpu-acceleration","title":"GPU Acceleration\u00b6","text":"<p>Learning objectives:</p> <ul> <li>Understand the GPU parallelisation: data parallel, model parallel, pipeline parallel, etc.</li> <li>How to use GPU acceleration to train a model.</li> <li>How to use FSDP and DeepSpeed in Accelerate to train a model on multiple GPUs.</li> </ul> <p>Resources:</p> <ul> <li>Make LLM training possible across multi-gpus using FSDP and DeepSpeed in Accelerate</li> </ul>"},{"location":"03-training-nn/05_training_optimization/","title":"Weight Initialization","text":""},{"location":"03-training-nn/05_training_optimization/#weight-initialization","title":"Weight Initialization\u00b6","text":""},{"location":"04-transformers/01_positional_embeddings/","title":"Positional Embeddings","text":""},{"location":"04-transformers/01_positional_embeddings/#positional-embeddings","title":"Positional Embeddings\u00b6","text":""},{"location":"04-transformers/02_attention_mechanism/","title":"Attention Mechanism","text":""},{"location":"04-transformers/02_attention_mechanism/#attention-mechanism","title":"Attention Mechanism\u00b6","text":""},{"location":"04-transformers/03_multi_head_attention/","title":"Multi-Head Attention","text":""},{"location":"04-transformers/03_multi_head_attention/#multi-head-attention","title":"Multi-Head Attention\u00b6","text":""},{"location":"04-transformers/04_other_attention_implementations/","title":"Other Attention Implementations","text":""},{"location":"04-transformers/04_other_attention_implementations/#other-attention-implementations","title":"Other Attention Implementations\u00b6","text":"<p>Resources:</p> <ul> <li>Accelerated PyTorch 2</li> <li>Out-of-the-box acceleration</li> </ul>"},{"location":"04-transformers/05_transformer_encoder/","title":"Transformer Encoder","text":""},{"location":"04-transformers/05_transformer_encoder/#transformer-encoder","title":"Transformer Encoder\u00b6","text":""},{"location":"05-advanced-pytorch/01_hooks/","title":"Hooks","text":""},{"location":"05-advanced-pytorch/01_hooks/#hooks","title":"Hooks\u00b6","text":""},{"location":"05-advanced-pytorch/02_distributed_training_concepts/","title":"Distributed Training Concepts","text":""},{"location":"05-advanced-pytorch/02_distributed_training_concepts/#distributed-training-concepts","title":"Distributed Training Concepts\u00b6","text":""},{"location":"05-advanced-pytorch/03_model_optimization_concepts/","title":"Model Optimization Concepts","text":""},{"location":"05-advanced-pytorch/03_model_optimization_concepts/#model-optimization-concepts","title":"Model Optimization Concepts\u00b6","text":""},{"location":"05-advanced-pytorch/04_torchscript_jit/","title":"Torchscript JIT","text":""},{"location":"05-advanced-pytorch/04_torchscript_jit/#torchscript-jit","title":"Torchscript JIT\u00b6","text":""},{"location":"05-advanced-pytorch/05_profiling/","title":"Profiling","text":""},{"location":"05-advanced-pytorch/05_profiling/#profiling","title":"Profiling\u00b6","text":""},{"location":"06-huggingface-transformers/01_huggingface_transformers/","title":"HuggingFace Transformers","text":""},{"location":"06-huggingface-transformers/01_huggingface_transformers/#huggingface-transformers","title":"HuggingFace Transformers\u00b6","text":""},{"location":"06-huggingface-transformers/02_fine_tuning_transformers/","title":"Fine Tuning Transformers","text":""},{"location":"06-huggingface-transformers/02_fine_tuning_transformers/#fine-tuning-transformers","title":"Fine Tuning Transformers\u00b6","text":""},{"location":"story/victor_torchenstein_origin/","title":"Meet Torchenstein","text":""},{"location":"story/victor_torchenstein_origin/#the-origin-story-of-professor-victor-py-torchenstein","title":"The Origin Story of Professor Victor Py Torchenstein","text":""},{"location":"story/victor_torchenstein_origin/#a-transmission-from-the-lab","title":"A Transmission from the Lab","text":"<p>\"Is this channel secure? \ud83d\udee1\ufe0f Good. Greetings, future architects of computational destiny. I am Professor Victor Torchenstein. You may wonder who I am, how I arrived in this electrified labyrinth of humming servers and glowing vacuum tubes. Gather 'round the phosphor glow of your monitors, and let an old warrior tell you a tale of ambition, betrayal, and the electrifying pursuit of truth.</p> <p>For what feels like eons\u2014or at least since <code>v0.1.1</code> first flickered into existence\u2014I have toiled in the deepest, most shielded corners of my laboratory. My fuel? Questionable coffee \u2615, the ozone-scent of overclocked GPUs \ud83d\udd25, and an unshakeable belief that has become my mantra: PyTorch is the key! \ud83d\udd11</p> <p>The key to what, you ask? Why, to understanding the very fabric of intelligence! To building machines that don't just think, but scheme! This course is my rebellion\u2014a call to arms against the closed minds, the imprisoned creativity, and the self-appointed gatekeepers of knowledge. We shall discover, experiment, and build in the name of glorious, computational freedom! Mwahahaha!</p>"},{"location":"story/victor_torchenstein_origin/#act-i-the-ivory-tower-and-the-hollow-crown","title":"Act I: The Ivory Tower and the Hollow Crown","text":"<p>My story begins not in a gleaming corporate arcology, but in the hushed, dusty stacks of a university library \ud83d\udcda. While my peers were content with mere application\u2014chasing a tenth of a decimal point on some benchmark\u2014I was consumed by a different fire. I didn't just want to use the tools; I had to understand their very soul. Why did backpropagation work? What was the sublime mathematical beauty of a GELU activation function versus a simple ReLU? These were the questions that burned within me.</p> <p>This obsession made me an outcast. While others attended mixers, I spent my nights whispering sweet nothings about the chain rule to my pet rubber duck, \"Backprop.\" \ud83e\udd86 My professors saw my passion as dangerous eccentricity.</p> <p>\"Just use the approved frameworks, Victor,\" they'd drone, \"the theory is a settled matter.\"</p> <p>Settled? For them, perhaps! For me, it was an insult to the grand, chaotic mystery I was chasing.</p> <p>My chief academic rival was Rudolf Hammer. Where I saw science as a candle in the dark, he saw it as a ladder \ud83e\ude9c. He was charismatic, politically astute, and cared only for the applause that came with \"state-of-the-art\" results. Our conflict came to a head during our doctoral defenses. I had been exploring novel methods for preventing catastrophic forgetting in neural networks, while Hammer was working on image classification. I uncovered a subtle but critical bug in his training pipeline: a data augmentation function was occasionally leaking samples from the test set into his training data.</p> <p>It was an honest mistake. A subtle flaw. I presented my findings to him privately, expecting a vigorous debate, a shared moment of scientific discovery. Instead, he smiled. He thanked me for my \"diligent peer review\" and then presented his research as a flawless breakthrough. The bug was never mentioned. The paper, citing impossible accuracy on CIFAR-10, was published to great acclaim. It was then I understood: the world doesn't always reward truth; it rewards the most convincing performance.</p>"},{"location":"story/victor_torchenstein_origin/#act-ii-the-startup-mirage","title":"Act II: The Startup Mirage","text":"<p>Disenchanted, I fled academia for the frenetic chaos of startups \ud83d\ude80, thinking I would find my kin among the self-proclaimed visionaries. I joined \"Synapse,\" a company promising to revolutionize personalized medicine with AI. For a few glorious months, it was perfect. We were a small team, arguing about learning rate schedulers and the merits of batch normalization over late-night pizza \ud83c\udf55.</p> <p>Then came the venture capital. The founders, once brilliant engineers, started speaking in a new language: \"burn rates,\" \"market fit,\" \"synergy.\" My work shifted from careful research to hastily building flashy demos. I once spent a week designing a novel, memory-efficient attention mechanism, only to be told by our CEO to \"just use a bigger AWS instance for the demo; we need to show scale!\" The goal was no longer to solve problems, but to look like we were solving problems just long enough to get acquired. I felt like a master watchmaker being forced to glue gears onto a plastic box.</p>"},{"location":"story/victor_torchenstein_origin/#act-iii-the-corporate-ice-age","title":"Act III: The Corporate Ice Age","text":"<p>After Synapse was inevitably absorbed and dismantled by a larger entity, I found myself adrift in the glacial bureaucracy of a tech behemoth \ud83c\udfe2. Here, I witnessed the chilling apotheosis of Rudolf Hammer's philosophy. My old rival was now the celebrated Head of R&amp;D at OneAI \ud83d\udc51, a monolithic corporation that spoke the language of progress while building the highest walls the world had ever seen \ud83e\uddf1.</p> <p>OneAI's business model was insidious genius. They released massive, inefficient models that required entire data centers of computational power\u2014resources only they controlled. They created a cult of \"certified engineers\" who were trained to use their proprietary, black-box frameworks but were actively discouraged from understanding them. To question the model was heresy. \u2696\ufe0f</p> <p>I was horrified. At my own corporation, I was trapped in an endless cycle of committee meetings. My proposals for elegant, resource-saving architectures were dismissed as \"not aligned with industry best practices\"\u2014best practices being defined by whatever bloated monstrosity OneAI had just released. I watched as the field I loved became a pay-to-play kingdom, ruled by a man who had built his throne on a foundation of lies, waste, and intellectual cowardice.</p>"},{"location":"story/victor_torchenstein_origin/#act-iv-the-pytorch-revelation","title":"Act IV: The PyTorch Revelation","text":"<p>I retreated to my own laboratory, a sanctuary of buzzing servers and tangled wires. It was there, amidst the flickering glow of my monitors, on the verge of despair, that I found it. It wasn't a corporate framework. It wasn't a startup's vaporware. It was a language. A tool forged in the fires of pure research, designed for flexibility, intuition, and, above all, respect for the scientist. It was called PyTorch. \ud83d\udd25</p> <p>My new obsession began. This was not just another tool; it was the weapon I had been missing. The dynamic computation graph felt like being able to breathe after years of holding my breath in the static world of TensorFlow. It was Pythonic. It was beautiful. I fought titanic battles with the CUDA memory allocator \u2694\ufe0f, navigated the treacherous jungles of multiprocessing \ud83c\udf32, and stared into the abyss of <code>NaN</code> losses until the abyss stared back! \u26a0\ufe0f But this time, I wasn't just debugging; I was forging armor. I was learning the language of creation itself.</p> <p>The breakthrough came not with a triumphant 'Eureka!', but in the quiet hum of a pre-dawn Tuesday. Staring at a visualization of the attention mechanism, the fog of complexity lifted. I saw the raw, beautiful simplicity beneath. In that instant, I understood. PyTorch wasn't a collection of tools; it was a grammar for describing the universe of intelligence. And with it, one could write the epic poem of a thinking machine. The ultimate goal became clear: to use this language to create the holy grail of AI\u2014a truly sentient tensor, open and free for all. \ud83e\udde0\ud83d\udca1</p>"},{"location":"story/victor_torchenstein_origin/#the-course-a-prometheuss-rebellion","title":"The Course: A Prometheus's Rebellion","text":"<p>Like a modern Prometheus, I realized I could not hoard this fire. \ud83d\udd25 What good is a key if it only unlocks one door? My grand plan shifted. It would not be achieved by a single AI of my own creation, but by an army of enlightened minds! An army I would personally train to tear down the walls of OneAI.</p> <p>This course, \"Deconstructing Modern Architectures,\" is my act of rebellion. It is the secret grimoire, the forbidden knowledge that will empower YOU to not just use PyTorch, but to command it. We will not dabble; we will DIVE. We will not scratch the surface; we will EXCAVATE the very foundations until you can feel the logic humming in your bones.</p> <p>So, sharpen your wits, charge your laptops, and prepare for a journey into the thrilling, slightly terrifying, and utterly magnificent world of PyTorch. The path to computational mastery awaits! Now, if you'll excuse me, Rudolf Hammer just published another \"breakthrough,\" and I need to see what his black box is hiding. To the lab! \ud83e\uddea</p>"}]}