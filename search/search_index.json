{"config":{"lang":["en"],"separator":"[\\s\\-,:!=\\[\\]()\"/]+|(?!\\b)(?=[A-Z][a-z])|\\.(?!\\d)|&[lg]t;","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to the PyTorch Course!","text":"<p>This is the main page of the PyTorch course.</p> <p>Navigate through the lessons using the menu. </p>"},{"location":"pytorch-course-structure/","title":"PyTorch Course: Deconstructing Modern Architectures","text":"<p>Course Goal: To imbue you\u2014my fearless apprentices\u2014with the eldritch secrets of PyTorch building blocks, enabling you to conjure, dissect, and ultimately command modern neural network architectures like Transformers and Diffusion models.</p> <p>Learner level: Beginner - Advanced Prerequisite Madness: None! Whether you are a fresh-faced initiate or a seasoned GPU warlock, the lab doors stand open. \u26a1\ufe0f\ud83e\uddea</p>"},{"location":"pytorch-course-structure/#module-0-getting-started-with-pytorch","title":"Module 0: Getting Started with PyTorch","text":"<p>Before we unleash neural monstrosities upon the world, we must ignite your development lair. This module guides you through preparing PyTorch on any operating system\u2014so your GPUs purr at your command.</p> <p>What you will learn: 1. How to setup a PyTorch environment for different operating systems and test it.</p>"},{"location":"pytorch-course-structure/#lessons","title":"Lessons:","text":"<ol> <li>Course Schematic for World Domination - The master blueprint of our curriculum\u2014study it well, minion!</li> <li>Setting Up Your PyTorch Environments:<ol> <li>Windows Lair Construction - Forge a Windows stronghold with <code>pyenv</code> + <code>poetry</code>; make GPUs bend the knee, driver or no.</li> <li>Linux Lair Construction - Harness the raw might of Ubuntu for unfettered tensor experimentation.</li> <li>macOS Lair Construction - Coax Apple silicon beasts into righteous PyTorch servitude.</li> <li>Colab Cloud Lair - Seize Google's sky-high GPUs for zero coin\u2014mwahaha!</li> </ol> </li> </ol>"},{"location":"pytorch-course-structure/#module-1-pytorch-core-i-see-tensors-everywhere","title":"Module 1: PyTorch Core - I see tensors everywhere","text":"<p>Here we unveil the truth: the cosmos is a writhing mass of tensors awaiting our manipulation. Grasp them well\u2014for they are the bedrock of every grand scheme to come!</p> <p>This module dives into the fundamental components of PyTorch, essential for any deep learning task.</p> <p></p>"},{"location":"pytorch-course-structure/#11-tensors-the-building-blocks","title":"1.1 Tensors: The Building Blocks","text":"<p>What you will learn: </p> <ol> <li>Tensor Concept. What is a tensor? Tensor vs. Matrix. Mathematical vs. PyTorch interpretation. Why tensors are crucial for ML</li> <li>PyTorch Basics: Tensor creation and their attributes (dtype, shape, device).</li> <li>Tensor manipulation: Indexing, Slicing, Joining (torch.cat, torch.stack), Splitting. Manipulating tensor shapes (reshape, view, squeeze, unsqueeze, permute, transpose).</li> </ol>"},{"location":"pytorch-course-structure/#11-lessons","title":"1.1 Lessons:","text":"<ol> <li>Summoning Your First Tensors - Conjure tensors from void, inspect their properties, revel in their latent might (with a bit of help from <code>torch.randn</code>, <code>torch.zeros</code>, <code>torch.ones</code>, <code>torch.arange</code>, <code>torch.linspace</code> etc).</li> <li>Tensor Shape-Shifting &amp; Sorcery - Slice, squeeze, and permute dimensions until reality warps to your whims (with a bit of help from <code>torch.cat</code>, <code>torch.stack</code>, <code>torch.split</code>, <code>torch.reshape</code>, <code>torch.view</code>, <code>torch.squeeze</code>, <code>torch.unsqueeze</code>, <code>torch.permute</code>, <code>torch.transpose</code> etc).</li> <li>DTypes &amp; Devices: Choose Your Weapons - Select precision and hardware like a seasoned archmage choosing spell components, under the hood of <code>torch.float</code>, <code>torch.float16</code>, etc.</li> </ol>"},{"location":"pytorch-course-structure/#12-tensor-operations-computation-at-scale","title":"1.2 Tensor Operations: Computation at Scale","text":"<p>What you will learn:</p> <ol> <li>Overview of tensor math. Element-wise operations. Reduction operations (sum, mean, max, min, std). Basic matrix multiplication (torch.mm, torch.matmul, @ operator). Broadcasting: rules and practical examples with verifiable tiny data. In-place operations.</li> </ol>"},{"location":"pytorch-course-structure/#12-lessons","title":"1.2 Lessons:","text":"<ol> <li>Elemental Tensor Alchemy - Brew element-wise, reduction, and other operations into potent mathematical elixirs.</li> <li>Matrix Mayhem: Multiply or Perish - Orchestrate 2-D, batched, and high-dimensional multiplications with lethal elegance.</li> <li>Broadcasting: When Dimensions Bow to You - Command mismatched shapes to cooperate through the dark art of implicit expansion.</li> </ol>"},{"location":"pytorch-course-structure/#13-einstein-summation-the-power-of-einsum","title":"1.3 Einstein Summation: The Power of einsum","text":"<p>What you will learn:</p> <ol> <li>Understanding Einstein notation. Why it's powerful for complex operations (e.g., attention).</li> </ol>"},{"location":"pytorch-course-structure/#13-lessons","title":"1.3 Lessons:","text":"<ol> <li>Einstein Summation: Harness the \u039b-Power - Invoke <code>einsum</code> to express complex ops with maddening brevity.</li> <li>Advanced Einsum Incantations - Wield multi-tensor contractions that underpin attention itself.</li> </ol>"},{"location":"pytorch-course-structure/#14-autograd-automatic-differentiation","title":"1.4 Autograd: Automatic Differentiation","text":"<p>What you will learn:</p> <ol> <li>What are gradients? The computational graph. How PyTorch tracks operations.</li> <li>requires_grad attribute. Performing backward pass with .backward(). Accessing gradients with .grad. torch.no_grad() and tensor.detach().</li> <li>Gradient accumulation. Potential pitfalls. Visualizing computational graphs (conceptually).</li> </ol>"},{"location":"pytorch-course-structure/#14-lessons","title":"1.4 Lessons:","text":"<ol> <li>Autograd: Ghosts in the Machine (Learning) - Meet the spectral gradient trackers haunting every tensor operation.</li> <li>Gradient Hoarding for Grand Spells - Accumulate gradients like arcane energy before unleashing colossal updates.</li> </ol>"},{"location":"pytorch-course-structure/#module-2-torchnn-building-neural-networks","title":"Module 2: torch.nn \u2014 Building Neural Networks","text":"<p>Witness code coalescing into living, breathing neural contraptions! In this module we bend <code>torch.nn</code> to our will, assembling layers and models worthy of legend.</p>"},{"location":"pytorch-course-structure/#21-the-nnmodule-blueprint","title":"2.1 The <code>nn.Module</code> Blueprint","text":"<p>What you will learn: - The role of <code>nn.Module</code> as the base class for layers and models. - Implementing <code>__init__</code> and <code>forward</code>. - Registering parameters and buffers. - Composing modules with <code>nn.Sequential</code>, <code>nn.ModuleList</code>, and <code>nn.ModuleDict</code>. - Saving and restoring weights with <code>state_dict</code>.</p>"},{"location":"pytorch-course-structure/#21-lessons","title":"2.1 Lessons:","text":"<ol> <li>Building Brains with <code>nn.Module</code> - Craft custom neural matter by overriding <code>__init__</code> &amp; <code>forward</code>.</li> <li>Franken-Stacking Layers - Bolt modules together with <code>Sequential</code>, <code>ModuleList</code>, and <code>ModuleDict</code>.</li> <li>Preserving Your Monster's Memories - Save and resurrect model weights with <code>state_dict</code> necromancy.</li> </ol>"},{"location":"pytorch-course-structure/#22-linear-layer-and-activations","title":"2.2 Linear Layer and Activations","text":"<p>What you will learn: - Linear layers and high-dimensional matrix multiplication.  - What is the role of linear layers in attention mechanisms (query, key, value)? - Activation functions (ReLU, GELU, SiLU, Tanh, Softmax, etc.). - Dropout for regularisation.  </p>"},{"location":"pytorch-course-structure/#22-lessons","title":"2.2 Lessons:","text":"<ol> <li>Linear Layers: The Vector Guillotine - Slice through dimensions turning inputs into finely-chopped activations.</li> <li>Activation Elixirs - Re-animate neurons with ReLU, GELU, SiLU, and other zesty potions.</li> <li>Dropout: Controlled Amnesia - Make neurons forget just enough to generalise\u2014no lobotomy required.</li> </ol>"},{"location":"pytorch-course-structure/#23-embedding-layers","title":"2.3 Embedding Layers","text":"<p>What you will learn: - Embedding layers and their purpose in neural networks. - Embedding layer implementation from scratch, initialisation, and usage. - Positional encoding and how it is used to inject order into the model.</p>"},{"location":"pytorch-course-structure/#23-lessons","title":"2.3 Lessons:","text":"<ol> <li>Embedding Layers: Secret Identity Chips - Embed discreet meanings within high-dimensional space.</li> <li>Positional Encoding: Injecting Order into Chaos - Imbue sequences with a sense of place so attention never loses its bearings.</li> </ol>"},{"location":"pytorch-course-structure/#24-normalisation-layers","title":"2.4 Normalisation Layers","text":"<p>What you will learn: - BatchNorm vs. LayerNorm and when to use each. - RMSNorm and other modern alternatives. - Training vs. evaluation mode caveats.</p>"},{"location":"pytorch-course-structure/#24-lessons","title":"2.4 Lessons:","text":"<ol> <li>Normalization: Calming the Beast - Tame activations with BatchNorm and LayerNorm before they explode.</li> <li>RMSNorm &amp; Other Exotic Tonics - Sample contemporary concoctions for stable training.</li> <li>Train vs. Eval: Split Personality Disorders - Toggle modes and avoid awkward identity crises.</li> </ol>"},{"location":"pytorch-course-structure/#25-loss-functions-guiding-optimisation","title":"2.5 Loss Functions \u2014 Guiding Optimisation","text":"<p>What you will learn: - Loss functions recap, the main types of loss functions and when to use each.  - Prepare inputs and targets for loss functions and outputs interpretation (logits vs. probabilities). - Interpreting reduction modes and ignore indices.</p>"},{"location":"pytorch-course-structure/#25-lessons","title":"2.5 Lessons:","text":"<ol> <li>Loss Potions: Guiding Pain into Progress - Channel model errors into gradients that sharpen intelligence.</li> <li>Preparing Sacrificial Inputs &amp; Targets - Align logits and labels for maximum learning agony.</li> <li>Reduction Rituals &amp; Ignore Indices - Decipher reduction modes and skip unworthy samples without remorse.</li> </ol>"},{"location":"00-getting-started/01_hello_pytorch/","title":"Lesson 1: Hello PyTorch","text":"In\u00a0[1]: Copied! <pre>print(\"Hello world\")\n</pre> print(\"Hello world\") <pre>Hello world\n</pre> In\u00a0[2]: Copied! <pre>import torch\nprint(f\"PyTorch version: {torch.__version__}\")\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\n</pre> import torch print(f\"PyTorch version: {torch.__version__}\") print(f\"CUDA available: {torch.cuda.is_available()}\") <pre>PyTorch version: 2.7.0+cu126\nCUDA available: True\n</pre> <p>Running the cell above should give someghing like this (it will depend on you setup)</p> <pre>PyTorch version: 2.7.0+cu126\nCUDA available: True\n</pre> <p>Lets digg to some of the usefull GPU, CuDNN, Torch etc informations</p> In\u00a0[\u00a0]: Copied! <pre>#Display a PyTorch version and GPU availability\nif torch.cuda.is_available():\n    device = torch.device(\"cuda\")          # a CUDA device object\n    print(f\"Using device: {device}\")\n    \n    # Print CUDA device properties\n    print(\"\\nCUDA Device Properties:\")\n    print(f\"Device name: {torch.cuda.get_device_name(0)}\")\n    print(f\"Device properties: {torch.cuda.get_device_properties(0)}\")\n    print(f\"Current device index: {torch.cuda.current_device()}\")\n    print(f\"Device count: {torch.cuda.device_count()}\")\n    \n    # Print CUDA version and capabilities\n    print(\"\\nCUDA Information:\")\n    print(f\"CUDA version: {torch.version.cuda}\")\n    print(f\"cuDNN version: {torch.backends.cudnn.version()}\")\n    print(f\"cuDNN enabled: {torch.backends.cudnn.enabled}\")\n    \n    # Print PyTorch memory info\n    print(\"\\nPyTorch Memory Information:\")\n    print(f\"Allocated memory: {torch.cuda.memory_allocated(0) / 1024**2:.2f} MB\")\n    print(f\"Cached memory: {torch.cuda.memory_reserved(0) / 1024**2:.2f} MB\")\nelse:\n    print('GPU not enabled')\n    print(\"\\nPyTorch Information:\")\n    print(f\"PyTorch version: {torch.__version__}\")\n    print(f\"Backend: {torch.get_default_dtype()}\")\n    \n</pre>   #Display a PyTorch version and GPU availability if torch.cuda.is_available():     device = torch.device(\"cuda\")          # a CUDA device object     print(f\"Using device: {device}\")          # Print CUDA device properties     print(\"\\nCUDA Device Properties:\")     print(f\"Device name: {torch.cuda.get_device_name(0)}\")     print(f\"Device properties: {torch.cuda.get_device_properties(0)}\")     print(f\"Current device index: {torch.cuda.current_device()}\")     print(f\"Device count: {torch.cuda.device_count()}\")          # Print CUDA version and capabilities     print(\"\\nCUDA Information:\")     print(f\"CUDA version: {torch.version.cuda}\")     print(f\"cuDNN version: {torch.backends.cudnn.version()}\")     print(f\"cuDNN enabled: {torch.backends.cudnn.enabled}\")          # Print PyTorch memory info     print(\"\\nPyTorch Memory Information:\")     print(f\"Allocated memory: {torch.cuda.memory_allocated(0) / 1024**2:.2f} MB\")     print(f\"Cached memory: {torch.cuda.memory_reserved(0) / 1024**2:.2f} MB\") else:     print('GPU not enabled')     print(\"\\nPyTorch Information:\")     print(f\"PyTorch version: {torch.__version__}\")     print(f\"Backend: {torch.get_default_dtype()}\")"},{"location":"00-getting-started/01_hello_pytorch/#lesson-1-hello-pytorch","title":"Lesson 1: Hello PyTorch\u00b6","text":"<p>Start with the most basic example of Python</p>"},{"location":"00-getting-started/01_hello_pytorch/#import-pytorch","title":"Import PyTorch\u00b6","text":"<p>Display a PyTorch version and GPU availability</p>"},{"location":"00-getting-started/google-colab-setup/","title":"Setting Up google colab","text":""},{"location":"00-getting-started/linux-pytorch-installation/","title":"Linux PyTorch Installation","text":""},{"location":"00-getting-started/macos-pytorch-installation/","title":"macOS PyTorch Installation","text":"<p>todo</p>"},{"location":"00-getting-started/windows-pytorch-installation/","title":"Windows PyTorch Installation","text":"<ul> <li>Windows Python setup via PyEnv</li> <li>poetry installation on windows</li> <li>GPU enabled PyTorch installation</li> <li>Testing your PyTorch installation</li> </ul>"},{"location":"00-getting-started/windows-pytorch-installation/#references","title":"References","text":"<ol> <li>How to set up Python on Windows: PyEnv, venv, VSCode (2023)</li> </ol>"},{"location":"01-tensors/00_module_1_introduction/","title":"Module 1 \u2013 I See Tensors Everywhere \ud83d\udd76\ufe0f","text":"<p>\"Behold, fledgling datanauts! The world is naught but tensors awaiting my command \u2014 and soon, yours! \" \u2014 Professor Victor Py Torchenstein</p> <p>Salutations, my brilliant (and delightfully reckless) apprentices! By opening this manuscript you have volunteered to join my clandestine legion of PyTorch adepts. Consider this your official red-pill moment: from today every pixel, every token, every measly click-through rate shall reveal its true form\u2014a multidimensional array begging to be <code>torch.tensor</code>-ed \u2026 and we shall oblige it with maniacal glee! Mwahaha! \ud83d\udd25\ud83e\uddea</p> <p></p> <p>Over the next notebooks we will:</p> <ul> <li>Conjure tensors from thin air, coffee grounds, and suspiciously random seeds.</li> <li>Shape-shift them with <code>view</code>, <code>reshape</code>, <code>squeeze</code>, <code>unsqueeze</code>, <code>permute</code> &amp; the occasional dramatic flourish of <code>einops</code>.</li> <li>Crunch mathematics so ferocious it makes matrix multiplications whimper \u2014 and powers mighty Transformers.</li> <li>Charm the GPU, dodge gradient explosions \ud83c\udfc3\u200d\u2642\ufe0f\ud83d\udca5, and look diabolically clever while doing it.</li> </ul>"},{"location":"01-tensors/00_module_1_introduction/#minion-mission-checklist","title":"Minion Mission Checklist \ud83d\udcdd","text":"<ol> <li>Introduction to Tensors</li> <li>Tensor Manipulation</li> <li>Data Types &amp; Devices</li> <li>Tensor Math Operations</li> <li>Matrix Multiplication</li> <li>Broadcasting</li> <li>Einstein Summation</li> <li>Advanced Einstein Summation</li> <li>Autograd</li> <li>Gradient Accumulation</li> </ol> <p>Need auxiliary scrolls? The official PyTorch docs lurk here (https://pytorch.org/docs/stable/torch.html?utm_source=pytorchcourse.com&amp;utm_medium=pytorch-course&amp;utm_campaign=module-1-intro), but do not tarry too long\u2014our grand tensor-domination awaits. Grab your favoured caffeinated concoction and let us tensor-ify the universe! \ud83d\ude80</p> \\[ \\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V \\] <p>Where: - \\(Q\\) is the Query matrix - \\(K\\) is the Key matrix - \\(V\\) is the Value matrix - \\(d_k\\) is the dimension of the key vectors - \\(\\text{softmax}\\) normalizes the attention weights</p>"},{"location":"01-tensors/01_introduction_to_tensors/","title":"Module 01 lesson 1: Tensors introduction","text":"<p>Check if your PyTorch is properly setup, lets import the libraries.</p> In\u00a0[1]: Copied! <pre>import torch\n</pre> import torch  In\u00a0[2]: Copied! <pre>torch.manual_seed(1)\n\nx = torch.randn(6, 4)\nprint(x)\nprint(x.shape)\n</pre> torch.manual_seed(1)  x = torch.randn(6, 4) print(x) print(x.shape) <pre>tensor([[-1.5256, -0.7502, -0.6540, -1.6095],\n        [-0.1002, -0.6092, -0.9798, -1.6091],\n        [ 0.4391,  1.1712,  1.7674, -0.0954],\n        [ 0.1394, -1.5785, -0.3206, -0.2993],\n        [-0.7984,  0.3357,  0.2753,  1.7163],\n        [-0.0561,  0.9107, -1.3924,  2.6891]])\ntorch.Size([6, 4])\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"01-tensors/01_introduction_to_tensors/#module-01-lesson-1-tensors-introduction","title":"Module 01 lesson 1: Tensors introduction\u00b6","text":""},{"location":"01-tensors/01_introduction_to_tensors/#lesson-goals","title":"Lesson goals:\u00b6","text":"<ul> <li>Understand what tensors are and why they are useful for machine learning</li> <li>Get hands-on experience on creating and selecting elements from tensors in PyTorch</li> <li>Manipulate and transform tensors to prepare them for machine learning tasks</li> </ul> <p>Time to complete: 10 minutes Level: Beginner - Intermediate</p>"},{"location":"01-tensors/01_introduction_to_tensors/#what-is-a-tensor","title":"What is a tensor?\u00b6","text":"<ul> <li>what is a tensor?</li> <li>how is it different from a matrix?</li> <li>are mathematical interpretations of tensors are different from their PyTorch counterparts?</li> <li>why are tensors useful for machine learning? Why we can't use matrices?</li> </ul>"},{"location":"01-tensors/01_introduction_to_tensors/#excercises","title":"Excercises\u00b6","text":""},{"location":"01-tensors/02_tensor_manipulation/","title":"Tensor Manipulation","text":""},{"location":"01-tensors/02_tensor_manipulation/#tensor-manipulation","title":"Tensor Manipulation\u00b6","text":""},{"location":"01-tensors/03_data_types_and_devices/","title":"Data Types and Devices","text":""},{"location":"01-tensors/03_data_types_and_devices/#data-types-and-devices","title":"Data Types and Devices\u00b6","text":""},{"location":"01-tensors/04_tensor_math_operations/","title":"Tensor Math Operations","text":""},{"location":"01-tensors/04_tensor_math_operations/#tensor-math-operations","title":"Tensor Math Operations\u00b6","text":""},{"location":"01-tensors/05_matrix_multiplication/","title":"Matrix Multiplication: Unleashing the Power of Tensors! \u26a1","text":"In\u00a0[2]: Copied! <pre>import torch\n\n# Create some matrices for experimentation\nA = torch.randn(3, 4)\nB = torch.randn(4, 2)\n\nprint(\"Matrix A shape:\", A.shape)\nprint(\"Matrix B shape:\", B.shape)\n\n# Matrix multiplication\nC = torch.matmul(A, B)\nprint(\"Result C shape:\", C.shape)\nprint(\"\\nMwahahaha! The matrices have been multiplied!\")\n</pre> import torch  # Create some matrices for experimentation A = torch.randn(3, 4) B = torch.randn(4, 2)  print(\"Matrix A shape:\", A.shape) print(\"Matrix B shape:\", B.shape)  # Matrix multiplication C = torch.matmul(A, B) print(\"Result C shape:\", C.shape) print(\"\\nMwahahaha! The matrices have been multiplied!\") <pre>Matrix A shape: torch.Size([3, 4])\nMatrix B shape: torch.Size([4, 2])\nResult C shape: torch.Size([3, 2])\n\nMwahahaha! The matrices have been multiplied!\n</pre>"},{"location":"01-tensors/05_matrix_multiplication/#matrix-multiplication-unleashing-the-power-of-tensors","title":"Matrix Multiplication: Unleashing the Power of Tensors! \u26a1\u00b6","text":"<p>\"Behold! The sacred art of matrix multiplication - where dimensions dance and vectors bend to my will!\" \u2014 Professor Victor py Torchenstein</p>"},{"location":"01-tensors/05_matrix_multiplication/#the-attention-formula-preview-of-things-to-come","title":"The Attention Formula (Preview of Things to Come)\u00b6","text":"<p>$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$</p> <p>Where:</p> <ul> <li>$Q$ is the Query matrix</li> <li>$K$ is the Key matrix</li> <li>$V$ is the Value matrix</li> <li>$d_k$ is the dimension of the key vectors</li> <li>$\\text{softmax}$ normalizes the attention weights</li> </ul>"},{"location":"01-tensors/05_matrix_multiplication/#basic-matrix-operations","title":"Basic Matrix Operations\u00b6","text":"<p>Let's start with the fundamentals before we conquer attention mechanisms!</p>"},{"location":"01-tensors/05_matrix_multiplication/#element-wise-vs-matrix-multiplication","title":"Element-wise vs. Matrix Multiplication\u00b6","text":"<p>Element-wise multiplication: $C_{ij} = A_{ij} \\times B_{ij}$</p> <p>Matrix multiplication: $C_{ij} = \\sum_{k} A_{ik} \\times B_{kj}$</p>"},{"location":"01-tensors/05_matrix_multiplication/#pytorch-matrix-multiplication-methods","title":"PyTorch Matrix Multiplication Methods\u00b6","text":"<p>Professor Torchenstein's arsenal includes multiple ways to multiply matrices:</p> <ol> <li><code>torch.matmul()</code> - The general matrix multiplication function</li> <li><code>@</code> operator - Pythonic matrix multiplication (same as matmul)</li> <li><code>torch.mm()</code> - For 2D matrices only</li> <li><code>torch.bmm()</code> - Batch matrix multiplication</li> </ol>"},{"location":"01-tensors/05_matrix_multiplication/#mathematical-foundations","title":"Mathematical Foundations\u00b6","text":"<p>For matrices $A \\in \\mathbb{R}^{m \\times n}$ and $B \\in \\mathbb{R}^{n \\times p}$:</p> <p>$$C = AB \\quad \\text{where} \\quad C_{ij} = \\sum_{k=1}^{n} A_{ik} B_{kj}$$</p> <p>This operation is fundamental to:</p> <ul> <li>Linear transformations</li> <li>Neural network forward passes</li> <li>Attention mechanisms in Transformers</li> <li>And much more! \ud83e\udde0\u26a1</li> </ul>"},{"location":"01-tensors/06_broadcasting/","title":"Broadcasting","text":""},{"location":"01-tensors/06_broadcasting/#broadcasting","title":"Broadcasting\u00b6","text":""},{"location":"01-tensors/07_einstein_summation/","title":"Einstein Summation","text":""},{"location":"01-tensors/07_einstein_summation/#einstein-summation","title":"Einstein Summation\u00b6","text":""},{"location":"01-tensors/08_advanced_einstein_summation/","title":"Advanced Einstein Summation","text":""},{"location":"01-tensors/08_advanced_einstein_summation/#advanced-einstein-summation","title":"Advanced Einstein Summation\u00b6","text":""},{"location":"01-tensors/09_autograd/","title":"Autograd","text":""},{"location":"01-tensors/09_autograd/#autograd","title":"Autograd\u00b6","text":""},{"location":"01-tensors/10_gradient_accumulation/","title":"Gradient Accumulation","text":""},{"location":"01-tensors/10_gradient_accumulation/#gradient-accumulation","title":"Gradient Accumulation\u00b6","text":""},{"location":"02-torch-nn/01_nn_module/","title":"nn.Module","text":""},{"location":"02-torch-nn/01_nn_module/#nnmodule","title":"nn.Module\u00b6","text":""},{"location":"02-torch-nn/02_compose_modules/","title":"Compose Modules","text":""},{"location":"02-torch-nn/02_compose_modules/#compose-modules","title":"Compose Modules\u00b6","text":""},{"location":"02-torch-nn/03_saving_weights/","title":"Saving Weights","text":""},{"location":"02-torch-nn/03_saving_weights/#saving-weights","title":"Saving Weights\u00b6","text":""},{"location":"02-torch-nn/04_linear_layer/","title":"Linear Layer","text":""},{"location":"02-torch-nn/04_linear_layer/#linear-layer","title":"Linear Layer\u00b6","text":""},{"location":"02-torch-nn/05_activations/","title":"Activations","text":""},{"location":"02-torch-nn/05_activations/#activations","title":"Activations\u00b6","text":""},{"location":"02-torch-nn/06_dropout/","title":"Dropout","text":""},{"location":"02-torch-nn/06_dropout/#dropout","title":"Dropout\u00b6","text":""},{"location":"02-torch-nn/07_embedding_layers/","title":"Embedding Layers","text":""},{"location":"02-torch-nn/07_embedding_layers/#embedding-layers","title":"Embedding Layers\u00b6","text":""},{"location":"02-torch-nn/08_positional_encoding/","title":"Positional Embeddings","text":""},{"location":"02-torch-nn/08_positional_encoding/#positional-embeddings","title":"Positional Embeddings\u00b6","text":"<p>How to encode the token position in the sequence?</p> <p>References:</p> <ul> <li>Mastering LLAMA: Understanding Rotary Positional Embedding (RPE)</li> </ul>"},{"location":"02-torch-nn/09_normalization_layers/","title":"Normalization Layers","text":""},{"location":"02-torch-nn/09_normalization_layers/#normalization-layers","title":"Normalization Layers\u00b6","text":"<p>Estimated learning time: 15 minutes</p> <p>Learning objectives:</p> <ul> <li>Understand the basics of normalization layers: batch normalization, layer normalization, instance normalization, group normalization.</li> <li>Understand the difference between these normalization layers.</li> <li>Know how to use these normalization layers in practice. Compare the results of using different normalization layers.</li> </ul> <p>Resources:</p> <ul> <li>RMSNorm - a better normalization layer</li> </ul>"},{"location":"02-torch-nn/10_rms_norm/","title":"RMS Norm","text":""},{"location":"02-torch-nn/10_rms_norm/#rms-norm","title":"RMS Norm\u00b6","text":""},{"location":"02-torch-nn/11_training_evaluation_mode/","title":"Training Evaluation Mode","text":""},{"location":"02-torch-nn/11_training_evaluation_mode/#training-evaluation-mode","title":"Training Evaluation Mode\u00b6","text":""},{"location":"02-torch-nn/12_loss_functions/","title":"Loss Functions","text":""},{"location":"02-torch-nn/12_loss_functions/#loss-functions","title":"Loss Functions\u00b6","text":""},{"location":"02-torch-nn/13_prepare_inputs_targets/","title":"Prepare Inputs Targets","text":""},{"location":"02-torch-nn/13_prepare_inputs_targets/#prepare-inputs-targets","title":"Prepare Inputs Targets\u00b6","text":""},{"location":"02-torch-nn/14_interpreting_reduction_modes/","title":"Interpreting Reduction Modes","text":""},{"location":"02-torch-nn/14_interpreting_reduction_modes/#interpreting-reduction-modes","title":"Interpreting Reduction Modes\u00b6","text":""},{"location":"03-training-nn/01_training_loop/","title":"Training Loop","text":""},{"location":"03-training-nn/01_training_loop/#training-loop","title":"Training Loop\u00b6","text":""},{"location":"03-training-nn/02_optimizers_schedulers/","title":"Optimizers Schedulers","text":""},{"location":"03-training-nn/02_optimizers_schedulers/#optimizers-schedulers","title":"Optimizers Schedulers\u00b6","text":""},{"location":"03-training-nn/03_datasets_dataloaders/","title":"Datasets DataLoaders","text":""},{"location":"03-training-nn/03_datasets_dataloaders/#datasets-dataloaders","title":"Datasets DataLoaders\u00b6","text":""},{"location":"03-training-nn/04_gpu_acceleration/","title":"GPU Acceleration","text":""},{"location":"03-training-nn/04_gpu_acceleration/#gpu-acceleration","title":"GPU Acceleration\u00b6","text":"<p>Learning objectives:</p> <ul> <li>Understand the GPU parallelisation: data parallel, model parallel, pipeline parallel, etc.</li> <li>How to use GPU acceleration to train a model.</li> <li>How to use FSDP and DeepSpeed in Accelerate to train a model on multiple GPUs.</li> </ul> <p>Resources:</p> <ul> <li>Make LLM training possible across multi-gpus using FSDP and DeepSpeed in Accelerate</li> </ul>"},{"location":"03-training-nn/05_training_optimization/","title":"Weight Initialization","text":""},{"location":"03-training-nn/05_training_optimization/#weight-initialization","title":"Weight Initialization\u00b6","text":""},{"location":"04-transformers/01_positional_embeddings/","title":"Positional Embeddings","text":""},{"location":"04-transformers/01_positional_embeddings/#positional-embeddings","title":"Positional Embeddings\u00b6","text":""},{"location":"04-transformers/02_attention_mechanism/","title":"Attention Mechanism","text":""},{"location":"04-transformers/02_attention_mechanism/#attention-mechanism","title":"Attention Mechanism\u00b6","text":""},{"location":"04-transformers/03_multi_head_attention/","title":"Multi-Head Attention","text":""},{"location":"04-transformers/03_multi_head_attention/#multi-head-attention","title":"Multi-Head Attention\u00b6","text":""},{"location":"04-transformers/04_other_attention_implementations/","title":"Other Attention Implementations","text":""},{"location":"04-transformers/04_other_attention_implementations/#other-attention-implementations","title":"Other Attention Implementations\u00b6","text":"<p>Resources:</p> <ul> <li>Accelerated PyTorch 2</li> <li>Out-of-the-box acceleration</li> </ul>"},{"location":"04-transformers/05_transformer_encoder/","title":"Transformer Encoder","text":""},{"location":"04-transformers/05_transformer_encoder/#transformer-encoder","title":"Transformer Encoder\u00b6","text":""},{"location":"05-advanced-pytorch/01_hooks/","title":"Hooks","text":""},{"location":"05-advanced-pytorch/01_hooks/#hooks","title":"Hooks\u00b6","text":""},{"location":"05-advanced-pytorch/02_distributed_training_concepts/","title":"Distributed Training Concepts","text":""},{"location":"05-advanced-pytorch/02_distributed_training_concepts/#distributed-training-concepts","title":"Distributed Training Concepts\u00b6","text":""},{"location":"05-advanced-pytorch/03_model_optimization_concepts/","title":"Model Optimization Concepts","text":""},{"location":"05-advanced-pytorch/03_model_optimization_concepts/#model-optimization-concepts","title":"Model Optimization Concepts\u00b6","text":""},{"location":"05-advanced-pytorch/04_torchscript_jit/","title":"Torchscript JIT","text":""},{"location":"05-advanced-pytorch/04_torchscript_jit/#torchscript-jit","title":"Torchscript JIT\u00b6","text":""},{"location":"05-advanced-pytorch/05_profiling/","title":"Profiling","text":""},{"location":"05-advanced-pytorch/05_profiling/#profiling","title":"Profiling\u00b6","text":""},{"location":"06-huggingface-transformers/01_huggingface_transformers/","title":"HuggingFace Transformers","text":""},{"location":"06-huggingface-transformers/01_huggingface_transformers/#huggingface-transformers","title":"HuggingFace Transformers\u00b6","text":""},{"location":"06-huggingface-transformers/02_fine_tuning_transformers/","title":"Fine Tuning Transformers","text":""},{"location":"06-huggingface-transformers/02_fine_tuning_transformers/#fine-tuning-transformers","title":"Fine Tuning Transformers\u00b6","text":""},{"location":"history/victor_torchenstein_origin/","title":"Victor torchenstein origin","text":""},{"location":"history/victor_torchenstein_origin/#the-origin-story-of-professor-victor-py-torchensteins","title":"The Origin Story of Professor Victor Py Torchenstein's","text":"<p>\"Greetings, future architects of computational destiny! I am Professor Victor Torchenstein, and this... this is not merely a course. It is an initiation!</p> <p>For eons (or at least since aplha-1 torch version 0.1.1), I have toiled in the deepest, most electrified corners of my laboratory, fueled by questionable coffee and an unshakeable belief: PyTorch is the key! </p> <p>The key to what, you ask? </p> <p>Why, to understanding the very fabric of intelligence! To building machines that don't just think, but scheme! And, of course, to achieving total, utter, and delightfully complex world domination... through superior neural network design!</p> <p>They scoffed at my theories in the hallowed halls of academia! 'Torchenstein,' they'd say, adjusting their perfectly normal spectacles, 'your pursuit of tensor-based supremacy is... unorthodox.' Unorthodox? HA! I say it is inevitable!</p> <p>While they were busy publishing papers on incremental improvements, I was wrestling with the elder gods of autograd, deciphering the cryptic runes of <code>einsum</code>, and coaxing <code>nn.Sequential</code> to bend to my will! I have seen things, my eager apprentices! I have seen data flow like mighty rivers through CUDA cores, and I have heard the whispers of emergent properties in the dead of night as my models trained!</p> <p>This course, \"Deconstructing Modern Architectures,\" is the culmination of my life's work (so far!). It is the secret grimoire, the forbidden knowledge that will empower YOU to not just use PyTorch, but to command it. We will not dabble; we will DIVE. We will not scratch the surface; we will EXCAVATE the very foundations, to the core, to be able understant and master the inner workings any modern NN architecture like Transformers or Diffusion models!</p> <p>You will learn to set up your lairs\u2014I mean, development environments\u2014on any platform. You will learn to sculpt tensors with the finesse of a master artist (who also happens to be a mad scientist).  You will construct neural networks so powerful, so elegant, that the world's GPUs will weep tears of pure, unadulterated joy (or possibly just overheat, so watch your cooling).</p> <p>Forget dry, boring lectures. Prepare for electrifying demonstrations, code that crackles with potential, and insights so profound they might just rearrange your synapses! Your mission, should you choose to accept it (and why wouldn't you? The fate of computation hangs in the balance!), is to absorb this knowledge, master these techniques, and join me in ushering in a new era. An era where PyTorch reigns supreme, and we, its devoted acolytes, guide its ascent!</p> <p>So, sharpen your wits, charge your laptops, and prepare for a journey into the thrilling, slightly terrifying, and utterly magnificent world of PyTorch. The path to computational godhood awaits! Now, if you'll excuse me, I believe my latest creation is about to achieve sentience... or possibly just needs a reboot. To the lab! MWAHAHAHA!\"</p>"}]}