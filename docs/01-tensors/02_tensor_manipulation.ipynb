{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f646b49b",
   "metadata": {},
   "source": [
    "# Tensor Shape-Shifting & Sorcery"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36fcb59e",
   "metadata": {},
   "source": [
    "**Module 1 | Lesson 2**\n",
    "\n",
    "---\n",
    "\n",
    "## Professor Torchenstein's Grand Directive\n",
    "\n",
    "Mwahahaha! You have summoned your first tensors from the ether! They are... raw. Untamed. Clumps of numerical clay awaiting a master's touch. A lesser mind would be content with their existence, but not you. Not us!\n",
    "\n",
    "Today, we sculpt! We will learn the arcane arts of tensor manipulation. We will not merely *use* data; we will bend it, twist it, and reshape it to our will until it confesses its secrets. This is not data processing; this is **tensor sorcery**! Prepare to command the very dimensions of your data!\n",
    "\n",
    "![pytorch tensors everywhere](/assets/images/torchenstein_working_computer.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b33d1d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Your Mission Briefing\n",
    "\n",
    "By the time you escape this chamber of knowledge, you will have etched the following incantations into your very soul:\n",
    "\n",
    "*   **The Art of Selection:** Pluck elements, rows, or slices from a tensor with masterful **slicing**.\n",
    "*   **Forbidden Fusions:** Combine disparate tensors into unified monstrosities with `torch.cat` and `torch.stack`.\n",
    "*   **Metamorphic Mastery:** Change a tensor's very form without altering its essence using `reshape` and `view`.\n",
    "*   **Dimensional Sorcery:** Add or remove dimensions at will with the mystical `squeeze` and `unsqueeze` commands.\n",
    "*   **The Grand Permutation:** Reorder dimensions to your strategic advantage with `permute` and `transpose`.\n",
    "\n",
    "**Estimated Time to Completion:** 20 minutes of exhilarating dimensional gymnastics.\n",
    "\n",
    "**What You'll Need:**\n",
    "*   The wisdom from our [last lesson on summoning tensors](../01_introduction_to_tensors.ipynb).\n",
    "*   A will of iron and a mind ready to be bent!\n",
    "*   Your PyTorch environment, humming with anticipation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8652e14f",
   "metadata": {},
   "source": [
    "## Part 1: The Art of Selection - Slicing\n",
    "\n",
    "Before you can reshape a tensor, you must learn to grasp its individual parts. Indexing is your scalpel, allowing you to perform precision surgery on your data. Slicing is your cleaver, letting you carve out whole sections for your grand experiments.\n",
    "\n",
    "We will start by summoning a test subjectâ€”a 2D tensor brimming with potential! We must also prepare our lab with the usual incantations (`import torch` and `manual_seed`) to ensure our results are repeatable. We are scientists, not chaos-wizards!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d0210a48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our subject tensor of shape torch.Size([5, 4]), ripe for dissection:\n",
      "tensor([[42, 67, 76, 14],\n",
      "        [26, 35, 20, 24],\n",
      "        [50, 13, 78, 14],\n",
      "        [10, 54, 31, 72],\n",
      "        [15, 95, 67,  6]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Set the seed for cosmic consistency\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Our test subject: A 2D tensor of integers. Imagine it's a map to a hidden treasure!\n",
    "# Or perhaps experimental results from a daring new potion.\n",
    "subject_tensor = torch.randint(0, 100, (5, 4))\n",
    "\n",
    "print(f\"Our subject tensor of shape {subject_tensor.shape}, ripe for dissection:\")\n",
    "print(subject_tensor)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e22d492f",
   "metadata": {},
   "source": [
    "### Sweeping Strikes: Accessing Rows and Columns\n",
    "\n",
    "Previous lesson: [01_introduction_to_tensors.ipynb](01_introduction_to_tensors.ipynb) gives you the basics for accessing element of a tensor.\n",
    "But what if we require an entire row or column for our dark machinations? For this, we use the colon `:`, the universal symbol for \"give me everything along this dimension!\"\n",
    "\n",
    "- `[row_index, :]` - Fetches the entire row.\n",
    "- `[:, column_index]` - Fetches the entire column.\n",
    "\n",
    "Let's seize the entire 3rd row (index 2) and the 2nd column (index 1).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91b34956",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The third row: tensor([50, 13, 78, 14])\n",
      "Shape of the row: torch.Size([4])\n",
      "\n",
      "The second column: tensor([67, 35, 13, 54, 95])\n",
      "Shape of the column: torch.Size([5])\n"
     ]
    }
   ],
   "source": [
    "# Get the entire 3rd row (index 2)\n",
    "third_row = subject_tensor[2, :] # or simply subject_tensor[2]\n",
    "print(f\"The third row: {third_row}\")\n",
    "print(f\"Shape of the row: {third_row.shape}\\n\")\n",
    "\n",
    "\n",
    "# Get the entire 2nd column (index 1)\n",
    "second_column = subject_tensor[:, 1]\n",
    "print(f\"The second column: {second_column}\")\n",
    "print(f\"Shape of the column: {second_column.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "933e7b12",
   "metadata": {},
   "source": [
    "### Carving Chunks: The Power of Slicing\n",
    "\n",
    "Mere elements are but trivialities! True power lies in carving out entire sub-regions of a tensor. Slicing uses the `start:end` notation. As with all Pythonic sorcery, the `start` is inclusive, but the `end` is **exclusive**.\n",
    "\n",
    "Let us carve out the block containing the 2nd and 3rd rows (indices 1 and 2), and the last two columns (indices 2 and 3).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd3e2f4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our carved sub-tensor:\n",
      "tensor([[20, 24],\n",
      "        [78, 14]])\n",
      "Shape of the sub-tensor: torch.Size([2, 2])\n"
     ]
    }
   ],
   "source": [
    "# Carve out rows 1 and 2, and columns 2 and 3\n",
    "sub_tensor = subject_tensor[1:3, 2:4]\n",
    "\n",
    "print(\"Our carved sub-tensor:\")\n",
    "print(sub_tensor)\n",
    "print(f\"Shape of the sub-tensor: {sub_tensor.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc94f52",
   "metadata": {},
   "source": [
    "### Conditional Conjuring: Boolean Mask Indexing\n",
    "\n",
    "Now for a truly diabolical technique! We can use a **boolean mask** to summon only the elements that meet our nefarious criteria. A boolean mask is a tensor of the same shape as our subject, but it contains only `True` or `False` values. When used for indexing, it returns a 1D tensor containing only the elements where the mask was `True`.\n",
    "\n",
    "Let's find all the alchemical ingredients in our tensor with a value greater than 50!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ac2ad0cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The boolean mask (True where value > 50):\n",
      "tensor([[False,  True,  True, False],\n",
      "        [False, False, False, False],\n",
      "        [False, False,  True, False],\n",
      "        [False,  True, False,  True],\n",
      "        [False,  True,  True, False]])\n",
      "\n",
      "Elements greater than 50:\n",
      "tensor([67, 76, 78, 54, 72, 95, 67])\n",
      "Shape of the result: torch.Size([7]) (always a 1D tensor!)\n",
      "\n",
      "Elements between 20 and 40:\n",
      "tensor([26, 35, 24, 31])\n"
     ]
    }
   ],
   "source": [
    "# Create the boolean mask\n",
    "mask = subject_tensor > 50\n",
    "\n",
    "print(\"The boolean mask (True where value > 50):\")\n",
    "print(mask)\n",
    "\n",
    "# Apply the mask\n",
    "selected_elements = subject_tensor[mask]\n",
    "\n",
    "print(\"\\nElements greater than 50:\")\n",
    "print(selected_elements)\n",
    "print(f\"Shape of the result: {selected_elements.shape} (always a 1D tensor!)\")\n",
    "\n",
    "# You can also combine conditions! Mwahaha!\n",
    "# Let's find elements between 20 and 40.\n",
    "mask_combined = (subject_tensor > 20) & (subject_tensor < 40)\n",
    "print(\"\\nElements between 20 and 40:\")\n",
    "print(subject_tensor[mask_combined])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a2baed",
   "metadata": {},
   "source": [
    "### Your Mission: The Slicer's Gauntlet\n",
    "\n",
    "Enough of my demonstrations! The scalpel is now in your hand. Prove your mastery with these challenges!\n",
    "\n",
    "1.  **The Corner Pocket:** From our `subject_tensor`, select the element in the very last row and last column.\n",
    "2.  **The Central Core:** Select the inner `3x2` block of the `subject_tensor` (that's rows 1-3 and columns 1-2).\n",
    "3.  **The Even Stevens:** Create a boolean mask to select only the elements in `subject_tensor` that are even numbers. (Hint: The modulo operator `%` is your friend!)\n",
    "4.  **The Grand Mutation:** Use your boolean mask from challenge 3 to **change** all even numbers in the `subject_tensor` to the value `-1`. Then, print the mutated tensor. Yes, my apprentice, indexing can be used for assignment! This is a pivotal secret!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0049a5d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1. The Corner Pocket ---\n",
      "The corner element is: 6\n",
      "\n",
      "--- 2. The Central Core ---\n",
      "The central core:\\ntensor([[35, 20],\n",
      "        [13, 78],\n",
      "        [54, 31]])\n",
      "\n",
      "--- 3. The Even Stevens ---\n",
      "The mask for even numbers:\\ntensor([[ True, False,  True,  True],\n",
      "        [ True, False,  True,  True],\n",
      "        [ True, False,  True,  True],\n",
      "        [ True,  True, False,  True],\n",
      "        [False, False, False,  True]])\n",
      "\n",
      "The even numbers themselves: tensor([42, 76, 14, 26, 20, 24, 50, 78, 14, 10, 54, 72,  6])\n",
      "\n",
      "--- 4. The Grand Mutation ---\n",
      "The tensor after mutating even numbers to -1:\n",
      "tensor([[-1, 67, -1, -1],\n",
      "        [-1, 35, -1, -1],\n",
      "        [-1, 13, -1, -1],\n",
      "        [-1, -1, 31, -1],\n",
      "        [15, 95, 67, -1]])\n"
     ]
    }
   ],
   "source": [
    "# Your code for the Slicer's Gauntlet goes here!\n",
    "\n",
    "# --- 1. The Corner Pocket ---\n",
    "print(\"--- 1. The Corner Pocket ---\")\n",
    "corner_element = subject_tensor[-1, -1] # Negative indexing for the win!\n",
    "print(f\"The corner element is: {corner_element.item()}\\n\")\n",
    "\n",
    "# --- 2. The Central Core ---\n",
    "print(\"--- 2. The Central Core ---\")\n",
    "central_core = subject_tensor[1:4, 1:3]\n",
    "print(f\"The central core:\\\\n{central_core}\\n\")\n",
    "\n",
    "# --- 3. The Even Stevens ---\n",
    "print(\"--- 3. The Even Stevens ---\")\n",
    "even_mask = subject_tensor % 2 == 0\n",
    "print(f\"The mask for even numbers:\\\\n{even_mask}\\n\")\n",
    "print(f\"The even numbers themselves: {subject_tensor[even_mask]}\\n\")\n",
    "\n",
    "\n",
    "# --- 4. The Grand Mutation ---\n",
    "print(\"--- 4. The Grand Mutation ---\")\n",
    "# Let's not mutate our original, that would be reckless! Let's clone it first.\n",
    "mutated_tensor = subject_tensor.clone()\n",
    "mutated_tensor[even_mask] = -1\n",
    "print(f\"The tensor after mutating even numbers to -1:\\n{mutated_tensor}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b0bf35",
   "metadata": {},
   "source": [
    "## Part 2: Forbidden Fusions - Joining Tensors\n",
    "\n",
    "Ah, but dissecting tensors is only half the art! A true master must also know how to **fuse** separate tensors into a single, magnificent whole. Sometimes your data comes in fragmentsâ€”perhaps different batches, different features, or different time steps. You must unite them!\n",
    "\n",
    "We have two primary spells for this dark ritual:\n",
    "- **`torch.cat()`** - The Concatenator! Joins tensors along an *existing* dimension.\n",
    "- **`torch.stack()`** - The Stacker! Creates a *new* dimension and stacks tensors along it.\n",
    "\n",
    "The difference is subtle but critical. Choose wrongly, and your creation will crumble! Let us forge some test subjects to demonstrate this power.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dea18dcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our test subjects, ready for fusion:\n",
      "Tensor A (shape torch.Size([2, 4])):\n",
      "tensor([[1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.]])\n",
      "\n",
      "Tensor B (shape torch.Size([2, 4])):\n",
      "tensor([[2., 2., 2., 2.],\n",
      "        [2., 2., 2., 2.]])\n",
      "\n",
      "Tensor C (shape torch.Size([2, 4])):\n",
      "tensor([[3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Three 2x3 tensors, our loyal minions awaiting fusion\n",
    "tensor_a = torch.ones(2, 4)\n",
    "tensor_b = torch.ones(2, 4) * 2\n",
    "tensor_c = torch.ones(2, 4) * 3\n",
    "\n",
    "print(\"Our test subjects, ready for fusion:\")\n",
    "print(f\"Tensor A (shape {tensor_a.shape}):\\n{tensor_a}\\n\")\n",
    "print(f\"Tensor B (shape {tensor_b.shape}):\\n{tensor_b}\\n\")\n",
    "print(f\"Tensor C (shape {tensor_c.shape}):\\n{tensor_c}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f7ebfb",
   "metadata": {},
   "source": [
    "### The Concatenator: `torch.cat()`\n",
    "\n",
    "`torch.cat()` joins tensors along an **existing dimension**. Think of it as gluing them end-to-end. \n",
    "\n",
    "The key rule: *All tensors must have the same shape, except along the dimension you're concatenating!*\n",
    "\n",
    "- `dim=0` (or `axis=0`): Concatenate along rows (vertically stack)\n",
    "- `dim=1` (or `axis=1`): Concatenate along columns (horizontally join)\n",
    "\n",
    "Let us witness this concatenation sorcery!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "dd117668",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concatenated along dimension 0 (rows) [stacking pancakes ðŸ¥žâ¬†ï¸â¬‡ï¸]:\n",
      "Result shape: torch.Size([6, 4])\n",
      "Result:\n",
      "tensor([[1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [2., 2., 2., 2.],\n",
      "        [2., 2., 2., 2.],\n",
      "        [3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.]])\n",
      "\n",
      "Concatenated along dimension 1 (columns) [laying bricks side by side ðŸ§±ðŸ§±ðŸ§±]:\n",
      "Result shape: torch.Size([2, 12])\n",
      "Result:\n",
      "tensor([[1., 1., 1., 1., 2., 2., 2., 2., 3., 3., 3., 3.],\n",
      "        [1., 1., 1., 1., 2., 2., 2., 2., 3., 3., 3., 3.]])\n"
     ]
    }
   ],
   "source": [
    "# Concatenating along dimension 0 (rows) - like stacking pancakes! ðŸ¥žâ¬†ï¸â¬‡ï¸\n",
    "cat_dim0 = torch.cat([tensor_a, tensor_b, tensor_c], dim=0)\n",
    "print(\"Concatenated along dimension 0 (rows) [stacking pancakes ðŸ¥žâ¬†ï¸â¬‡ï¸]:\")\n",
    "print(f\"Result shape: {cat_dim0.shape}\")\n",
    "print(f\"Result:\\n{cat_dim0}\\n\")\n",
    "\n",
    "# Concatenating along dimension 1 (columns) - like laying bricks side by side! ðŸ§±ðŸ§±ðŸ§±\n",
    "cat_dim1 = torch.cat([tensor_a, tensor_b, tensor_c], dim=1)\n",
    "print(\"Concatenated along dimension 1 (columns) [laying bricks side by side ðŸ§±ðŸ§±ðŸ§±]:\")\n",
    "print(f\"Result shape: {cat_dim1.shape}\")\n",
    "print(f\"Result:\\n{cat_dim1}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4e4c43",
   "metadata": {},
   "source": [
    "### The Concatenation Rules: When Shapes Don't Match\n",
    "\n",
    "Now, let us test the fundamental law of concatenation with unequal tensors! Remember: **All tensors must have the same shape, except along the dimension you're concatenating.**\n",
    "\n",
    "Eg1. If you joining 2D matrices along rows (dim=0) the number of collumns should be the same. \n",
    "\n",
    "Let's create two tensors with different shapes and see what happens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ab05e2af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wide tensor [big cake ðŸŽ‚] (shape torch.Size([3, 8])):\n",
      "tensor([[4., 4., 4., 4., 4., 4., 4., 4.],\n",
      "        [4., 4., 4., 4., 4., 4., 4., 4.],\n",
      "        [4., 4., 4., 4., 4., 4., 4., 4.]])\n",
      "\n",
      "Narrow tensor [small cupcake ðŸ§ ] (shape torch.Size([3, 2])):\n",
      "tensor([[5., 5.],\n",
      "        [5., 5.],\n",
      "        [5., 5.]])\n",
      "\n",
      "âŒ Attempting to concatenate along dimension 0 (rows), stack cake on top of cupcake ...\n",
      "ðŸŽ‚/ðŸ§ This couldn't work! \n",
      "Error as expected: Sizes of tensors must match except in dimension 0. Expected size 8 but got size 2 for tensor number 1 in the list.\n",
      "Our unequal test subjects:\n",
      "Wide tensor [big cake ðŸŽ‚] (torch.Size([3, 8])):\n",
      "tensor([[4., 4., 4., 4., 4., 4., 4., 4.],\n",
      "        [4., 4., 4., 4., 4., 4., 4., 4.],\n",
      "        [4., 4., 4., 4., 4., 4., 4., 4.]])\n",
      "\n",
      "Narrow tensor [small cupcake ðŸ§] (torch.Size([3, 2])):\n",
      "tensor([[5., 5.],\n",
      "        [5., 5.],\n",
      "        [5., 5.]])\n",
      "\n",
      "âœ… Concatenating along dimension 1 (columns) - SUCCESS!\n",
      "Result shape: torch.Size([3, 10])\n",
      "Result:\n",
      "tensor([[4., 4., 4., 4., 4., 4., 4., 4., 5., 5.],\n",
      "        [4., 4., 4., 4., 4., 4., 4., 4., 5., 5.],\n",
      "        [4., 4., 4., 4., 4., 4., 4., 4., 5., 5.]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create tensors with different shapes\n",
    "tensor_wide = torch.ones(3, 8) * 4   # 3x5 tensor filled with 4s\n",
    "tensor_narrow = torch.ones(3, 2) * 5  # 3x2 tensor filled with 5s\n",
    "\n",
    "print(f\"Wide tensor [big cake ðŸŽ‚] (shape {tensor_wide.shape}):\\n{tensor_wide}\\n\")\n",
    "print(f\"Narrow tensor [small cupcake ðŸ§ ] (shape {tensor_narrow.shape}):\\n{tensor_narrow}\\n\")\n",
    "\n",
    "# This FAILS: Concatenating along dimension 0, you can stack pancakes with different sizes \n",
    "# They have different column counts (5 vs 2)\n",
    "print(\"âŒ Attempting to concatenate along dimension 0 (rows), stack cake on top of cupcake ...\")\n",
    "try:\n",
    "    cat_cols_fail = torch.cat([tensor_wide, tensor_narrow], dim=0)\n",
    "except RuntimeError as e:\n",
    "    print(f\"ðŸŽ‚/ðŸ§ This couldn't work! \\nError as expected: {str(e)}\")\n",
    "    \n",
    "\n",
    "print(\"Our unequal test subjects:\")\n",
    "print(f\"Wide tensor [big cake ðŸŽ‚] ({tensor_wide.shape}):\\n{tensor_wide}\\n\")\n",
    "print(f\"Narrow tensor [small cupcake ðŸ§] ({tensor_narrow.shape}):\\n{tensor_narrow}\\n\")\n",
    "\n",
    "# This WORKS: Concatenating along dimension 1 (columns)\n",
    "# Both have 3 rows, so we can lay them side by side horizontally\n",
    "print(\"âœ… Concatenating along dimension 1 (columns) - SUCCESS!\")\n",
    "cat_rows_success = torch.cat([tensor_wide, tensor_narrow], dim=1)\n",
    "print(f\"Result shape: {cat_rows_success.shape}\")\n",
    "print(f\"Result:\\n{cat_rows_success}\\n\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "859dab6e",
   "metadata": {},
   "source": [
    "### The Stacker: `torch.stack()` - Creating New Dimensions!\n",
    "\n",
    "`torch.stack()` is more dramatic than concatenation! It **creates an entirely new dimension** and places each tensor along it. Think of it as the difference between:\n",
    "- **Concatenation**: Gluing pieces end-to-end in the same plane ðŸ§©âž¡ï¸ðŸ§©\n",
    "- **Stacking**: Creating a whole new layer/dimension ðŸ“š (like stacking books on top of each other)\n",
    "\n",
    "**Critical Rule**: All input tensors must have *identical* shapesâ€”no exceptions!\n",
    "\n",
    "Let's start simple and build our intuition step by step...\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dda3aed",
   "metadata": {},
   "source": [
    "#### Step 1: Stacking 1D Tensors â†’ Creating a 2D Matrix\n",
    "\n",
    "Let's start with something simple: three 1D tensors (think of them as rulers ðŸ“). When we stack them, we create a 2D matrix!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "2a641b6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our three rulers ðŸ“ (1D tensors):\n",
      "Ruler 1: tensor([1, 2, 3, 4]) (shape: torch.Size([4]))\n",
      "Ruler 2: tensor([10, 20, 30, 40]) (shape: torch.Size([4]))\n",
      "Ruler 3: tensor([100, 200, 300, 400]) (shape: torch.Size([4]))\n",
      "\n",
      "Stacked rulers ðŸŸ° (dim=0) - like placing rulers on top of each other:\n",
      "Result shape: torch.Size([3, 4])\n",
      "Result:\n",
      "tensor([[  1,   2,   3,   4],\n",
      "        [ 10,  20,  30,  40],\n",
      "        [100, 200, 300, 400]])\n",
      "\n",
      "Access individual rulers:\n",
      "First ruler:  tensor([1, 2, 3, 4])\n",
      "Second ruler: tensor([10, 20, 30, 40])\n",
      "Third ruler:  tensor([100, 200, 300, 400])\n"
     ]
    }
   ],
   "source": [
    "# Let's create simple 1D tensors first\n",
    "ruler_1 = torch.tensor([1, 2, 3, 4])      # A ruler with numbers 1,2,3,4\n",
    "ruler_2 = torch.tensor([10, 20, 30, 40])  # A ruler with numbers 10,20,30,40  \n",
    "ruler_3 = torch.tensor([100, 200, 300, 400]) # A ruler with numbers 100,200,300,400\n",
    "\n",
    "print(\"Our three rulers ðŸ“ (1D tensors):\")\n",
    "print(f\"Ruler 1: {ruler_1} (shape: {ruler_1.shape})\")\n",
    "print(f\"Ruler 2: {ruler_2} (shape: {ruler_2.shape})\")\n",
    "print(f\"Ruler 3: {ruler_3} (shape: {ruler_3.shape})\\n\")\n",
    "\n",
    "# Stack them to create a 2D matrix (like putting rulers on top of each other)\n",
    "stacked_rulers = torch.stack([ruler_1, ruler_2, ruler_3], dim=0)\n",
    "print(\"Stacked rulers ðŸŸ° (dim=0) - like placing rulers on top of each other:\")\n",
    "print(f\"Result shape: {stacked_rulers.shape}\")  # Notice: (3,4) - we added a new dimension!\n",
    "print(f\"Result:\\n{stacked_rulers}\\n\")\n",
    "\n",
    "# Each \"ruler\" is now accessible as a row\n",
    "print(\"Access individual rulers:\")\n",
    "print(f\"First ruler:  {stacked_rulers[0]}\")\n",
    "print(f\"Second ruler: {stacked_rulers[1]}\")  \n",
    "print(f\"Third ruler:  {stacked_rulers[2]}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "01c3fdcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stacked rulers â¸ï¸ (dim=1) - like arranging rulers side by side:\n",
      "Result shape: torch.Size([4, 3])\n",
      "Result:\n",
      "tensor([[  1,  10, 100],\n",
      "        [  2,  20, 200],\n",
      "        [  3,  30, 300],\n",
      "        [  4,  40, 400]])\n",
      "\n",
      "Notice the pattern:\n",
      "Each row shows the 1st, 2nd, 3rd... element from ALL rulers\n",
      "Position 0 from all rulers: tensor([  1,  10, 100])\n",
      "Position 1 from all rulers: tensor([  2,  20, 200])\n"
     ]
    }
   ],
   "source": [
    "# We can also stack along dimension 1 (different arrangement)\n",
    "stack_dim1 = torch.stack([ruler_1, ruler_2, ruler_3], dim=1)\n",
    "print(\"Stacked rulers â¸ï¸ (dim=1) - like arranging rulers side by side:\")\n",
    "print(f\"Result shape: {stack_dim1.shape}\")  # Notice: (4,3) - different arrangement!\n",
    "print(f\"Result:\\n{stack_dim1}\\n\")\n",
    "\n",
    "# Each column now represents values from all three rulers at the same position\n",
    "print(\"Notice the pattern:\")\n",
    "print(\"Each row shows the 1st, 2nd, 3rd... element from ALL rulers\")\n",
    "print(f\"Position 0 from all rulers: {stack_dim1[0]}\")  # [1, 10, 100]\n",
    "print(f\"Position 1 from all rulers: {stack_dim1[1]}\")  # [2, 20, 200]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4cc64cb",
   "metadata": {},
   "source": [
    "#### Step 2: Stacking 2D Tensors â†’ Creating a 3D Cube!\n",
    "\n",
    "Now for the mind-bending part! When we stack 2D tensors (matrices), we create a **3D tensor**. Think of it like:\n",
    "\n",
    "**ðŸ“„ 2D tensor** = A page from a book (has rows and columns)  \n",
    "**ðŸ“– Stacking 2D tensors** = Creating a book with multiple pages  \n",
    "**ðŸ“š 3D tensor** = The entire book! (pages Ã— rows Ã— columns)\n",
    "\n",
    "**Key Metaphors to Remember:**\n",
    "- **Book metaphor**: `tensor[page][row][column]` ðŸ“š\n",
    "- **RGB image**: `tensor[channel][height][width]` ðŸ–¼ï¸ (like stacking color layers: Red, Green, Blue)\n",
    "\n",
    "These metaphors help you \"see\" how changing the dimension you stack along changes the meaning of each axis in your tensor. ðŸ¤“\n",
    "\n",
    "Let's see this dimensional magic in action:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5f737c86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our three pages (2D tensors):\n",
      "ðŸ“„ Page 1 (shape torch.Size([5, 2])):\n",
      "tensor([[1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.]])\n",
      "\n",
      "ðŸ“„ Page 2 (shape torch.Size([5, 2])):\n",
      "tensor([[2., 2.],\n",
      "        [2., 2.],\n",
      "        [2., 2.],\n",
      "        [2., 2.],\n",
      "        [2., 2.]])\n",
      "\n",
      "ðŸ“„ Page 3 (shape torch.Size([5, 2])):\n",
      "tensor([[3., 3.],\n",
      "        [3., 3.],\n",
      "        [3., 3.],\n",
      "        [3., 3.],\n",
      "        [3., 3.]])\n",
      "\n",
      "ðŸ“š BEHOLD! Our 3D book (stacked along dim=0):\n",
      "Book shape: torch.Size([3, 5, 2])\n",
      "Full book:\n",
      "tensor([[[1., 1.],\n",
      "         [1., 1.],\n",
      "         [1., 1.],\n",
      "         [1., 1.],\n",
      "         [1., 1.]],\n",
      "\n",
      "        [[2., 2.],\n",
      "         [2., 2.],\n",
      "         [2., 2.],\n",
      "         [2., 2.],\n",
      "         [2., 2.]],\n",
      "\n",
      "        [[3., 3.],\n",
      "         [3., 3.],\n",
      "         [3., 3.],\n",
      "         [3., 3.],\n",
      "         [3., 3.]]])\n",
      "\n",
      "ðŸ” Accessing different parts of our 3D tensor:\n",
      "ðŸ“– Entire first page (book[0]):\n",
      "tensor([[1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.]])\n",
      "\n",
      "ðŸ“ First row of second page (book[1, 0]): tensor([2., 2.])\n",
      "ðŸŽ¯ Specific element - page 2, row 1, column 2 (book[1, 0, 1]): 2.0\n",
      "\n",
      "ðŸ¤” Think of it as: Book[page_number][row_number][column_number]\n"
     ]
    }
   ],
   "source": [
    "# Create three 2D \"pages\" for our book\n",
    "page_1 = torch.ones(5, 2) * 1    # Page 1: all 1s\n",
    "page_2 = torch.ones(5, 2) * 2    # Page 2: all 2s  \n",
    "page_3 = torch.ones(5, 2) * 3    # Page 3: all 3s\n",
    "\n",
    "print(\"Our three pages (2D tensors):\")\n",
    "print(f\"ðŸ“„ Page 1 (shape {page_1.shape}):\\n{page_1}\\n\")\n",
    "print(f\"ðŸ“„ Page 2 (shape {page_2.shape}):\\n{page_2}\\n\") \n",
    "print(f\"ðŸ“„ Page 3 (shape {page_3.shape}):\\n{page_3}\\n\")\n",
    "\n",
    "# Stack them along dimension 0 to create a 3D \"book\"\n",
    "book = torch.stack([page_1, page_2, page_3], dim=0)\n",
    "print(\"ðŸ“š BEHOLD! Our 3D book (stacked along dim=0):\")\n",
    "print(f\"Book shape: {book.shape}\")  # (3, 2, 3) = (pages, rows, columns)\n",
    "print(f\"Full book:\\n{book}\\n\")\n",
    "\n",
    "# Now we can access individual pages, rows, or even specific elements!\n",
    "print(\"ðŸ” Accessing different parts of our 3D tensor:\")\n",
    "print(f\"ðŸ“– Entire first page (book[0]):\\n{book[0]}\\n\")\n",
    "print(f\"ðŸ“ First row of second page (book[1, 0]): {book[1, 0]}\")\n",
    "print(f\"ðŸŽ¯ Specific element - page 2, row 1, column 2 (book[1, 0, 1]): {book[1, 0, 1].item()}\")\n",
    "\n",
    "print(f\"\\nðŸ¤” Think of it as: Book[page_number][row_number][column_number]\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456cbd69",
   "metadata": {},
   "source": [
    "#### Step 3: The Dimension Dance - Where You Stack Matters!\n",
    "\n",
    "When stacking 2D tensors, **which dimension you choose creates very different 3D shapes**! \n",
    "\n",
    "**ðŸ§± The Clay Tablet Box Metaphor:**\n",
    "\n",
    "For better understanding, imagine that when you use the `stack()` method, a **new 3D package** is created with an extended dimension. If you stack 2D objects (like clay tablets), you first create a 3D box, then arrange your 2D tablets inside.\n",
    "\n",
    "Picture this: You have three identical clay tablets ðŸ§± and an empty 3D box ðŸ“¦. There are **exactly 3 different ways** to arrange them inside!\n",
    "\n",
    "**ðŸ“ 3D Box Coordinates (always viewed from the same angle):**\n",
    "- **Depth** = `dim=0` (front to back)\n",
    "- **Height** = `dim=1` (bottom to top)  \n",
    "- **Width** = `dim=2` (left to right)\n",
    "\n",
    "**ðŸŽ¯ The Three Stacking Strategies:**\n",
    "\n",
    "- **`dim=0`**: **Stack tablets front-to-back** â†’ Shape `(tablets, rows, cols)` ðŸ“š  \n",
    "  *Each tablet goes deeper into the box, one behind the other*\n",
    "\n",
    "- **`dim=1`**: **Stack tablets bottom-to-top** â†’ Shape `(rows, tablets, cols)` ðŸ—‚ï¸  \n",
    "  *Each tablet is placed higher in the box, building upward* - we start with last tablets\n",
    "\n",
    "- **`dim=2`**: **Slide tablets left-to-right** â†’ Shape `(rows, cols, tablets)` ðŸ“‘  \n",
    "  *Each tablet slides sideways, arranged side by side*\n",
    "\n",
    "The dimension you choose determines **which direction** your tablets extend in the 3D space!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1825fb2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§± Starting with three identical clay tablets:\n",
      "Each tablet shape: torch.Size([5, 2]) (5 rows, 2 columns)\n",
      "\n",
      "ðŸ“š Stacking front-to-back (dim=0): Shape torch.Size([3, 5, 2])\n",
      "   (depth=3, heith=5, width=2) - tablets go deeper into the box\n",
      "Front of the box):\n",
      "tensor([[1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.]])\n",
      "\n",
      "ðŸ—‚ï¸ Stacking bottom-to-top (dim=1): Shape torch.Size([5, 3, 2])\n",
      "   (depth=5, heigh=3, width=2) - tablets build upward\n",
      "Notice how each level contains one slice from ALL tablets:\n",
      "Front of the box):\n",
      "tensor([[1., 1.],\n",
      "        [2., 2.],\n",
      "        [3., 3.]])\n",
      "\n",
      "ðŸ“‘ Sliding left-to-right (dim=2): Shape torch.Size([5, 2, 3])\n",
      "   (depth=5, heigh=2, width=3) - tablets slide sideways\n",
      "Each position now contains values from ALL tablets:\n",
      "Front of the box):\n",
      "tensor([[1., 2., 3.],\n",
      "        [1., 2., 3.]])\n",
      "\n",
      "\n",
      "ðŸŽ¯ Key Insight: The dimension you choose determines WHERE the tablets extend!\n",
      "   dim=0: Tablets extend front-to-back (depth)\n",
      "   dim=1: Tablets extend bottom-to-top (height)\n",
      "   dim=2: Tablets extend left-to-right (width)\n"
     ]
    }
   ],
   "source": [
    "# Let's arrange our three 2D clay tablets in three different ways!\n",
    "print(\"ðŸ§± Starting with three identical clay tablets:\")\n",
    "print(f\"Each tablet shape: {page_1.shape} (5 rows, 2 columns)\")\n",
    "\n",
    "# dim=0: Stack tablets front-to-back (into the box)\n",
    "stack_dim0 = torch.stack([page_1, page_2, page_3], dim=0)\n",
    "print(f\"\\nðŸ“š Stacking front-to-back (dim=0): Shape {stack_dim0.shape}\")\n",
    "print(\"   (depth=3, heith=5, width=2) - tablets go deeper into the box\")\n",
    "print(f\"Front of the box):\\n{stack_dim0[0]}\\n\")\n",
    "\n",
    "# dim=1: Stack tablets bottom-to-top (building upward)  \n",
    "stack_dim1 = torch.stack([page_1, page_2, page_3], dim=1)\n",
    "print(f\"ðŸ—‚ï¸ Stacking bottom-to-top (dim=1): Shape {stack_dim1.shape}\")\n",
    "print(\"   (depth=5, heigh=3, width=2) - tablets build upward\")\n",
    "print(\"Notice how each level contains one slice from ALL tablets:\")\n",
    "print(f\"Front of the box):\\n{stack_dim1[0]}\\n\")\n",
    "\n",
    "# dim=2: Slide tablets left-to-right (arranging sideways)\n",
    "stack_dim2 = torch.stack([page_1, page_2, page_3], dim=2) \n",
    "print(f\"ðŸ“‘ Sliding left-to-right (dim=2): Shape {stack_dim2.shape}\")\n",
    "print(\"   (depth=5, heigh=2, width=3) - tablets slide sideways\")\n",
    "print(\"Each position now contains values from ALL tablets:\")\n",
    "print(f\"Front of the box):\\n{stack_dim2[0]}\\n\")\n",
    "\n",
    "print(\"\\nðŸŽ¯ Key Insight: The dimension you choose determines WHERE the tablets extend!\")\n",
    "print(\"   dim=0: Tablets extend front-to-back (depth)\")  \n",
    "print(\"   dim=1: Tablets extend bottom-to-top (height)\")\n",
    "print(\"   dim=2: Tablets extend left-to-right (width)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cbfb8c8",
   "metadata": {},
   "source": [
    "### The Fusion Dilemma: When to Cat vs. Stack?\n",
    "\n",
    "This choice torments many apprentices! Let me illuminate the path:\n",
    "\n",
    "**Use `torch.cat()` when:**\n",
    "- Tensors represent *different parts* of the same data (e.g., different batches of images, different chunks of text)\n",
    "- You want to *extend* an existing dimension\n",
    "- Example: Concatenating multiple batches of training data\n",
    "\n",
    "**Use `torch.stack()` when:**\n",
    "- Tensors represent *parallel data* of the same type (e.g., predictions from different models, different time steps)  \n",
    "- You need to create a *new dimension* to organize the data\n",
    "- Example: Combining RGB channels to form a color image, or collecting multiple predictions\n",
    "\n",
    "Observe this real-world scenario!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "7798acea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Individual images:\n",
      "Image 1 shape: torch.Size([2, 3])\n",
      "Image 2 shape: torch.Size([2, 3])\n",
      "Image 3 shape: torch.Size([2, 3])\n",
      "\n",
      "Batch of images shape: torch.Size([3, 2, 3])\n",
      "Perfect for feeding into a neural network!\n",
      "\n",
      "RGB image shape: torch.Size([3, 2, 3])\n",
      "The classic (C, H, W) format!\n"
     ]
    }
   ],
   "source": [
    "# Real-world example: Building a batch of images\n",
    "# Imagine these are grayscale images (height=2, width=3)\n",
    "image1 = torch.randn(2, 3)  \n",
    "image2 = torch.randn(2, 3)\n",
    "image3 = torch.randn(2, 3)\n",
    "\n",
    "print(\"Individual images:\")\n",
    "print(f\"Image 1 shape: {image1.shape}\")\n",
    "print(f\"Image 2 shape: {image2.shape}\")  \n",
    "print(f\"Image 3 shape: {image3.shape}\\n\")\n",
    "\n",
    "# STACK them to create a batch (batch_size=3, height=2, width=3)\n",
    "image_batch = torch.stack([image1, image2, image3], dim=0)\n",
    "print(f\"Batch of images shape: {image_batch.shape}\")\n",
    "print(\"Perfect for feeding into a neural network!\\n\")\n",
    "\n",
    "# Now imagine we have RGB channels for one image\n",
    "red_channel = torch.randn(2, 3)\n",
    "green_channel = torch.randn(2, 3) \n",
    "blue_channel = torch.randn(2, 3)\n",
    "\n",
    "# STACK them to create RGB image (channels=3, height=2, width=3)\n",
    "rgb_image = torch.stack([red_channel, green_channel, blue_channel], dim=0)\n",
    "print(f\"RGB image shape: {rgb_image.shape}\")\n",
    "print(\"The classic (C, H, W) format!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4dd7aeb",
   "metadata": {},
   "source": [
    "### Your Mission: The Fusion Master's Gauntlet\n",
    "\n",
    "The theory is yoursâ€”now prove your mastery! Complete these fusion challenges:\n",
    "\n",
    "1. **The Triple Stack**: Create three 1D tensors of length 4 with different values. Stack them to create a 2D tensor of shape `(3, 4)`.\n",
    "\n",
    "2. **The Horizontal Fusion**: Create two 2D tensors of shape `(3, 2)`. Concatenate them horizontally to create a `(3, 4)` tensor.\n",
    "\n",
    "3. **The Batch Builder**: You have 5 individual \"samples\" (each a 1D tensor of length 3). Stack them to create a proper batch tensor of shape `(5, 3)` suitable for training.\n",
    "\n",
    "4. **The Dimension Disaster**: Try to concatenate two tensors with different shapes: `(2, 3)` and `(2, 4)` along dimension 1. Observe the error messageâ€”it's quite educational! Then fix it by concatenating along dimension 0 instead.\n",
    "\n",
    "5. **The Multi-Fusion**: Create a tensor of shape `(2, 6)` by first stacking three `(2, 2)` tensors, then concatenating the result with another `(3, 6)` tensor. This requires combining both operations!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c9beb09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code for the Fusion Master's Gauntlet goes here!\n",
    "\n",
    "print(\"--- 1. The Triple Stack ---\")\n",
    "tensor1 = torch.tensor([1, 2, 3, 4])\n",
    "tensor2 = torch.tensor([5, 6, 7, 8]) \n",
    "tensor3 = torch.tensor([9, 10, 11, 12])\n",
    "triple_stack = torch.stack([tensor1, tensor2, tensor3], dim=0)\n",
    "print(f\"Triple stack result:\\n{triple_stack}\")\n",
    "print(f\"Shape: {triple_stack.shape}\\n\")\n",
    "\n",
    "print(\"--- 2. The Horizontal Fusion ---\")\n",
    "left_tensor = torch.randn(3, 2)\n",
    "right_tensor = torch.randn(3, 2)\n",
    "horizontal_fusion = torch.cat([left_tensor, right_tensor], dim=1)\n",
    "print(f\"Horizontal fusion shape: {horizontal_fusion.shape}\\n\")\n",
    "\n",
    "print(\"--- 3. The Batch Builder ---\")\n",
    "samples = [torch.randn(3) for _ in range(5)]  # 5 samples of length 3\n",
    "batch = torch.stack(samples, dim=0)\n",
    "print(f\"Batch shape: {batch.shape}\")\n",
    "print(\"Ready for neural network training!\\n\")\n",
    "\n",
    "print(\"--- 4. The Dimension Disaster ---\")\n",
    "disaster_a = torch.randn(2, 3)\n",
    "disaster_b = torch.randn(2, 4)\n",
    "try:\n",
    "    # This will fail!\n",
    "    bad_cat = torch.cat([disaster_a, disaster_b], dim=1)\n",
    "except RuntimeError as e:\n",
    "    print(f\"Error (as expected): {e}\")\n",
    "    \n",
    "# The fix: concatenate along dimension 0\n",
    "good_cat = torch.cat([disaster_a, disaster_b], dim=0)  \n",
    "print(f\"Fixed by concatenating along dim 0: {good_cat.shape}\\n\")\n",
    "\n",
    "print(\"--- 5. The Multi-Fusion ---\")\n",
    "# First, create and stack three (2,2) tensors\n",
    "small_tensors = [torch.randn(2, 2) for _ in range(3)]\n",
    "# Actually, let's concatenate the (2,2) tensors along dim=1 first\n",
    "concat_part = torch.cat(small_tensors, dim=1)  # Shape: (2, 6)\n",
    "print(f\"Multi-fusion result shape: {concat_part.shape}\")\n",
    "print(\"The key was concatenating, not stacking!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-course-YUNTNAZF-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
