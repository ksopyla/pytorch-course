{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summoning Your First Tensors\n",
    "\n",
    "**Module 1 | Lesson 1**\n",
    "\n",
    "---\n",
    "\n",
    "## Professor Torchenstein's Grand Directive\n",
    "\n",
    "Mwahahaha! Welcome, my brilliant acolytes. Today, we shall peel back the very fabric of reality—or, at the very least, the fabric of a PyTorch tensor. We are not merely learning; we are engaging in the sacred act of *creation*!\n",
    "\n",
    "\"Prepare your minds! The tensors... they are about to be summoned!\"\n",
    "\n",
    "---\n",
    "\n",
    "![pytorch tensors everywhere](/assets/images/torchenstein_presenting_cube.png)\n",
    "\n",
    "## Your Mission Briefing\n",
    "\n",
    "By the end of this dark ritual, you will have mastered the arcane arts of:\n",
    "\n",
    "*   **Understanding** what a tensor is and why it's the fundamental building block of all modern AI.\n",
    "*   **Summoning** tensors from nothingness using a variety of powerful PyTorch functions.\n",
    "*   **Inspecting** the very soul of a tensor: its shape, data type, and the device it inhabits.\n",
    "*   **Simple Indexing** the main way to access elements of a tensor.\n",
    "*   **Creating sequences** of numbers with `torch.arange` and `torch.linspace` and `_like` methods.\n",
    "\n",
    "**Estimated Time to Completion:** 15 glorious minutes of pure, unadulterated learning.\n",
    "\n",
    "**What You'll Need:**\n",
    "*   A mind hungry for forbidden knowledge!\n",
    "*   A working PyTorch environment, ready for spellcasting.\n",
    "*   (Optional but recommended) A beverage of your choice—creation is thirsty work!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Theory Behind the Magic: What is a Tensor, *Really*?\n",
    "\n",
    "First, we must understand the incantation before we cast the spell. You've heard the word \"tensor,\" whispered in the hallowed halls of academia and screamed during GPU memory overflows. But what *is* it?\n",
    "\n",
    "Forget what the mathematicians told you about coordinate transformations for a moment. In our glorious domain of PyTorch, a tensor is simply a **multi-dimensional array of numbers**. It is the generalization of vectors and matrices to an arbitrary number of dimensions. Think of it as the ultimate container for your data!\n",
    "\n",
    "- A **scalar** (a single number, like `5`) is a 0-dimensional tensor.\n",
    "- A **vector** (a list of numbers, like `[1, 2, 3]`) is a 1-dimensional tensor.\n",
    "- A **matrix** (a grid of numbers) is a 2-dimensional tensor.\n",
    "- And a **tensor**? It can be all of those, and so much more! 3D, 4D, 5D... all await your command!\n",
    "\n",
    "**Why not just use matrices?** Mwahaha, a foolish question! Modern data is complex!\n",
    "- An image is not a flat grid; it's a 3D tensor (`height`, `width`, `channels`).\n",
    "- A batch of images for training is a 4D tensor (`batch_size`, `height`, `width`, `channels`).\n",
    "- Text data is often represented as 3D tensors (`batch_size`, `sequence_length`, `embedding_dimension`).\n",
    "\n",
    "Tensors give us the power to mold and shape all this data with a single, unified tool. They are the clay from which we will sculpt our magnificent AI creations!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Ritual: Summoning Your First Tensors\n",
    "\n",
    "Enough theory! The time has come to channel the raw power of PyTorch. We will now perform the summoning rituals—the core functions you will use constantly in your dark arts.\n",
    "\n",
    "First, let's prepare the laboratory by importing `torch` and setting a manual seed. Why the seed? To ensure our \"random\" experiments are reproducible! We are scientists, not gamblers!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Conjuring from Existing Data (`torch.tensor`)\n",
    "\n",
    "The most direct way to create a tensor is from existing data, like a Python list. The `torch.tensor()` command consumes your data and transmutes it into a glorious PyTorch tensor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n",
      "<class 'torch.Tensor'>\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Set the seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# A humble Python list\n",
    "my_list = [[1, 2, 3], [4, 5, 6]]\n",
    "\n",
    "# The transmutation!\n",
    "my_tensor = torch.tensor(my_list)\n",
    "\n",
    "print(my_tensor)\n",
    "print(type(my_tensor))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Summoning Tensors of a Specific Size\n",
    "\n",
    "Often, you won't have data yet. You simply need a tensor of a particular shape, a blank canvas for your masterpiece.\n",
    "\n",
    "- `torch.randn(shape)`: Summons a tensor filled with random numbers from a standard normal distribution (mean 0, variance 1). Perfect for initializing weights in a neural network!\n",
    "- `torch.zeros(shape)`: Creates a tensor of the given shape filled entirely with zeros.\n",
    "- `torch.ones(shape)`: Creates a tensor of the given shape filled entirely with ones."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A random tensor:\n",
      " tensor([[-0.7658, -0.7506,  1.3525],\n",
      "        [ 0.6863, -0.3278,  0.7950]])\n",
      "\n",
      "A tensor of zeros:\n",
      " tensor([[0., 0.],\n",
      "        [0., 0.],\n",
      "        [0., 0.]])\n",
      "\n",
      "A tensor of ones:\n",
      " tensor([[[1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1.]],\n",
      "\n",
      "        [[1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1.],\n",
      "         [1., 1., 1., 1.]]])\n"
     ]
    }
   ],
   "source": [
    "# A 2x3 tensor of random numbers\n",
    "random_tensor = torch.randn(2, 3)\n",
    "print(f\"A random tensor:\\n {random_tensor}\\n\")\n",
    "\n",
    "# A 3x2 tensor of zeros\n",
    "zeros_tensor = torch.zeros(3, 2)\n",
    "print(f\"A tensor of zeros:\\n {zeros_tensor}\\n\")\n",
    "\n",
    "# A 2x3x4 tensor of ones\n",
    "ones_tensor = torch.ones(2, 3, 4)\n",
    "print(f\"A tensor of ones:\\n {ones_tensor}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  3. Inspecting Your Creation\n",
    "\n",
    "A true master understands their creation. Once you have summoned a tensor, you must learn to inspect its very soul. These are the three most critical attributes you will constantly examine:\n",
    "\n",
    "- `.shape`: Reveals the dimensions of your tensor. A vital sanity check!\n",
    "- `.dtype`: Shows the data type of the elements within the tensor (e.g., `torch.float8`,  `torch.float32`, `torch.int64`).\n",
    "- `.device`: Tells you where the tensor lives—on the humble CPU or the glorious GPU.\n",
    "\n",
    "More details about the data types and device types you will learn in [03_data_types_and_device_types](03_data_types_and_device_types.ipynb)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The tensor:\n",
      " tensor([[ 2.2082, -0.6380,  0.4617,  0.2674],\n",
      "        [ 0.5349,  0.8094,  1.1103, -1.6898],\n",
      "        [-0.9890,  0.9580,  1.3221,  0.8172]])\n",
      "\n",
      "Shape of the tensor: torch.Size([3, 4])\n",
      "Data type of the tensor: torch.float32\n",
      "Device the tensor is on: cpu\n"
     ]
    }
   ],
   "source": [
    "# Let's create a tensor to inspect\n",
    "inspection_tensor = torch.randn(3, 4)\n",
    "\n",
    "print(f\"The tensor:\\n {inspection_tensor}\\n\")\n",
    "\n",
    "# Inspecting its soul\n",
    "print(f\"Shape of the tensor: {inspection_tensor.shape}\")\n",
    "print(f\"Data type of the tensor: {inspection_tensor.dtype}\")\n",
    "print(f\"Device the tensor is on: {inspection_tensor.device}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4 Precision Strikes: Accessing Elements\n",
    "\n",
    "To access a single, quivering element within our tensor, we use the `[row, column]` notation, just as you would with a common Python list of lists. Remember, my apprentice: dimensions are zero-indexed! The first row is row `0`, not row 1! A classic pitfall for the uninitiated.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Element at [1, 3]: 9\n",
      "Its value is: 9\n",
      "Notice its data type: torch.int64\n",
      "And its shape: torch.Size([]) (a 0-dimensional tensor!)\n"
     ]
    }
   ],
   "source": [
    "subject_tensor = torch.randint(0, 100, (5, 4))\n",
    "\n",
    "# Let's pluck the element at the 2nd row (index 1) and 4th column (index 3).\n",
    "single_element = subject_tensor[1, 3]\n",
    "\n",
    "print(f\"Element at [1, 3]: {single_element}\")\n",
    "\n",
    "# .item() is a glorious spell to extract the raw Python number from a single-element tensor.\n",
    "# Use it when you need to pass a tensor's value to other libraries or just print it cleanly!\n",
    "print(f\"Its value is: {single_element.item()}\")\n",
    "\n",
    "print(f\"Notice its data type: {single_element.dtype}\")\n",
    "print(f\"And its shape: {single_element.shape} (a 0-dimensional tensor!)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Creating Sequential Tensors\n",
    "\n",
    "Sometimes, you need tensors with predictable, orderly values.\n",
    "\n",
    "- `torch.arange(start, end, step)`: Creates a 1D tensor with values from `start` (inclusive) to `end` (exclusive), with a given `step`. It's the PyTorch version of Python's `range()`.\n",
    "- `torch.linspace(start, end, steps)`: Creates a 1D tensor with a specific number of `steps` evenly spaced between `start` and `end` (both inclusive).\n",
    "\n",
    "#### Your Mission: A Gauntlet of Sequences!\n",
    "\n",
    "Your list of challenges grows, apprentice! Prove your mastery.\n",
    "\n",
    "1.  **Odd Numbers:** Create a 1D tensor of all odd numbers from 1 to 19.\n",
    "2.  **Evenly Spaced:** Create a 1D tensor with 9 evenly spaced numbers from 50 to 100.\n",
    "3.  **Countdown:** Create a tensor that counts down from 10 to 0 in steps of 0.5.\n",
    "4.  **Pi Sequence:** The famous `sin` and `cos` functions, used in positional encodings, operate on radians. Create a tensor with 17 evenly spaced numbers from `-π` to `π`. (Hint: `torch.pi` is your friend!)\n",
    "5.  **`arange` vs. `linspace`:** Create a tensor of numbers from 0 to 1 with a step of 0.1 using `arange`. Then, create a tensor from 0 to 1 with 11 steps using `linspace`. Observe the subtle but critical difference in their outputs! What causes it?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1. Odd Numbers ---\n",
      "tensor([ 1,  3,  5,  7,  9, 11, 13, 15, 17, 19])\n",
      "\n",
      "--- 2. Evenly Spaced ---\n",
      "tensor([ 50.0000,  56.2500,  62.5000,  68.7500,  75.0000,  81.2500,  87.5000,\n",
      "         93.7500, 100.0000])\n",
      "\n",
      "--- 3. Countdown ---\n",
      "tensor([10.0000,  9.5000,  9.0000,  8.5000,  8.0000,  7.5000,  7.0000,  6.5000,\n",
      "         6.0000,  5.5000,  5.0000,  4.5000,  4.0000,  3.5000,  3.0000,  2.5000,\n",
      "         2.0000,  1.5000,  1.0000,  0.5000,  0.0000])\n",
      "\n",
      "--- 4. Pi Sequence ---\n",
      "tensor([-3.1416, -2.7489, -2.3562, -1.9635, -1.5708, -1.1781, -0.7854, -0.3927,\n",
      "         0.0000,  0.3927,  0.7854,  1.1781,  1.5708,  1.9635,  2.3562,  2.7489,\n",
      "         3.1416])\n",
      "\n",
      "--- 5. arange vs. linspace ---\n",
      "arange result (0 to 0.9): tensor([0.0000, 0.1000, 0.2000, 0.3000, 0.4000, 0.5000, 0.6000, 0.7000, 0.8000,\n",
      "        0.9000])\n",
      "linspace result (0 to 1, 10 steps): tensor([0.0000, 0.1111, 0.2222, 0.3333, 0.4444, 0.5556, 0.6667, 0.7778, 0.8889,\n",
      "        1.0000])\n",
      "\n",
      "Notice how arange's result doesn't include 1, while linspace does!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Your code for the challenges goes here!\n",
    "\n",
    "print(\"--- 1. Odd Numbers ---\")\n",
    "odd_numbers = torch.arange(1, 20, 2)\n",
    "print(f\"{odd_numbers}\\n\")\n",
    "\n",
    "print(\"--- 2. Evenly Spaced ---\")\n",
    "evenly_spaced = torch.linspace(50, 100, 9)\n",
    "print(f\"{evenly_spaced}\\n\")\n",
    "\n",
    "print(\"--- 3. Countdown ---\")\n",
    "countdown = torch.arange(10, -0.1, -0.5)\n",
    "print(f\"{countdown}\\n\")\n",
    "\n",
    "print(\"--- 4. Pi Sequence ---\")\n",
    "pi_seq = torch.linspace(-torch.pi, torch.pi, 17)\n",
    "print(f\"{pi_seq}\\n\")\n",
    "\n",
    "print(\"--- 5. arange vs. linspace ---\")\n",
    "# arange may suffer from floating point errors and not include the endpoint!\n",
    "arange_ex = torch.arange(0, 1, 0.1) \n",
    "# linspace is often safer for float ranges as it guarantees the number of points.\n",
    "linspace_ex = torch.linspace(0, 1, 10)\n",
    "print(f\"arange result (0 to 0.9): {arange_ex}\")\n",
    "print(f\"linspace result (0 to 1, 10 steps): {linspace_ex}\\n\")\n",
    "print(\"Notice how arange's result doesn't include 1, while linspace does!\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Creating Tensors from Other Tensors (the `_like` methods)\n",
    "\n",
    "Behold, a most elegant form of mimicry! Often, you will need to create a new tensor that has the exact same shape as another. PyTorch provides the `_like` methods for this very purpose.\n",
    "\n",
    "- `torch.zeros_like(input_tensor)`: Creates a tensor of all zeros with the same `shape`, `dtype`, and `device` as the input tensor.\n",
    "- `torch.ones_like(input_tensor)`: The same, but for ones!\n",
    "- `torch.randn_like(input_tensor)`: The same, but for random numbers!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's start with a template tensor\n",
    "template_tensor = torch.ones(2, 4)\n",
    "print(f\"Our template tensor:\\n {template_tensor}\\n\")\n",
    "\n",
    "# Now, create tensors LIKE our template\n",
    "zeros_mimic = torch.zeros_like(template_tensor)\n",
    "print(f\"A zeros tensor created from the template:\\n {zeros_mimic}\\n\")\n",
    "\n",
    "random_mimic = torch.randn_like(template_tensor)\n",
    "print(f\"A random tensor created from the template:\\n {random_mimic}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Real-World Sorcery: Where are Sequential Tensors Used?\n",
    "\n",
    "You may wonder, \"Professor, is this just for making neat little rows of numbers?\" A fair question from a novice! The answer is a resounding **NO!** These sequential tensors are the silent bedrock of many powerful constructs:\n",
    "\n",
    "*   **Positional Encodings in Transformers:** How does a Transformer know the order of words in a sentence? It doesn't, inherently! We must *inject* that information. The very first step is often to create a tensor representing the positions `[0, 1, 2, ..., sequence_length - 1]` using `torch.arange`. This sequence is then transformed into a high-dimensional positional embedding.\n",
    "\n",
    "*   **Generating Time-Series Data:** When working with audio, financial data, or any kind of signal, you often need a time axis. `torch.linspace` is perfect for creating a smooth, evenly-spaced time vector to plot or process your data against.\n",
    "\n",
    "*   **Creating Coordinate Grids in Vision:** For advanced image manipulation, you might need a grid representing the `(x, y)` coordinates of every pixel. You can generate the `x` and `y` vectors separately using `torch.arange` and then combine them to form this essential grid.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Your Mission: Forge Your Own Creation!\n",
    "\n",
    "A true master never stops practicing. I leave you with these challenges to solidify your newfound power. Do not be afraid to experiment! To the lab!\n",
    "\n",
    "1. **Apprentice Challenge:** Create a 2D tensor (a matrix) of shape `(3, 5)` filled with random numbers. Then, print its shape and data type to the console.\n",
    "\n",
    "2. **Artisan Challenge:** Create a 1D tensor of your favorite numbers (at least 4). Then, create a second tensor of all ones that has the *exact same shape* as your first tensor.\n",
    "\n",
    "3. **Create the bias vector:** You are tasked with creating the initial \"bias\" vector for a small neural network layer with 10 output neurons. For arcane reasons, the master architect (me!) has decreed that it must be a 1D tensor, filled with zeros, except for the very last element, which must be `1`. Create this specific tensor!\n",
    "4. **Positional Encoding Denominator:** In the legendary Transformer architecture, a key component is the denominator `10000^(2i / d_model)`. Your mission is to create this 1D tensor. Let `d_model = 128`. The term `i` represents dimension pairs, so it goes from `0` to `d_model/2 - 1`. Use `torch.arange` to create the `2i` sequence first, then perform the final calculation. This is a vital step in building the neural networks of tomorrow!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code for the final challenges goes here!\n",
    "\n",
    "# Apprentice Challenge Solution\n",
    "print(\"--- Apprentice Challenge ---\")\n",
    "apprentice_tensor = torch.randn(3, 5)\n",
    "print(f\"Tensor Shape: {apprentice_tensor.shape}\")\n",
    "print(f\"Tensor DType: {apprentice_tensor.dtype}\\n\")\n",
    "\n",
    "\n",
    "# Artisan Challenge Solution\n",
    "print(\"--- Artisan Challenge ---\")\n",
    "favorite_numbers = torch.tensor([3.14, 42, 1337, 99.9])\n",
    "ones_like_faves = torch.ones_like(favorite_numbers)\n",
    "print(f\"Favorite Numbers Tensor: {favorite_numbers}\")\n",
    "print(f\"Ones-Like Tensor: {ones_like_faves}\\n\")\n",
    "\n",
    "\n",
    "# Master Challenge Solution\n",
    "print(\"--- Bias Vector Challenge ---\")\n",
    "bias_vector = torch.zeros(10)\n",
    "bias_vector[9] = 1\n",
    "print(f\"Masterful Bias Vector: {bias_vector}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 6. Positional Encoding Denominator (Master Challenge) ---\n",
      "The first 5 values of the denominator are:\n",
      "tensor([1.0000, 1.1548, 1.3335, 1.5399, 1.7783])\n",
      "\n",
      "The last 5 values of the denominator are:\n",
      "tensor([4869.6753, 5623.4131, 6493.8164, 7498.9419, 8659.6436])\n",
      "\n",
      "Shape of the denominator tensor: torch.Size([64])\n"
     ]
    }
   ],
   "source": [
    "print(\"--- 4. Positional Encoding Denominator ---\")\n",
    "d_model = 128\n",
    "# Create the sequence for 2i (i.e., 0, 2, 4, ... up to d_model-2)\n",
    "two_i = torch.arange(0, d_model, 2)\n",
    "# Calculate the denominator\n",
    "denominator = 10000 ** (two_i / d_model)\n",
    "print(f\"The first 5 values of the denominator are:\\n{denominator[:5]}\")\n",
    "print(f\"\\nThe last 5 values of the denominator are:\\n{denominator[-5:]}\")\n",
    "print(f\"\\nShape of the denominator tensor: {denominator.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary: The Knowledge Is Yours!\n",
    "\n",
    "Magnificent! You've wrestled with the raw chaos of creation and emerged victorious! Let's recount the powerful secrets you've assimilated today:\n",
    "\n",
    "*   **Tensors are Everything:** You now understand that a tensor is a multi-dimensional array, the fundamental data structure for every piece of data you will encounter in your machine learning journey.\n",
    "*   **The Summoning Rituals:** You have mastered the core incantations for creating tensors: `torch.tensor`, `torch.randn`/`zeros`/`ones`, the powerful `_like` variants, and the sequence generators `torch.arange` and `torch.linspace`.\n",
    "*   **Know Your Creation:** You have learned the vital importance of inspecting your tensors using `.shape`, `.dtype`, and `.device` to understand their nature and prevent catastrophic errors.\n",
    "\n",
    "You have taken your first, most important step. The power is now in your hands!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Professor Torchenstein's Outro\n",
    "\n",
    "Do you feel it? The hum of latent power in your very fingertips? That, my apprentice, is the feeling of true understanding. You have summoned your first tensors, and they have answered your call. But this is merely the beginning! Our creations are still... rigid. Inflexible.\n",
    "\n",
    "In our next lesson, we will learn the dark arts of **Tensor Shape-Shifting & Sorcery**! We will slice, squeeze, and permute our tensors until reality itself seems to bend to our will.\n",
    "\n",
    "Until then, keep your learning rates high and your gradients flowing. The future of AI is in our hands! Mwahahahahaha!\n",
    "\n",
    "<video controls width=\"100%\" style=\"margin: 20px 0;\">\n",
    "    <source src=\"/assets/images/torchenstein_claps_proud.mp4\" type=\"video/mp4\">\n",
    "    Your browser does not support the video tag.\n",
    "</video>\n",
    "\n",
    "\n",
    "[Proceed to the Next Experiment: Tensor Shape-Shifting!](../02_tensor_manipulation)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-course-YUNTNAZF-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
