{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f646b49b",
   "metadata": {},
   "source": [
    "# Tensor Surgery & Assembly"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36fcb59e",
   "metadata": {},
   "source": [
    "**Module 1 | Lesson 2a**\n",
    "\n",
    "---\n",
    "\n",
    "## Professor Torchenstein's Grand Directive\n",
    "\n",
    "Mwahahaha! You have summoned your first tensors from the ether! They are... raw. Untamed. Clumps of numerical clay awaiting a master's touch. A lesser mind would be content with their existence, but not you. Not us!\n",
    "\n",
    "Today, we become **tensor surgeons**! We will dissect tensors with the precision of a master anatomist, join them with the skill of a mad scientist, and divide them like a seasoned alchemist splitting compounds. This is not mere data processing; this is **tensor surgery and assembly**! Prepare to wield your digital scalpel and fusion apparatus!\n",
    "\n",
    "![pytorch tensors everywhere](/assets/images/torchenstein_working_computer.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9b33d1d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Your Mission Briefing\n",
    "\n",
    "By the time you escape this surgical theater, you will have mastered the fundamental arts of tensor manipulation:\n",
    "\n",
    "*   **ðŸ”ª The Art of Selection:** Pluck elements, rows, or slices from a tensor with surgical **slicing** precision.\n",
    "*   **ðŸ§¬ Forbidden Fusions:** Combine disparate tensors into unified monstrosities with `torch.cat` and `torch.stack`.\n",
    "*   **âœ‚ï¸ The Great Division:** Split larger tensors into manageable pieces using `torch.split` and `torch.chunk`.\n",
    "\n",
    "**Estimated Time to Completion:** 20 minutes of surgical tensor mastery.\n",
    "\n",
    "**What You'll Need:**\n",
    "*   The wisdom from our [last lesson on summoning tensors](../01_introduction_to_tensors).\n",
    "*   A steady hand for precision cuts and fusions!\n",
    "*   Your PyTorch environment, humming with anticipation.\n",
    "\n",
    "**Coming Next:** In lesson 2b, you'll learn the metamorphic arts of reshaping, squeezing, and permuting tensors to transform their very essence!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8652e14f",
   "metadata": {},
   "source": [
    "## Part 1: The Art of Selection - Slicing\n",
    "\n",
    "Before you can reshape a tensor, you must learn to grasp its individual parts. Indexing is your scalpel, allowing you to perform precision surgery on your data. Slicing is your cleaver, letting you carve out whole sections for your grand experiments.\n",
    "\n",
    "We will start by summoning a test subjectâ€”a 2D tensor brimming with potential! We must also prepare our lab with the usual incantations (`import torch` and `manual_seed`) to ensure our results are repeatable. We are scientists, not chaos-wizards!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0210a48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our subject tensor of shape torch.Size([5, 4]), ripe for dissection:\n",
      "tensor([[42, 67, 76, 14],\n",
      "        [26, 35, 20, 24],\n",
      "        [50, 13, 78, 14],\n",
      "        [10, 54, 31, 72],\n",
      "        [15, 95, 67,  6]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Set the seed for cosmic consistency\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Our test subject: A 2D tensor of integers. Imagine it's a map to a hidden treasure!\n",
    "# Or perhaps experimental results from a daring new potion.\n",
    "subject_tensor = torch.randint(0, 100, (5, 4))\n",
    "\n",
    "print(f\"Our subject tensor of shape {subject_tensor.shape}, ripe for dissection:\")\n",
    "print(subject_tensor)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e22d492f",
   "metadata": {},
   "source": [
    "### Sweeping Strikes: Accessing Rows and Columns\n",
    "\n",
    "Previous lesson: [01_introduction_to_tensors.ipynb](01_introduction_to_tensors.ipynb) gives you the basics for accessing element of a tensor.\n",
    "But what if we require an entire row or column for our dark machinations? For this, we use the colon `:`, the universal symbol for \"give me everything along this dimension!\"\n",
    "\n",
    "- `[row_index, :]` - Fetches the entire row.\n",
    "- `[:, column_index]` - Fetches the entire column.\n",
    "\n",
    "Let's seize the entire 3rd row (index 2) and the 2nd column (index 1).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91b34956",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The third row: tensor([50, 13, 78, 14])\n",
      "Shape of the row: torch.Size([4])\n",
      "\n",
      "The second column: tensor([67, 35, 13, 54, 95])\n",
      "Shape of the column: torch.Size([5])\n"
     ]
    }
   ],
   "source": [
    "# Get the entire 3rd row (index 2)\n",
    "third_row = subject_tensor[2, :] # or simply subject_tensor[2]\n",
    "print(f\"The third row: {third_row}\")\n",
    "print(f\"Shape of the row: {third_row.shape}\\n\")\n",
    "\n",
    "\n",
    "# Get the entire 2nd column (index 1)\n",
    "second_column = subject_tensor[:, 1]\n",
    "print(f\"The second column: {second_column}\")\n",
    "print(f\"Shape of the column: {second_column.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "933e7b12",
   "metadata": {},
   "source": [
    "### Carving Chunks: The Power of Slicing\n",
    "\n",
    "Mere elements are but trivialities! True power lies in carving out entire sub-regions of a tensor. Slicing uses the `start:end` notation. As with all Pythonic sorcery, the `start` is inclusive, but the `end` is **exclusive**.\n",
    "\n",
    "Let us carve out the block containing the 2nd and 3rd rows (indices 1 and 2), and the last two columns (indices 2 and 3).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd3e2f4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our carved sub-tensor:\n",
      "tensor([[20, 24],\n",
      "        [78, 14]])\n",
      "Shape of the sub-tensor: torch.Size([2, 2])\n"
     ]
    }
   ],
   "source": [
    "# Carve out rows 1 and 2, and columns 2 and 3\n",
    "sub_tensor = subject_tensor[1:3, 2:4]\n",
    "\n",
    "print(\"Our carved sub-tensor:\")\n",
    "print(sub_tensor)\n",
    "print(f\"Shape of the sub-tensor: {sub_tensor.shape}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc94f52",
   "metadata": {},
   "source": [
    "### Conditional Conjuring: Boolean Mask Indexing\n",
    "\n",
    "Now for a truly diabolical technique! We can use a **boolean mask** to summon only the elements that meet our nefarious criteria. A boolean mask is a tensor of the same shape as our subject, but it contains only `True` or `False` values. When used for indexing, it returns a 1D tensor containing only the elements where the mask was `True`.\n",
    "\n",
    "Let's find all the alchemical ingredients in our tensor with a value greater than 50!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac2ad0cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The boolean mask (True where value > 50):\n",
      "tensor([[False,  True,  True, False],\n",
      "        [False, False, False, False],\n",
      "        [False, False,  True, False],\n",
      "        [False,  True, False,  True],\n",
      "        [False,  True,  True, False]])\n",
      "\n",
      "Elements greater than 50:\n",
      "tensor([67, 76, 78, 54, 72, 95, 67])\n",
      "Shape of the result: torch.Size([7]) (always a 1D tensor!)\n",
      "\n",
      "Elements between 20 and 40:\n",
      "tensor([26, 35, 24, 31])\n"
     ]
    }
   ],
   "source": [
    "# Create the boolean mask\n",
    "mask = subject_tensor > 50\n",
    "\n",
    "print(\"The boolean mask (True where value > 50):\")\n",
    "print(mask)\n",
    "\n",
    "# Apply the mask\n",
    "selected_elements = subject_tensor[mask]\n",
    "\n",
    "print(\"\\nElements greater than 50:\")\n",
    "print(selected_elements)\n",
    "print(f\"Shape of the result: {selected_elements.shape} (always a 1D tensor!)\")\n",
    "\n",
    "# You can also combine conditions! Mwahaha!\n",
    "# Let's find elements between 20 and 40.\n",
    "mask_combined = (subject_tensor > 20) & (subject_tensor < 40)\n",
    "print(\"\\nElements between 20 and 40:\")\n",
    "print(subject_tensor[mask_combined])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8a2baed",
   "metadata": {},
   "source": [
    "### Your Mission: The Slicer's Gauntlet\n",
    "\n",
    "Enough of my demonstrations! The scalpel is now in your hand. Prove your mastery with these challenges!\n",
    "\n",
    "1.  **The Corner Pocket:** From our `subject_tensor`, select the element in the very last row and last column.\n",
    "2.  **The Central Core:** Select the inner `3x2` block of the `subject_tensor` (that's rows 1-3 and columns 1-2).\n",
    "3.  **The Even Stevens:** Create a boolean mask to select only the elements in `subject_tensor` that are even numbers. (Hint: The modulo operator `%` is your friend!)\n",
    "4.  **The Grand Mutation:** Use your boolean mask from challenge 3 to **change** all even numbers in the `subject_tensor` to the value `-1`. Then, print the mutated tensor. Yes, my apprentice, indexing can be used for assignment! This is a pivotal secret!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0049a5d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1. The Corner Pocket ---\n",
      "The corner element is: 6\n",
      "\n",
      "--- 2. The Central Core ---\n",
      "The central core:\\ntensor([[35, 20],\n",
      "        [13, 78],\n",
      "        [54, 31]])\n",
      "\n",
      "--- 3. The Even Stevens ---\n",
      "The mask for even numbers:\\ntensor([[ True, False,  True,  True],\n",
      "        [ True, False,  True,  True],\n",
      "        [ True, False,  True,  True],\n",
      "        [ True,  True, False,  True],\n",
      "        [False, False, False,  True]])\n",
      "\n",
      "The even numbers themselves: tensor([42, 76, 14, 26, 20, 24, 50, 78, 14, 10, 54, 72,  6])\n",
      "\n",
      "--- 4. The Grand Mutation ---\n",
      "The tensor after mutating even numbers to -1:\n",
      "tensor([[-1, 67, -1, -1],\n",
      "        [-1, 35, -1, -1],\n",
      "        [-1, 13, -1, -1],\n",
      "        [-1, -1, 31, -1],\n",
      "        [15, 95, 67, -1]])\n"
     ]
    }
   ],
   "source": [
    "# Your code for the Slicer's Gauntlet goes here!\n",
    "\n",
    "# --- 1. The Corner Pocket ---\n",
    "print(\"--- 1. The Corner Pocket ---\")\n",
    "corner_element = subject_tensor[-1, -1] # Negative indexing for the win!\n",
    "print(f\"The corner element is: {corner_element.item()}\\n\")\n",
    "\n",
    "# --- 2. The Central Core ---\n",
    "print(\"--- 2. The Central Core ---\")\n",
    "central_core = subject_tensor[1:4, 1:3]\n",
    "print(f\"The central core:\\\\n{central_core}\\n\")\n",
    "\n",
    "# --- 3. The Even Stevens ---\n",
    "print(\"--- 3. The Even Stevens ---\")\n",
    "even_mask = subject_tensor % 2 == 0\n",
    "print(f\"The mask for even numbers:\\\\n{even_mask}\\n\")\n",
    "print(f\"The even numbers themselves: {subject_tensor[even_mask]}\\n\")\n",
    "\n",
    "\n",
    "# --- 4. The Grand Mutation ---\n",
    "print(\"--- 4. The Grand Mutation ---\")\n",
    "# Let's not mutate our original, that would be reckless! Let's clone it first.\n",
    "mutated_tensor = subject_tensor.clone()\n",
    "mutated_tensor[even_mask] = -1\n",
    "print(f\"The tensor after mutating even numbers to -1:\\n{mutated_tensor}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50b0bf35",
   "metadata": {},
   "source": [
    "## Part 2: Forbidden Fusions - Joining Tensors\n",
    "\n",
    "Ah, but dissecting tensors is only half the art! A true master must also know how to **fuse** separate tensors into a single, magnificent whole. Sometimes your data comes in fragmentsâ€”perhaps different batches, different features, or different time steps. You must unite them!\n",
    "\n",
    "We have two primary spells for this dark ritual:\n",
    "- **`torch.cat()`** - The Concatenator! Joins tensors along an *existing* dimension.\n",
    "- **`torch.stack()`** - The Stacker! Creates a *new* dimension and stacks tensors along it.\n",
    "\n",
    "The difference is subtle but critical. Choose wrongly, and your creation will crumble! Let us forge some test subjects to demonstrate this power.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "dea18dcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our test subjects, ready for fusion:\n",
      "Tensor A (shape torch.Size([2, 4])):\n",
      "tensor([[1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.]])\n",
      "\n",
      "Tensor B (shape torch.Size([2, 4])):\n",
      "tensor([[2., 2., 2., 2.],\n",
      "        [2., 2., 2., 2.]])\n",
      "\n",
      "Tensor C (shape torch.Size([2, 4])):\n",
      "tensor([[3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Three 2x3 tensors, our loyal minions awaiting fusion\n",
    "tensor_a = torch.ones(2, 4)\n",
    "tensor_b = torch.ones(2, 4) * 2\n",
    "tensor_c = torch.ones(2, 4) * 3\n",
    "\n",
    "print(\"Our test subjects, ready for fusion:\")\n",
    "print(f\"Tensor A (shape {tensor_a.shape}):\\n{tensor_a}\\n\")\n",
    "print(f\"Tensor B (shape {tensor_b.shape}):\\n{tensor_b}\\n\")\n",
    "print(f\"Tensor C (shape {tensor_c.shape}):\\n{tensor_c}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31f7ebfb",
   "metadata": {},
   "source": [
    "### The Concatenator: `torch.cat()`\n",
    "\n",
    "`torch.cat()` joins tensors along an **existing dimension**. Think of it as gluing them end-to-end. \n",
    "\n",
    "The key rule: *All tensors must have the same shape, except along the dimension you're concatenating!*\n",
    "\n",
    "- `dim=0` (or `axis=0`): Concatenate along rows (vertically stack)\n",
    "- `dim=1` (or `axis=1`): Concatenate along columns (horizontally join)\n",
    "\n",
    "Let us witness this concatenation sorcery!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd117668",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concatenated along dimension 0 (rows) [stacking pancakes ðŸ¥žâ¬†ï¸â¬‡ï¸]:\n",
      "Result shape: torch.Size([6, 4])\n",
      "Result:\n",
      "tensor([[1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [2., 2., 2., 2.],\n",
      "        [2., 2., 2., 2.],\n",
      "        [3., 3., 3., 3.],\n",
      "        [3., 3., 3., 3.]])\n",
      "\n",
      "Concatenated along dimension 1 (columns) [laying bricks side by side ðŸ§±ðŸ§±ðŸ§±]:\n",
      "Result shape: torch.Size([2, 12])\n",
      "Result:\n",
      "tensor([[1., 1., 1., 1., 2., 2., 2., 2., 3., 3., 3., 3.],\n",
      "        [1., 1., 1., 1., 2., 2., 2., 2., 3., 3., 3., 3.]])\n"
     ]
    }
   ],
   "source": [
    "# Concatenating along dimension 0 (rows) - like stacking pancakes! ðŸ¥žâ¬†ï¸â¬‡ï¸\n",
    "cat_dim0 = torch.cat([tensor_a, tensor_b, tensor_c], dim=0)\n",
    "print(\"Concatenated along dimension 0 (rows) [stacking pancakes ðŸ¥žâ¬†ï¸â¬‡ï¸]:\")\n",
    "print(f\"Result shape: {cat_dim0.shape}\")\n",
    "print(f\"Result:\\n{cat_dim0}\\n\")\n",
    "\n",
    "# Concatenating along dimension 1 (columns) - like laying bricks side by side! ðŸ§±ðŸ§±ðŸ§±\n",
    "cat_dim1 = torch.cat([tensor_a, tensor_b, tensor_c], dim=1)\n",
    "print(\"Concatenated along dimension 1 (columns) [laying bricks side by side ðŸ§±ðŸ§±ðŸ§±]:\")\n",
    "print(f\"Result shape: {cat_dim1.shape}\")\n",
    "print(f\"Result:\\n{cat_dim1}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4e4c43",
   "metadata": {},
   "source": [
    "### The Concatenation Rules: When Shapes Don't Match\n",
    "\n",
    "Now, let us test the fundamental law of concatenation with unequal tensors! Remember: **All tensors must have the same shape, except along the dimension you're concatenating.**\n",
    "\n",
    "Eg1. If you joining 2D matrices along rows (dim=0) the number of collumns should be the same. \n",
    "\n",
    "Let's create two tensors with different shapes and see what happens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ab05e2af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wide tensor [big cake ðŸŽ‚] (shape torch.Size([3, 8])):\n",
      "tensor([[4., 4., 4., 4., 4., 4., 4., 4.],\n",
      "        [4., 4., 4., 4., 4., 4., 4., 4.],\n",
      "        [4., 4., 4., 4., 4., 4., 4., 4.]])\n",
      "\n",
      "Narrow tensor [small cupcake ðŸ§ ] (shape torch.Size([3, 2])):\n",
      "tensor([[5., 5.],\n",
      "        [5., 5.],\n",
      "        [5., 5.]])\n",
      "\n",
      "âŒ Attempting to concatenate along dimension 0 (rows), stack cake on top of cupcake ...\n",
      "ðŸŽ‚/ðŸ§ This couldn't work! \n",
      "Error as expected: Sizes of tensors must match except in dimension 0. Expected size 8 but got size 2 for tensor number 1 in the list.\n",
      "Our unequal test subjects:\n",
      "Wide tensor [big cake ðŸŽ‚] (torch.Size([3, 8])):\n",
      "tensor([[4., 4., 4., 4., 4., 4., 4., 4.],\n",
      "        [4., 4., 4., 4., 4., 4., 4., 4.],\n",
      "        [4., 4., 4., 4., 4., 4., 4., 4.]])\n",
      "\n",
      "Narrow tensor [small cupcake ðŸ§] (torch.Size([3, 2])):\n",
      "tensor([[5., 5.],\n",
      "        [5., 5.],\n",
      "        [5., 5.]])\n",
      "\n",
      "âœ… Concatenating along dimension 1 (columns) - SUCCESS!\n",
      "Result shape: torch.Size([3, 10])\n",
      "Result:\n",
      "tensor([[4., 4., 4., 4., 4., 4., 4., 4., 5., 5.],\n",
      "        [4., 4., 4., 4., 4., 4., 4., 4., 5., 5.],\n",
      "        [4., 4., 4., 4., 4., 4., 4., 4., 5., 5.]])\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create tensors with different shapes\n",
    "tensor_wide = torch.ones(3, 8) * 4   # 3x5 tensor filled with 4s\n",
    "tensor_narrow = torch.ones(3, 2) * 5  # 3x2 tensor filled with 5s\n",
    "\n",
    "print(f\"Wide tensor [big cake ðŸŽ‚] (shape {tensor_wide.shape}):\\n{tensor_wide}\\n\")\n",
    "print(f\"Narrow tensor [small cupcake ðŸ§ ] (shape {tensor_narrow.shape}):\\n{tensor_narrow}\\n\")\n",
    "\n",
    "# This FAILS: Concatenating along dimension 0, you can stack pancakes with different sizes \n",
    "# They have different column counts (5 vs 2)\n",
    "print(\"âŒ Attempting to concatenate along dimension 0 (rows), stack cake on top of cupcake ...\")\n",
    "try:\n",
    "    cat_cols_fail = torch.cat([tensor_wide, tensor_narrow], dim=0)\n",
    "except RuntimeError as e:\n",
    "    print(f\"ðŸŽ‚/ðŸ§ This couldn't work! \\nError as expected: {str(e)}\")\n",
    "    \n",
    "\n",
    "print(\"Our unequal test subjects:\")\n",
    "print(f\"Wide tensor [big cake ðŸŽ‚] ({tensor_wide.shape}):\\n{tensor_wide}\\n\")\n",
    "print(f\"Narrow tensor [small cupcake ðŸ§] ({tensor_narrow.shape}):\\n{tensor_narrow}\\n\")\n",
    "\n",
    "# This WORKS: Concatenating along dimension 1 (columns)\n",
    "# Both have 3 rows, so we can lay them side by side horizontally\n",
    "print(\"âœ… Concatenating along dimension 1 (columns) - SUCCESS!\")\n",
    "cat_rows_success = torch.cat([tensor_wide, tensor_narrow], dim=1)\n",
    "print(f\"Result shape: {cat_rows_success.shape}\")\n",
    "print(f\"Result:\\n{cat_rows_success}\\n\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "859dab6e",
   "metadata": {},
   "source": [
    "### The Stacker: `torch.stack()` - Creating New Dimensions!\n",
    "\n",
    "`torch.stack()` is more dramatic than concatenation! It **creates an entirely new dimension** and places each tensor along it. Think of it as the difference between:\n",
    "- **Concatenation**: Gluing pieces end-to-end in the same plane ðŸ§©âž¡ï¸ðŸ§©\n",
    "- **Stacking**: Creating a whole new layer/dimension ðŸ“š (like stacking books on top of each other)\n",
    "\n",
    "**Critical Rule**: All input tensors must have *identical* shapesâ€”no exceptions!\n",
    "\n",
    "Let's start simple and build our intuition step by step...\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dda3aed",
   "metadata": {},
   "source": [
    "#### Step 1: Stacking 1D Tensors â†’ Creating a 2D Matrix\n",
    "\n",
    "Let's start with something simple: three 1D tensors (think of them as rulers ðŸ“). When we stack them, we create a 2D matrix!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2a641b6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our three rulers ðŸ“ (1D tensors):\n",
      "Ruler 1: tensor([1, 2, 3, 4]) (shape: torch.Size([4]))\n",
      "Ruler 2: tensor([10, 20, 30, 40]) (shape: torch.Size([4]))\n",
      "Ruler 3: tensor([100, 200, 300, 400]) (shape: torch.Size([4]))\n",
      "\n",
      "Stacked rulers ðŸŸ° (dim=0) - like placing rulers on top of each other:\n",
      "Result shape: torch.Size([3, 4])\n",
      "Result:\n",
      "tensor([[  1,   2,   3,   4],\n",
      "        [ 10,  20,  30,  40],\n",
      "        [100, 200, 300, 400]])\n",
      "\n",
      "Access individual rulers:\n",
      "First ruler:  tensor([1, 2, 3, 4])\n",
      "Second ruler: tensor([10, 20, 30, 40])\n",
      "Third ruler:  tensor([100, 200, 300, 400])\n"
     ]
    }
   ],
   "source": [
    "# Let's create simple 1D tensors first\n",
    "ruler_1 = torch.tensor([1, 2, 3, 4])      # A ruler with numbers 1,2,3,4\n",
    "ruler_2 = torch.tensor([10, 20, 30, 40])  # A ruler with numbers 10,20,30,40  \n",
    "ruler_3 = torch.tensor([100, 200, 300, 400]) # A ruler with numbers 100,200,300,400\n",
    "\n",
    "print(\"Our three rulers ðŸ“ (1D tensors):\")\n",
    "print(f\"Ruler 1: {ruler_1} (shape: {ruler_1.shape})\")\n",
    "print(f\"Ruler 2: {ruler_2} (shape: {ruler_2.shape})\")\n",
    "print(f\"Ruler 3: {ruler_3} (shape: {ruler_3.shape})\\n\")\n",
    "\n",
    "# Stack them to create a 2D matrix (like putting rulers on top of each other)\n",
    "stacked_rulers = torch.stack([ruler_1, ruler_2, ruler_3], dim=0)\n",
    "print(\"Stacked rulers ðŸŸ° (dim=0) - like placing rulers on top of each other:\")\n",
    "print(f\"Result shape: {stacked_rulers.shape}\")  # Notice: (3,4) - we added a new dimension!\n",
    "print(f\"Result:\\n{stacked_rulers}\\n\")\n",
    "\n",
    "# Each \"ruler\" is now accessible as a row\n",
    "print(\"Access individual rulers:\")\n",
    "print(f\"First ruler:  {stacked_rulers[0]}\")\n",
    "print(f\"Second ruler: {stacked_rulers[1]}\")  \n",
    "print(f\"Third ruler:  {stacked_rulers[2]}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "01c3fdcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stacked rulers â¸ï¸ (dim=1) - like arranging rulers side by side:\n",
      "Result shape: torch.Size([4, 3])\n",
      "Result:\n",
      "tensor([[  1,  10, 100],\n",
      "        [  2,  20, 200],\n",
      "        [  3,  30, 300],\n",
      "        [  4,  40, 400]])\n",
      "\n",
      "Notice the pattern:\n",
      "Each row shows the 1st, 2nd, 3rd... element from ALL rulers\n",
      "Position 0 from all rulers: tensor([  1,  10, 100])\n",
      "Position 1 from all rulers: tensor([  2,  20, 200])\n"
     ]
    }
   ],
   "source": [
    "# We can also stack along dimension 1 (different arrangement)\n",
    "stack_dim1 = torch.stack([ruler_1, ruler_2, ruler_3], dim=1)\n",
    "print(\"Stacked rulers â¸ï¸ (dim=1) - like arranging rulers side by side:\")\n",
    "print(f\"Result shape: {stack_dim1.shape}\")  # Notice: (4,3) - different arrangement!\n",
    "print(f\"Result:\\n{stack_dim1}\\n\")\n",
    "\n",
    "# Each column now represents values from all three rulers at the same position\n",
    "print(\"Notice the pattern:\")\n",
    "print(\"Each row shows the 1st, 2nd, 3rd... element from ALL rulers\")\n",
    "print(f\"Position 0 from all rulers: {stack_dim1[0]}\")  # [1, 10, 100]\n",
    "print(f\"Position 1 from all rulers: {stack_dim1[1]}\")  # [2, 20, 200]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4cc64cb",
   "metadata": {},
   "source": [
    "#### Step 2: Stacking 2D Tensors â†’ Creating a 3D Cube!\n",
    "\n",
    "Now for the mind-bending part! When we stack 2D tensors (matrices), we create a **3D tensor**. Think of it like:\n",
    "\n",
    "**ðŸ“„ 2D tensor** = A page from a book (has rows and columns)  \n",
    "**ðŸ“– Stacking 2D tensors** = Creating a book with multiple pages  \n",
    "**ðŸ“š 3D tensor** = The entire book! (pages Ã— rows Ã— columns)\n",
    "\n",
    "**Key Metaphors to Remember:**\n",
    "- **Book metaphor**: `tensor[page][row][column]` ðŸ“š\n",
    "- **RGB image**: `tensor[channel][height][width]` ðŸ–¼ï¸ (like stacking color layers: Red, Green, Blue)\n",
    "\n",
    "These metaphors help you \"see\" how changing the dimension you stack along changes the meaning of each axis in your tensor. ðŸ¤“\n",
    "\n",
    "Let's see this dimensional magic in action:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5f737c86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our three pages (2D tensors):\n",
      "ðŸ“„ Page 1 (shape torch.Size([5, 2])):\n",
      "tensor([[1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.]])\n",
      "\n",
      "ðŸ“„ Page 2 (shape torch.Size([5, 2])):\n",
      "tensor([[2., 2.],\n",
      "        [2., 2.],\n",
      "        [2., 2.],\n",
      "        [2., 2.],\n",
      "        [2., 2.]])\n",
      "\n",
      "ðŸ“„ Page 3 (shape torch.Size([5, 2])):\n",
      "tensor([[3., 3.],\n",
      "        [3., 3.],\n",
      "        [3., 3.],\n",
      "        [3., 3.],\n",
      "        [3., 3.]])\n",
      "\n",
      "ðŸ“š BEHOLD! Our 3D book (stacked along dim=0):\n",
      "Book shape: torch.Size([3, 5, 2])\n",
      "Full book:\n",
      "tensor([[[1., 1.],\n",
      "         [1., 1.],\n",
      "         [1., 1.],\n",
      "         [1., 1.],\n",
      "         [1., 1.]],\n",
      "\n",
      "        [[2., 2.],\n",
      "         [2., 2.],\n",
      "         [2., 2.],\n",
      "         [2., 2.],\n",
      "         [2., 2.]],\n",
      "\n",
      "        [[3., 3.],\n",
      "         [3., 3.],\n",
      "         [3., 3.],\n",
      "         [3., 3.],\n",
      "         [3., 3.]]])\n",
      "\n",
      "ðŸ” Accessing different parts of our 3D tensor:\n",
      "ðŸ“– Entire first page (book[0]):\n",
      "tensor([[1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.]])\n",
      "\n",
      "ðŸ“ First row of second page (book[1, 0]): tensor([2., 2.])\n",
      "ðŸŽ¯ Specific element - page 2, row 1, column 2 (book[1, 0, 1]): 2.0\n",
      "\n",
      "ðŸ¤” Think of it as: Book[page_number][row_number][column_number]\n"
     ]
    }
   ],
   "source": [
    "# Create three 2D \"pages\" for our book\n",
    "page_1 = torch.ones(5, 2) * 1    # Page 1: all 1s\n",
    "page_2 = torch.ones(5, 2) * 2    # Page 2: all 2s  \n",
    "page_3 = torch.ones(5, 2) * 3    # Page 3: all 3s\n",
    "\n",
    "print(\"Our three pages (2D tensors):\")\n",
    "print(f\"ðŸ“„ Page 1 (shape {page_1.shape}):\\n{page_1}\\n\")\n",
    "print(f\"ðŸ“„ Page 2 (shape {page_2.shape}):\\n{page_2}\\n\") \n",
    "print(f\"ðŸ“„ Page 3 (shape {page_3.shape}):\\n{page_3}\\n\")\n",
    "\n",
    "# Stack them along dimension 0 to create a 3D \"book\"\n",
    "book = torch.stack([page_1, page_2, page_3], dim=0)\n",
    "print(\"ðŸ“š BEHOLD! Our 3D book (stacked along dim=0):\")\n",
    "print(f\"Book shape: {book.shape}\")  # (3, 2, 3) = (pages, rows, columns)\n",
    "print(f\"Full book:\\n{book}\\n\")\n",
    "\n",
    "# Now we can access individual pages, rows, or even specific elements!\n",
    "print(\"ðŸ” Accessing different parts of our 3D tensor:\")\n",
    "print(f\"ðŸ“– Entire first page (book[0]):\\n{book[0]}\\n\")\n",
    "print(f\"ðŸ“ First row of second page (book[1, 0]): {book[1, 0]}\")\n",
    "print(f\"ðŸŽ¯ Specific element - page 2, row 1, column 2 (book[1, 0, 1]): {book[1, 0, 1].item()}\")\n",
    "\n",
    "print(f\"\\nðŸ¤” Think of it as: Book[page_number][row_number][column_number]\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "456cbd69",
   "metadata": {},
   "source": [
    "#### Step 3: The Dimension Dance - Where You Stack Matters!\n",
    "\n",
    "When stacking 2D tensors, **which dimension you choose creates very different 3D shapes**! \n",
    "\n",
    "**ðŸ§± The Clay Tablet Box Metaphor:**\n",
    "\n",
    "For better understanding, imagine that when you use the `stack()` method, a **new 3D package** is created with an extended dimension. If you stack 2D objects (like clay tablets), you first create a 3D box, then arrange your 2D tablets inside.\n",
    "\n",
    "Picture this: You have three identical clay tablets ðŸ§± and an empty 3D box ðŸ“¦. There are **exactly 3 different ways** to arrange them inside!\n",
    "\n",
    "**ðŸ“ 3D Box Coordinates (always viewed from the same angle):**\n",
    "- **Depth** = `dim=0` (front to back)\n",
    "- **Height** = `dim=1` (bottom to top)  \n",
    "- **Width** = `dim=2` (left to right)\n",
    "\n",
    "**ðŸŽ¯ The Three Stacking Strategies:**\n",
    "\n",
    "- **`dim=0`**: **Stack tablets front-to-back** â†’ Shape `(tablets, rows, cols)` ðŸ“š  \n",
    "  *Each tablet goes deeper into the box, one behind the other*\n",
    "\n",
    "- **`dim=1`**: **Stack tablets bottom-to-top** â†’ Shape `(rows, tablets, cols)` ðŸ—‚ï¸  \n",
    "  *Each tablet is placed higher in the box, building upward* - we start with last tablets\n",
    "\n",
    "- **`dim=2`**: **Slide tablets left-to-right** â†’ Shape `(rows, cols, tablets)` ðŸ“‘  \n",
    "  *Each tablet slides sideways, arranged side by side*\n",
    "\n",
    "The dimension you choose determines **which direction** your tablets extend in the 3D space!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1825fb2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ§± Starting with three identical clay tablets:\n",
      "Each tablet shape: torch.Size([5, 2]) (5 rows, 2 columns)\n",
      "\n",
      "ðŸ“š Stacking front-to-back (dim=0): Shape torch.Size([3, 5, 2])\n",
      "   (depth=3, heith=5, width=2) - tablets go deeper into the box\n",
      "Front of the box):\n",
      "tensor([[1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.],\n",
      "        [1., 1.]])\n",
      "\n",
      "ðŸ—‚ï¸ Stacking bottom-to-top (dim=1): Shape torch.Size([5, 3, 2])\n",
      "   (depth=5, heigh=3, width=2) - tablets build upward\n",
      "Notice how each level contains one slice from ALL tablets:\n",
      "Front of the box):\n",
      "tensor([[1., 1.],\n",
      "        [2., 2.],\n",
      "        [3., 3.]])\n",
      "\n",
      "ðŸ“‘ Sliding left-to-right (dim=2): Shape torch.Size([5, 2, 3])\n",
      "   (depth=5, heigh=2, width=3) - tablets slide sideways\n",
      "Each position now contains values from ALL tablets:\n",
      "Front of the box):\n",
      "tensor([[1., 2., 3.],\n",
      "        [1., 2., 3.]])\n",
      "\n",
      "\n",
      "ðŸŽ¯ Key Insight: The dimension you choose determines WHERE the tablets extend!\n",
      "   dim=0: Tablets extend front-to-back (depth)\n",
      "   dim=1: Tablets extend bottom-to-top (height)\n",
      "   dim=2: Tablets extend left-to-right (width)\n"
     ]
    }
   ],
   "source": [
    "# Let's arrange our three 2D clay tablets in three different ways!\n",
    "print(\"ðŸ§± Starting with three identical clay tablets:\")\n",
    "print(f\"Each tablet shape: {page_1.shape} (5 rows, 2 columns)\")\n",
    "\n",
    "# dim=0: Stack tablets front-to-back (into the box)\n",
    "stack_dim0 = torch.stack([page_1, page_2, page_3], dim=0)\n",
    "print(f\"\\nðŸ“š Stacking front-to-back (dim=0): Shape {stack_dim0.shape}\")\n",
    "print(\"   (depth=3, heith=5, width=2) - tablets go deeper into the box\")\n",
    "print(f\"Front of the box):\\n{stack_dim0[0]}\\n\")\n",
    "\n",
    "# dim=1: Stack tablets bottom-to-top (building upward)  \n",
    "stack_dim1 = torch.stack([page_1, page_2, page_3], dim=1)\n",
    "print(f\"ðŸ—‚ï¸ Stacking bottom-to-top (dim=1): Shape {stack_dim1.shape}\")\n",
    "print(\"   (depth=5, heigh=3, width=2) - tablets build upward\")\n",
    "print(\"Notice how each level contains one slice from ALL tablets:\")\n",
    "print(f\"Front of the box):\\n{stack_dim1[0]}\\n\")\n",
    "\n",
    "# dim=2: Slide tablets left-to-right (arranging sideways)\n",
    "stack_dim2 = torch.stack([page_1, page_2, page_3], dim=2) \n",
    "print(f\"ðŸ“‘ Sliding left-to-right (dim=2): Shape {stack_dim2.shape}\")\n",
    "print(\"   (depth=5, heigh=2, width=3) - tablets slide sideways\")\n",
    "print(\"Each position now contains values from ALL tablets:\")\n",
    "print(f\"Front of the box):\\n{stack_dim2[0]}\\n\")\n",
    "\n",
    "print(\"\\nðŸŽ¯ Key Insight: The dimension you choose determines WHERE the tablets extend!\")\n",
    "print(\"   dim=0: Tablets extend front-to-back (depth)\")  \n",
    "print(\"   dim=1: Tablets extend bottom-to-top (height)\")\n",
    "print(\"   dim=2: Tablets extend left-to-right (width)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cbfb8c8",
   "metadata": {},
   "source": [
    "### The Fusion Dilemma: When to Cat vs. Stack?\n",
    "\n",
    "This choice torments many apprentices! Let me illuminate the path:\n",
    "\n",
    "**Use `torch.cat()` when:**\n",
    "- Tensors represent *different parts* of the same data (e.g., different batches of images, different chunks of text)\n",
    "- You want to *extend* an existing dimension\n",
    "- Example: Concatenating multiple batches of training data\n",
    "\n",
    "**Use `torch.stack()` when:**\n",
    "- Tensors represent *parallel data* of the same type (e.g., predictions from different models, different time steps)  \n",
    "- You need to create a *new dimension* to organize the data\n",
    "- Example: Combining RGB channels to form a color image, or collecting multiple predictions\n",
    "\n",
    "Observe this real-world scenario!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7798acea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Individual images:\n",
      "Image 1 shape: torch.Size([16, 24])\n",
      "Image 2 shape: torch.Size([16, 24])\n",
      "Image 3 shape: torch.Size([16, 24])\n",
      "\n",
      "Batch of images shape: torch.Size([3, 16, 24])\n",
      "Perfect for feeding into a neural network!\n",
      "\n",
      "RGB image shape: torch.Size([3, 32, 32])\n",
      "The classic (C, H, W) format!\n"
     ]
    }
   ],
   "source": [
    "# Real-world example: Building a batch of images\n",
    "# Imagine these are grayscale images (height=16, width=24)\n",
    "image1 = torch.randn(16, 24)  \n",
    "image2 = torch.randn(16, 24)\n",
    "image3 = torch.randn(16, 24)\n",
    "\n",
    "print(\"Individual images:\")\n",
    "print(f\"Image 1 shape: {image1.shape}\")\n",
    "print(f\"Image 2 shape: {image2.shape}\")  \n",
    "print(f\"Image 3 shape: {image3.shape}\\n\")\n",
    "\n",
    "# STACK them to create a batch (batch_size=3, height=2, width=3)\n",
    "image_batch = torch.stack([image1, image2, image3], dim=0)\n",
    "print(f\"Batch of images shape: {image_batch.shape}\")\n",
    "print(\"Perfect for feeding into a neural network!\\n\")\n",
    "\n",
    "# Now imagine we have RGB channels for one image\n",
    "red_channel = torch.randn(32, 32)\n",
    "green_channel = torch.randn(32, 32) \n",
    "blue_channel = torch.randn(32, 32)\n",
    "\n",
    "# STACK them to create RGB image (channels=3, height=2, width=3)\n",
    "rgb_image = torch.stack([red_channel, green_channel, blue_channel], dim=0)\n",
    "print(f\"RGB image shape: {rgb_image.shape}\")\n",
    "print(\"The classic (C, H, W) format!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8fe88b2",
   "metadata": {},
   "source": [
    "### Your Mission: The Fusion Master's Gauntlet\n",
    "\n",
    "The theory is yoursâ€”now prove your mastery! Complete these fusion challenges:\n",
    "\n",
    "1. **The Triple Stack**: Create three 1D tensors of length 4 with different values. Stack them to create a 2D tensor of shape `(3, 4)`.\n",
    "\n",
    "2. **The Horizontal Fusion**: Create two 2D tensors of shape `(3, 2)`. Concatenate them horizontally to create a `(3, 4)` tensor.\n",
    "\n",
    "3. **The Batch Builder**: You have 5 individual \"samples\" (each a 1D tensor of length 3). Stack them to create a proper batch tensor of shape `(5, 3)` suitable for training.\n",
    "\n",
    "4. **The Dimension Disaster**: Try to concatenate two tensors with different shapes: `(2, 3)` and `(2, 4)` along dimension 0. Observe the error messageâ€”it's quite educational! Then fix it by concatenating along dimension 1 instead.\n",
    "\n",
    "5. **The Multi-Fusion**: Create a tensor of shape `(2, 6)` by first stacking three `(2, 2)` tensors, then concatenating the result with another `(3, 6)` tensor. This requires combining both operations!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "090c24f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1. The Triple Stack ---\n",
      "Triple stack result:\n",
      "tensor([[ 1,  2,  3,  4],\n",
      "        [ 5,  6,  7,  8],\n",
      "        [ 9, 10, 11, 12]])\n",
      "Shape: torch.Size([3, 4])\n",
      "\n",
      "--- 2. The Horizontal Fusion ---\n",
      "Horizontal fusion shape: torch.Size([3, 4])\n",
      "\n",
      "--- 3. The Batch Builder ---\n",
      "Batch shape: torch.Size([5, 3])\n",
      "Ready for neural network training!\n",
      "\n",
      "--- 4. The Dimension Disaster ---\n",
      "Error (as expected): Sizes of tensors must match except in dimension 0. Expected size 3 but got size 4 for tensor number 1 in the list.\n",
      "Fixed by concatenating along dim 1: torch.Size([2, 7])\n",
      "\n",
      "--- 5. The Multi-Fusion ---\n",
      "Multi-fusion result shape: torch.Size([2, 6])\n",
      "The key was concatenating, not stacking!\n"
     ]
    }
   ],
   "source": [
    "# Your code for the Fusion Master's Gauntlet goes here!\n",
    "\n",
    "print(\"--- 1. The Triple Stack ---\")\n",
    "tensor1 = torch.tensor([1, 2, 3, 4])\n",
    "tensor2 = torch.tensor([5, 6, 7, 8]) \n",
    "tensor3 = torch.tensor([9, 10, 11, 12])\n",
    "triple_stack = torch.stack([tensor1, tensor2, tensor3], dim=0)\n",
    "print(f\"Triple stack result:\\n{triple_stack}\")\n",
    "print(f\"Shape: {triple_stack.shape}\\n\")\n",
    "\n",
    "print(\"--- 2. The Horizontal Fusion ---\")\n",
    "left_tensor = torch.randn(3, 2)\n",
    "right_tensor = torch.randn(3, 2)\n",
    "horizontal_fusion = torch.cat([left_tensor, right_tensor], dim=1)\n",
    "print(f\"Horizontal fusion shape: {horizontal_fusion.shape}\\n\")\n",
    "\n",
    "print(\"--- 3. The Batch Builder ---\")\n",
    "samples = [torch.randn(3) for _ in range(5)]  # 5 samples of length 3\n",
    "batch = torch.stack(samples, dim=0)\n",
    "print(f\"Batch shape: {batch.shape}\")\n",
    "print(\"Ready for neural network training!\\n\")\n",
    "\n",
    "print(\"--- 4. The Dimension Disaster ---\")\n",
    "disaster_a = torch.randn(2, 3)\n",
    "disaster_b = torch.randn(2, 4)\n",
    "try:\n",
    "    # This will fail!\n",
    "    bad_cat = torch.cat([disaster_a, disaster_b], dim=0)\n",
    "except RuntimeError as e:\n",
    "    print(f\"Error (as expected): {e}\")\n",
    "    \n",
    "# The fix: concatenate along dimension 1\n",
    "good_cat = torch.cat([disaster_a, disaster_b], dim=1)  \n",
    "print(f\"Fixed by concatenating along dim 1: {good_cat.shape}\\n\")\n",
    "\n",
    "print(\"--- 5. The Multi-Fusion ---\")\n",
    "# First, create and stack three (2,2) tensors\n",
    "small_tensors = [torch.randn(2, 2) for _ in range(3)]\n",
    "# Actually, let's concatenate the (2,2) tensors along dim=1 first\n",
    "concat_part = torch.cat(small_tensors, dim=1)  # Shape: (2, 6)\n",
    "print(f\"Multi-fusion result shape: {concat_part.shape}\")\n",
    "print(\"The key was concatenating, not stacking!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "924651db",
   "metadata": {},
   "source": [
    "## Part 3: The Great Division - Splitting Tensors\n",
    "\n",
    "Ah, but the surgeon's art is not complete until we master both creation AND division! Just as we learned to fuse tensors, we must also learn to **split** them apart. Sometimes your grand creation becomes too unwieldy, or you need to distribute pieces to different parts of your neural network. \n",
    "\n",
    "Fear not! PyTorch provides elegant tools for this delicate operation:\n",
    "- **`torch.split()`** - The Precise Slicer! Splits a tensor into chunks of a specified size.\n",
    "- **`torch.chunk()`** - The Equal Divider! Splits a tensor into a specified number of roughly equal chunks.\n",
    "- **`torch.unbind()`** - The Dimension Destroyer! Removes a dimension by splitting along it.\n",
    "\n",
    "Let us prepare a worthy subject for our division experiments!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1327b211",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Our subject for division experiments:\n",
      "Shape: torch.Size([4, 6])\n",
      "Tensor:\n",
      "tensor([[ 1,  2,  3,  4,  5,  6],\n",
      "        [ 7,  8,  9, 10, 11, 12],\n",
      "        [13, 14, 15, 16, 17, 18],\n",
      "        [19, 20, 21, 22, 23, 24]])\n",
      "\n",
      "Think of this as a chocolate bar with 4 rows and 6 columns!\n",
      "We'll learn different ways to break it apart... ðŸ«\n"
     ]
    }
   ],
   "source": [
    "# Create a test subject for our splitting experiments\n",
    "split_subject = torch.arange(1, 25).reshape(4, 6)  # A 4x6 tensor with numbers 1-24\n",
    "\n",
    "print(\"Our subject for division experiments:\")\n",
    "print(f\"Shape: {split_subject.shape}\")\n",
    "print(f\"Tensor:\\n{split_subject}\\n\")\n",
    "\n",
    "print(\"Think of this as a chocolate bar with 4 rows and 6 columns!\")\n",
    "print(\"We'll learn different ways to break it apart... ðŸ«\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7eefb9a",
   "metadata": {},
   "source": [
    "### The Precise Slicer: `torch.split()`\n",
    "\n",
    "`torch.split(tensor, split_size, dim)` cuts your tensor into pieces of a **specific size** along a chosen dimension. Think of it as a precision guillotine that creates uniform chunks!\n",
    "\n",
    "**Key Parameters:**\n",
    "- `split_size`: Size of each chunk (except possibly the last one)\n",
    "- `dim`: Which dimension to split along\n",
    "- **Returns:** A tuple of tensors (not a single tensor!)\n",
    "\n",
    "Let's slice our chocolate bar into precise pieces!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6bb7a87b",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "'(' was never closed (2944205258.py, line 3)",
     "output_type": "error",
     "traceback": [
      "  \u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[31m    \u001b[39m\u001b[31mtorch.split(\u001b[39m\n               ^\n\u001b[31mSyntaxError\u001b[39m\u001b[31m:\u001b[39m '(' was never closed\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Split along dimension 0 (rows) - like cutting horizontal slices of chocolate\n",
    "row_splits = torch.split(split_subject, split_size=2, dim=0)\n",
    "\n",
    "print(\"ðŸ« Split into row pieces (split_size=2, dim=0):\")\n",
    "print(f\"Number of pieces: {len(row_splits)}\")\n",
    "for i, piece in enumerate(row_splits):\n",
    "    print(f\"Piece {i+1} shape {piece.shape}:\\n{piece}\\n\")\n",
    "\n",
    "# Split along dimension 1 (columns) - like cutting vertical slices  \n",
    "col_splits = torch.split(split_subject, split_size=3, dim=1)\n",
    "print(\"ðŸ« Split into column pieces (split_size=3, dim=1):\")\n",
    "print(f\"Number of pieces: {len(col_splits)}\")\n",
    "for i, piece in enumerate(col_splits):\n",
    "    print(f\"Piece {i+1} shape {piece.shape}:\\n{piece}\\n\")\n",
    "\n",
    "print(\"ðŸŽ¯ Notice: torch.split() returns a TUPLE of tensors, not a single tensor!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "303d0af6",
   "metadata": {},
   "source": [
    "### The Equal Divider: `torch.chunk()`\n",
    "\n",
    "`torch.chunk(tensor, chunks, dim)` divides your tensor into a **specified number** of roughly equal pieces. It's like asking: \"I need exactly 3 pieces, make them as equal as possible!\"\n",
    "\n",
    "**Key Difference from `split()`:**\n",
    "- `torch.split()`: \"Cut into pieces of size X\" (you control piece size)\n",
    "- `torch.chunk()`: \"Cut into exactly N pieces\" (you control number of pieces)\n",
    "\n",
    "If the dimension doesn't divide evenly, the last chunk will be smaller.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "fd35b6d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ‚ï¸ Chunk into exactly 2 row pieces:\n",
      "Number of chunks: 2\n",
      "Chunk 1 shape torch.Size([2, 6]):\n",
      "tensor([[ 1,  2,  3,  4,  5,  6],\n",
      "        [ 7,  8,  9, 10, 11, 12]])\n",
      "\n",
      "Chunk 2 shape torch.Size([2, 6]):\n",
      "tensor([[13, 14, 15, 16, 17, 18],\n",
      "        [19, 20, 21, 22, 23, 24]])\n",
      "\n",
      "âœ‚ï¸ Chunk into exactly 3 column pieces:\n",
      "Number of chunks: 3\n",
      "Chunk 1 shape torch.Size([4, 2]):\n",
      "tensor([[ 1,  2],\n",
      "        [ 7,  8],\n",
      "        [13, 14],\n",
      "        [19, 20]])\n",
      "\n",
      "Chunk 2 shape torch.Size([4, 2]):\n",
      "tensor([[ 3,  4],\n",
      "        [ 9, 10],\n",
      "        [15, 16],\n",
      "        [21, 22]])\n",
      "\n",
      "Chunk 3 shape torch.Size([4, 2]):\n",
      "tensor([[ 5,  6],\n",
      "        [11, 12],\n",
      "        [17, 18],\n",
      "        [23, 24]])\n",
      "\n",
      "âœ‚ï¸ Chunk into 4 pieces (uneven division - 6 columns Ã· 4 chunks):\n",
      "Number of chunks: 3\n",
      "Chunk 1 shape torch.Size([4, 2]): 2 columns\n",
      "Chunk 2 shape torch.Size([4, 2]): 2 columns\n",
      "Chunk 3 shape torch.Size([4, 2]): 2 columns\n",
      "Notice: First 2 chunks get 2 columns each, last 2 chunks get 1 column each!\n"
     ]
    }
   ],
   "source": [
    "# Chunk into exactly 2 pieces along dimension 0 (rows)\n",
    "row_chunks = torch.chunk(split_subject, chunks=2, dim=0)\n",
    "print(\"âœ‚ï¸ Chunk into exactly 2 row pieces:\")\n",
    "print(f\"Number of chunks: {len(row_chunks)}\")\n",
    "for i, chunk in enumerate(row_chunks):\n",
    "    print(f\"Chunk {i+1} shape {chunk.shape}:\\n{chunk}\\n\")\n",
    "\n",
    "# Chunk into exactly 3 pieces along dimension 1 (columns)\n",
    "# Note: 6 columns Ã· 3 chunks = 2 columns per chunk (perfect division!)\n",
    "col_chunks = torch.chunk(split_subject, chunks=3, dim=1)\n",
    "print(\"âœ‚ï¸ Chunk into exactly 3 column pieces:\")\n",
    "print(f\"Number of chunks: {len(col_chunks)}\")\n",
    "for i, chunk in enumerate(col_chunks):\n",
    "    print(f\"Chunk {i+1} shape {chunk.shape}:\\n{chunk}\\n\")\n",
    "\n",
    "# What happens with uneven division? Let's try 4 chunks from 6 columns\n",
    "uneven_chunks = torch.chunk(split_subject, chunks=4, dim=1)\n",
    "print(\"âœ‚ï¸ Chunk into 4 pieces (uneven division - 6 columns Ã· 4 chunks):\")\n",
    "print(f\"Number of chunks: {len(uneven_chunks)}\")\n",
    "for i, chunk in enumerate(uneven_chunks):\n",
    "    print(f\"Chunk {i+1} shape {chunk.shape}: {chunk.shape[1]} columns\")\n",
    "print(\"Notice: First 2 chunks get 2 columns each, last 2 chunks get 1 column each!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a26fe47",
   "metadata": {},
   "source": [
    "### The Dimension Destroyer: `torch.unbind()`\n",
    "\n",
    "`torch.unbind()` is the most dramatic! It **removes an entire dimension** by splitting the tensor along it. Each slice becomes a separate tensor with one fewer dimension.\n",
    "\n",
    "This is incredibly useful for:\n",
    "- Processing each image in a batch separately\n",
    "- Accessing individual time steps in sequence data\n",
    "- Converting RGB channels into separate grayscale images\n",
    "\n",
    "Think of it as the opposite of `torch.stack()`!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d390833e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ’¥ Unbind along dimension 0 (separate each row):\n",
      "Number of tensors: 4\n",
      "Original shape: torch.Size([4, 6]) â†’ Individual tensor shape: torch.Size([6])\n",
      "Row 1: tensor([1, 2, 3, 4, 5, 6])\n",
      "Row 2: tensor([ 7,  8,  9, 10, 11, 12])\n",
      "Row 3: tensor([13, 14, 15, 16, 17, 18])\n",
      "Row 4: tensor([19, 20, 21, 22, 23, 24])\n",
      "\n",
      "==================================================\n",
      "\n",
      "ðŸ’¥ Unbind along dimension 1 (separate each column):\n",
      "Number of tensors: 6\n",
      "Original shape: torch.Size([4, 6]) â†’ Individual tensor shape: torch.Size([4])\n",
      "Column 1: tensor([ 1,  7, 13, 19])\n",
      "Column 2: tensor([ 2,  8, 14, 20])\n",
      "Column 3: tensor([ 3,  9, 15, 21])\n",
      "Column 4: tensor([ 4, 10, 16, 22])\n",
      "Column 5: tensor([ 5, 11, 17, 23])\n",
      "Column 6: tensor([ 6, 12, 18, 24])\n",
      "\n",
      "ðŸ§  Key Insight: unbind() reduces dimensions! 2D â†’ 1D tensors\n",
      "   It's like taking apart the 3D book we built earlier!\n"
     ]
    }
   ],
   "source": [
    "# Unbind along dimension 0 - separate each row into individual tensors\n",
    "unbound_rows = torch.unbind(split_subject, dim=0)\n",
    "print(\"ðŸ’¥ Unbind along dimension 0 (separate each row):\")\n",
    "print(f\"Number of tensors: {len(unbound_rows)}\")\n",
    "print(f\"Original shape: {split_subject.shape} â†’ Individual tensor shape: {unbound_rows[0].shape}\")\n",
    "for i, row_tensor in enumerate(unbound_rows):\n",
    "    print(f\"Row {i+1}: {row_tensor}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*50 + \"\\n\")\n",
    "\n",
    "# Unbind along dimension 1 - separate each column into individual tensors  \n",
    "unbound_cols = torch.unbind(split_subject, dim=1)\n",
    "print(\"ðŸ’¥ Unbind along dimension 1 (separate each column):\")\n",
    "print(f\"Number of tensors: {len(unbound_cols)}\")\n",
    "print(f\"Original shape: {split_subject.shape} â†’ Individual tensor shape: {unbound_cols[0].shape}\")\n",
    "for i, col_tensor in enumerate(unbound_cols):\n",
    "    print(f\"Column {i+1}: {col_tensor}\")\n",
    "\n",
    "print(f\"\\nðŸ§  Key Insight: unbind() reduces dimensions! 2D â†’ 1D tensors\")\n",
    "print(f\"   It's like taking apart the 3D book we built earlier!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9957e8e3",
   "metadata": {},
   "source": [
    "### Your Mission: The Division Master's Gauntlet\n",
    "\n",
    "The scalpel is in your hands! Prove your surgical precision with these division challenges:\n",
    "\n",
    "1. **The Even Split**: Create a tensor of shape `(6, 8)` filled with sequential numbers 1-48. Split it into pieces of size 2 along dimension 0.\n",
    "\n",
    "2. **The Perfect Division**: Take the same tensor and chunk it into exactly 3 equal pieces along dimension 1.\n",
    "\n",
    "3. **The Dimension Destroyer**: Create a 3D tensor of shape `(4, 3, 2)` filled with ones. Use `unbind()` to separate it along dimension 0, then print the shape of one of the resulting tensors.\n",
    "\n",
    "4. **The Reconstruction Test**: Split a tensor into pieces, then use `torch.cat()` to put it back together. Create a `(2, 9)` tensor, split it into 3 pieces along dimension 1, then concatenate them back. Verify the result matches the original!\n",
    "\n",
    "5. **Real-World RGB**: Create a \"fake RGB image\" tensor of shape `(3, 4, 4)` representing 3 color channels. Use `unbind()` to separate the R, G, and B channels into individual 2D tensors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9c564e35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- 1. The Even Split ---\n",
      "Original tensor shape: torch.Size([6, 8])\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "split() got an unexpected keyword argument 'split_size'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[23]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m large_tensor = torch.arange(\u001b[32m1\u001b[39m, \u001b[32m49\u001b[39m).reshape(\u001b[32m6\u001b[39m, \u001b[32m8\u001b[39m)  \u001b[38;5;66;03m# 6x8 tensor with numbers 1-48\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mOriginal tensor shape: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlarge_tensor.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m split_pieces = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlarge_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mSplit into \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(split_pieces)\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m pieces of size 2:\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, piece \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(split_pieces):\n",
      "\u001b[31mTypeError\u001b[39m: split() got an unexpected keyword argument 'split_size'"
     ]
    }
   ],
   "source": [
    "# Your code for the Division Master's Gauntlet goes here!\n",
    "\n",
    "print(\"--- 1. The Even Split ---\")\n",
    "large_tensor = torch.arange(1, 49).reshape(6, 8)  # 6x8 tensor with numbers 1-48\n",
    "print(f\"Original tensor shape: {large_tensor.shape}\")\n",
    "split_pieces = torch.split(large_tensor, split_size=2, dim=0)\n",
    "print(f\"Split into {len(split_pieces)} pieces of size 2:\")\n",
    "for i, piece in enumerate(split_pieces):\n",
    "    print(f\"  Piece {i+1}: shape {piece.shape}\")\n",
    "\n",
    "print(\"\\n--- 2. The Perfect Division ---\")\n",
    "chunk_pieces = torch.chunk(large_tensor, chunks=3, dim=1)\n",
    "print(f\"Chunked into exactly {len(chunk_pieces)} pieces:\")\n",
    "for i, chunk in enumerate(chunk_pieces):\n",
    "    print(f\"  Chunk {i+1}: shape {chunk.shape}\")\n",
    "\n",
    "print(\"\\n--- 3. The Dimension Destroyer ---\")\n",
    "tensor_3d = torch.ones(4, 3, 2)\n",
    "print(f\"Original 3D tensor shape: {tensor_3d.shape}\")\n",
    "unbound_3d = torch.unbind(tensor_3d, dim=0)\n",
    "print(f\"After unbinding: {len(unbound_3d)} tensors, each shape: {unbound_3d[0].shape}\")\n",
    "print(\"Dimension destroyed! 3D â†’ 2D\")\n",
    "\n",
    "print(\"\\n--- 4. The Reconstruction Test ---\")\n",
    "original = torch.arange(1, 19).reshape(2, 9)\n",
    "print(f\"Original: {original}\")\n",
    "split_parts = torch.split(original, split_size=3, dim=1)\n",
    "reconstructed = torch.cat(split_parts, dim=1)\n",
    "print(f\"Reconstructed: {reconstructed}\")\n",
    "print(f\"Perfect match: {torch.equal(original, reconstructed)}\")\n",
    "\n",
    "print(\"\\n--- 5. Real-World RGB ---\")\n",
    "rgb_image = torch.randn(3, 4, 4)  # 3 channels, 4x4 image\n",
    "print(f\"RGB image shape: {rgb_image.shape}\")\n",
    "red, green, blue = torch.unbind(rgb_image, dim=0)\n",
    "print(f\"Red channel shape: {red.shape}\")\n",
    "print(f\"Green channel shape: {green.shape}\")  \n",
    "print(f\"Blue channel shape: {blue.shape}\")\n",
    "print(\"Ready for individual channel processing!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71879b23",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Summary: The Surgeon's Knowledge Is Complete!\n",
    "\n",
    "Magnificent work, my surgical apprentice! You have mastered the fundamental operations of tensor surgery and assembly. Let us review your newly acquired powers:\n",
    "\n",
    "### **ðŸ”ª The Art of Selection** \n",
    "- **Indexing & Slicing**: Extract elements, rows, columns, or sub-regions with surgical precision\n",
    "- **Boolean Masking**: Select elements that meet specific criteria\n",
    "- **Key Tools**: `tensor[index]`, `tensor[start:end]`, `tensor[mask]`\n",
    "\n",
    "### **ðŸ§¬ Forbidden Fusions**\n",
    "- **Concatenation**: Join tensors along existing dimensions with `torch.cat()`\n",
    "- **Stacking**: Create new dimensions and organize tensors with `torch.stack()`\n",
    "- **Key Rule**: Understanding when to extend vs. when to create new dimensions\n",
    "\n",
    "### **âœ‚ï¸ The Great Division**\n",
    "- **Precise Splitting**: Cut into specific-sized pieces with `torch.split()`\n",
    "- **Equal Division**: Divide into a set number of chunks with `torch.chunk()`\n",
    "- **Dimension Destruction**: Remove dimensions entirely with `torch.unbind()`\n",
    "\n",
    "You now possess the core skills to **dissect any tensor**, **assemble complex structures**, and **divide them back into manageable pieces**. These are the fundamental surgical techniques you'll use in every neural network adventure ahead!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4a031f",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Professor Torchenstein's Outro\n",
    "\n",
    "Spectacular! You've wielded the surgical tools with the precision of a master! Your tensors have been sliced, fused, and divided according to your will. But tell me, my gifted apprenticeâ€”do you feel that tingling sensation in your neural pathways? That's the hunger for more power!\n",
    "\n",
    "Your tensors may now be perfectly **assembled**, but they are still... rigid. Static in their dimensions. What if I told you that we could transform their very **essence** without changing their soul? What if we could make a 1D tensor **become** a 2D matrix, or a 3D cube **collapse** into a flat surface?\n",
    "\n",
    "In our next lesson, **Tensor Metamorphosis: Shape-Shifting Mastery**, we shall unlock the secrets of transformation itself! We will reshape reality, squeeze dimensions out of existence, and permute the cosmic order of our data!\n",
    "\n",
    "Until then, practice your surgical techniques. The metamorphosis chamber awaits! Mwahahahahaha!\n",
    "\n",
    "<video controls width=\"100%\" style=\"margin: 20px 0;\">\n",
    "    <source src=\"/assets/images/torchenstein_claps_proud.mp4\" type=\"video/mp4\">\n",
    "    Your browser does not support the video tag.\n",
    "</video>\n",
    "\n",
    "[Proceed to the Metamorphosis Chamber!](02b_tensor_metamorphosis.ipynb){ .md-button .md-button--primary }\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-course-YUNTNAZF-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
