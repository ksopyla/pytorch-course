{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fa1af409",
   "metadata": {},
   "source": [
    "# Tensor Metamorphosis: Shape-Shifting Mastery\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cb19fad",
   "metadata": {},
   "source": [
    "**Module 1 | Lesson 2b**\n",
    "\n",
    "---\n",
    "\n",
    "## Professor Torchenstein's Grand Directive\n",
    "\n",
    "Ah, my brilliant apprentice! Do you feel it? That electric tingle of mastery coursing through your neural pathways? You have learned to **slice** tensors with surgical precision and **fuse** them into magnificent constructions! But now... NOW we transcend mere cutting and pasting!\n",
    "\n",
    "Today, we unlock the ultimate power: **METAMORPHOSIS**! We shall transform the very **essence** of tensor structure without disturbing a single precious datum within! Think of it as the most elegant magic—changing form while preserving the soul!\n",
    "\n",
    "**\"Behold! We shall `reshape()` reality itself and make dimensions `unsqueeze()` from the void! The tensors... they will obey our geometric commands!\"**\n",
    "\n",
    "![Torchenstein holding motherboard](/assets/images/torchenstein_holding_motherboard.png)\n",
    "\n",
    "---\n",
    "\n",
    "### Your Mission Briefing\n",
    "\n",
    "By the time you emerge from this metamorphosis chamber, you will command the arcane arts of:\n",
    "\n",
    "*   **🔄 The Great Reshape & View Metamorphosis:** Transform tensor structures with `torch.reshape()` and `torch.view()` while understanding memory layout secrets.\n",
    "*   **🗜️ The Squeeze & Unsqueeze Dimension Dance:** Add and remove dimensions of size 1 with surgical precision using `squeeze()` and `unsqueeze()`.\n",
    "*   **🚀 The Expand & Repeat Replication Magic:** Efficiently expand data with `torch.expand()` or fully replicate it with `torch.repeat()`.\n",
    "*   **📊 Specialized Shape Sorcery:** Flatten complex structures into submission with `torch.flatten()` and restore them with `torch.unflatten()`.\n",
    "\n",
    "**Estimated Time to Completion:** 20 minutes of pure shape-shifting enlightenment.\n",
    "\n",
    "**What You'll Need:**\n",
    "*   The wisdom from our previous experiments: [tensor summoning](01_introduction_to_tensors.ipynb) and [tensor surgery](02a_tensor_manipulation.ipynb).\n",
    "*   A willingness to bend reality to your computational will!\n",
    "*   Your PyTorch laboratory, humming with metamorphic potential.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1b33231",
   "metadata": {},
   "source": [
    "## Part 1: Memory Layout Foundations 🧱\n",
    "\n",
    "### The Deep Theory Behind Memory Layout Magic\n",
    "\n",
    "Ah, my curious apprentice! To truly master tensor metamorphosis, you must understand the **fundamental secret** that lies beneath: **how tensors live in your computer's memory**! This knowledge will separate you from the mere code-monkeys and elevate you to the ranks of true PyTorch sorcerers!\n",
    "\n",
    "***The Universal Truth: Everything is a 1D Array! 📏***\n",
    "It is just a long, sequential line of storage locations:\n",
    "```\n",
    "Computer Memory (Always 1D):\n",
    "[addr_0][addr_1][addr_2][addr_3][addr_4][addr_5][addr_6][addr_7]...\n",
    "```\n",
    "\n",
    "**The Multi-Dimensional Illusion:**\n",
    "When we have a \"2D tensor\" or \"3D tensor,\" it's really just our **interpretation** of how to read this 1D memory! The computer doesn't care about rows and columns—that's just how WE choose to organize and access the data.\n",
    "\n",
    "### Row-Major vs Column-Major: The Ancient Battle! ⚔️\n",
    "\n",
    "There are two ways to store multi-dimensional data in this 1D memory:\n",
    "\n",
    "**🇨 Row-Major (C-style) - PyTorch's Choice:**\n",
    "Store data row by row, left to right, then move to the next row.\n",
    "\n",
    "**🇫 Column-Major (Fortran-style):**  \n",
    "Store data column by column, top to bottom, then move to the next column.\n",
    "\n",
    "Let's visualize this with a 3×4 matrix containing numbers 1-12:\n",
    "\n",
    "```\n",
    "Visual Matrix:\n",
    "[ 1  2  3  4]\n",
    "[ 5  6  7  8]  \n",
    "[ 9 10 11 12]\n",
    "\n",
    "Row-Major Memory Layout (PyTorch default):\n",
    "Memory: [1][2][3][4][5][6][7][8][9][10][11][12]\n",
    "        └─  row1  ─┘└─  row2  ─┘└─   row3   ──┘\n",
    "\n",
    "Column-Major Memory Layout (Not PyTorch):\n",
    "Memory: [1][5][9][2][6][10][3][7][11][4][8][12]\n",
    "        └ col1  ┘└ col2   ┘└─ col3 ─┘└─ col4 ─┘\n",
    "```\n",
    "\n",
    "**PyTorch uses Row-Major** because it's the standard for C/C++ and most modern systems! This is **not dependent on your OS or hardware**—it's a software design choice.\n",
    "\n",
    "### What Makes Memory \"Contiguous\"? 🧩\n",
    "\n",
    "**Contiguous Memory access:** You try to read the tensor's elements in the **expected sequential order** in the 1D memory array.\n",
    "\n",
    "**Non-Contiguous Memory access:** You try to get the tensor's elements which are scattered—they exist in memory but not in the order you'd expect when reading row by row.\n",
    "\n",
    "### The Transpose Tragedy - Why Memory Becomes Non-Contiguous\n",
    "\n",
    "Let's witness the moment when contiguous memory becomes scattered:\n",
    "\n",
    "```\n",
    "Original 3×4 Tensor (Contiguous):\n",
    "Visual:           Memory Layout:\n",
    "[ 1  2  3  4]     [1][2][3][4][5][6][7][8][9][10][11][12]\n",
    "[ 5  6  7  8]  →  \n",
    "[ 9 10 11 12]    \n",
    "\n",
    "After Transpose to 4×3 (Non-Contiguous):\n",
    "Visual:          Expected Memory for New Shape:\n",
    "[ 1  5  9]       [1][5][9][2][6][10][3][7][11][4][8][12]\n",
    "[ 2  6 10]  \n",
    "[ 3  7 11]       But ACTUAL memory is still:\n",
    "[ 4  8 12]       [1][2][3][4][5][6][7][8][9][10][11][12]\n",
    "```\n",
    "\n",
    "**The Problem:** To read row 1 of the transposed tensor `[1, 5, 9]`, PyTorch must jump around in memory: address 0 → address 4 → address 8. This \"jumping around\" makes it non-contiguous!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "394507a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔬 MEMORY LAYOUT IN ACTION - ROW-MAJOR DEMONSTRATION\n",
      "=================================================================\n",
      "🧠 Raw Data in Computer Memory (1D Reality):\n",
      "   Memory: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "   Shape: torch.Size([12]) ← This is how it ACTUALLY lives!\n",
      "\n",
      "📐 ROW-MAJOR INTERPRETATION AS 3×4 MATRIX:\n",
      "   Same memory: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "   But interpreted as 3×4:\n",
      "tensor([[ 1,  2,  3,  4],\n",
      "        [ 5,  6,  7,  8],\n",
      "        [ 9, 10, 11, 12]])\n",
      "   💡 Row 1: [1,2,3,4] from memory positions 0-3\n",
      "   💡 Row 2: [5,6,7,8] from memory positions 4-7\n",
      "   💡 Row 3: [9,10,11,12] from memory positions 8-11\n",
      "\n",
      "🔄 DIFFERENT INTERPRETATION: 4×3 MATRIX:\n",
      "   Same memory: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "   But interpreted as 4×3:\n",
      "tensor([[ 1,  2,  3],\n",
      "        [ 4,  5,  6],\n",
      "        [ 7,  8,  9],\n",
      "        [10, 11, 12]])\n",
      "   💡 Row 1: [1,2,3], Row 2: [4,5,6], Row 3: [7,8,9], Row 4: [10,11,12]\n",
      "\n",
      "✨ THE FUNDAMENTAL INSIGHT:\n",
      "   - Memory never changes: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
      "   - Only our INTERPRETATION changes!\n",
      "   - This is the foundation of tensor metamorphosis!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Set the seed for cosmic consistency\n",
    "torch.manual_seed(42)\n",
    "\n",
    "print(\"🔬 MEMORY LAYOUT IN ACTION - ROW-MAJOR DEMONSTRATION\")\n",
    "print(\"=\" * 65)\n",
    "\n",
    "# Create our test subject: numbers 1-12 in sequential memory\n",
    "data = torch.arange(1, 13)  \n",
    "print(\"🧠 Raw Data in Computer Memory (1D Reality):\")\n",
    "print(f\"   Memory: {data.tolist()}\")\n",
    "print(f\"   Shape: {data.shape} ← This is how it ACTUALLY lives!\")\n",
    "\n",
    "print(f\"\\n📐 ROW-MAJOR INTERPRETATION AS 3×4 MATRIX:\")\n",
    "matrix_3x4 = data.reshape(3, 4)\n",
    "print(f\"   Same memory: {data.tolist()}\")\n",
    "print(f\"   But interpreted as 3×4:\")\n",
    "print(matrix_3x4)\n",
    "print(f\"   💡 Row 1: [1,2,3,4] from memory positions 0-3\")\n",
    "print(f\"   💡 Row 2: [5,6,7,8] from memory positions 4-7\")\n",
    "print(f\"   💡 Row 3: [9,10,11,12] from memory positions 8-11\")\n",
    "\n",
    "print(f\"\\n🔄 DIFFERENT INTERPRETATION: 4×3 MATRIX:\")\n",
    "matrix_4x3 = data.reshape(4, 3)  \n",
    "print(f\"   Same memory: {data.tolist()}\")\n",
    "print(f\"   But interpreted as 4×3:\")\n",
    "print(matrix_4x3)\n",
    "print(f\"   💡 Row 1: [1,2,3], Row 2: [4,5,6], Row 3: [7,8,9], Row 4: [10,11,12]\")\n",
    "\n",
    "print(f\"\\n✨ THE FUNDAMENTAL INSIGHT:\")\n",
    "print(f\"   - Memory never changes: {data.tolist()}\")\n",
    "print(f\"   - Only our INTERPRETATION changes!\")\n",
    "print(f\"   - This is the foundation of tensor metamorphosis!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dad10b8",
   "metadata": {},
   "source": [
    "\n",
    "🧟‍♂️ **Remember, dear tensor alchemist!** ✨\n",
    "\n",
    "> \n",
    "> Shape and form are but illusions!  \n",
    "> The memory remains unchanged—it's only our interpretation that morphs! \n",
    "> 🪄🔬\n",
    " _– Prof. Torchenstein_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "852da0c9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "##  PyTorch's Memory Management System 🏭\n",
    "\n",
    "Now that you understand how memory is fundamentally organized, prepare to witness PyTorch's DIABOLICAL system for managing that memory! This is where the magic happens, my apprentice—where PyTorch transforms from a simple library into a memory manipulation GENIUS!\n",
    "\n",
    "### 🧠 **The Trinity of Tensor Existence - Storage, Data Pointers, and Views**\n",
    "\n",
    "PyTorch has crafted an elegant three-tier system to encapsulate how tensor data lives, breathes, and transforms in memory. Understanding this trinity will separate you from the memory-blind masses forever!\n",
    "\n",
    "**🎭 `tensor`** - **The Mask of Interpretation**\n",
    "- **What it REALLY is:** Your personal window into the memory abyss! A tensor is merely an interpretation layer that can represent the entire memory buffer, a clever view of it, or just a slice of the underlying numerical reality.\n",
    "- **The Secret:** Multiple tensors can wear different masks while peering into the SAME underlying memory vault!\n",
    "\n",
    "**📦 `tensor.storage()`** - **The Memory Vault Master**\n",
    "- **What it is:** PyTorch's high-level Storage object—the supreme overlord that commands the actual data buffer in the memory depths!\n",
    "- **When shared:** Multiple tensor minions can pledge allegiance to the same Storage master, but each can gaze upon different regions of its domain (like examining different rows of the same data matrix)\n",
    "- **Think of it as:** The entire **memory palace** that hoards all your numerical treasures, while individual tensors are merely **different keys** to access various chambers within!\n",
    "\n",
    "**🎯 `tensor.data_ptr()`** - **The Exact Memory Coordinates** \n",
    "- **What it is:** The raw memory address (a cold, hard integer) that points to the EXACT byte where this particular tensor's data journey begins in the vast memory ocean!\n",
    "- **When different:** When tensors are views gazing upon different territories of the same memory kingdom (like viewing different slices of the same storage empire)\n",
    "- **Think of it as:** The precise **GPS coordinates** within the memory warehouse—while `.storage()` tells you which warehouse, `.data_ptr()` tells you the exact shelf, row, and position!\n",
    "\n",
    "**⚡ The Torchenstein Memory Hierarchy:**\n",
    "```\n",
    "🏰 Computer Memory (The Kingdom)\n",
    "  └── 📦 Storage Object (The Memory Palace)  \n",
    "      ├── 🎯 data_ptr() #1 (Throne Room) ← tensor_a points here\n",
    "      ├── 🎯 data_ptr() #2 (Armory) ← tensor_b[10:] points here  \n",
    "      └── 🎯 data_ptr() #3 (Treasury) ← tensor_c.view(...) points here\n",
    "```\n",
    "\n",
    "**💡 The Memory Sharing Conspiracy Matrix:**\n",
    "\n",
    "| Scenario | Same Storage? | Same data_ptr? | What's Really Happening | Example |\n",
    "|----------|---------------|----------------|------------------------|---------|\n",
    "| **True Copy** | ❌ No | ❌ No | Complete independence—separate kingdoms! | `tensor.clone()` |\n",
    "| **Shape Change** | ✅ Yes | ✅ Yes | Same palace, same throne room, different interpretation | `tensor.reshape(3,4)` |\n",
    "| **Slice View** | ✅ Yes | ❌ No | Same palace, different room within it | `tensor[2:]` |\n",
    "\n",
    "*The ultimate truth: PyTorch's genius lies in maximizing memory sharing while maintaining the illusion of independence! Mwahahaha!*\n",
    "\n",
    "Let's witness this diabolical PyTorch memory system in action and see the conspiracy unfold!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "486d5999",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏭 PYTORCH'S MEMORY MANAGEMENT IN ACTION\n",
      "=======================================================\n",
      "Original tensor: tensor([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12])\n",
      "\n",
      "📐 SCENARIO 1: Shape Change (reshape)\n",
      "   Reshaped: \n",
      "tensor([[ 1,  2,  3,  4],\n",
      "        [ 5,  6,  7,  8],\n",
      "        [ 9, 10, 11, 12]])\n",
      "   📦 Same storage? True \n",
      "\toriginal.storage().data_ptr()=2814071805632 \n",
      "\treshaped.storage().data_ptr()=2814071805632\n",
      "   🎯 Same data_ptr? True\n",
      "\toriginal.data_ptr()=2814071805632 \n",
      "\treshaped.data_ptr()=2814071805632\n",
      "\n",
      "✂️ SCENARIO 2: Slice View\n",
      "   Sliced tensor: tensor([ 5,  6,  7,  8,  9, 10, 11, 12])\n",
      "   📦 Same storage? True\n",
      "\toriginal.storage().data_ptr()=2814071805632 \n",
      "\tsliced.storage().data_ptr()=2814071805632\n",
      "   🎯 Same data_ptr? False\n",
      "\toriginal.data_ptr()=2814071805632 \n",
      "\tsliced.data_ptr()=2814071805664\n",
      "   🧮 Memory offset: 32 bytes = 4 elements\n",
      "\n",
      "📋 SCENARIO 3: True Copy (clone)\n",
      "   Cloned tensor: tensor([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12])\n",
      "   📦 Same storage? False\n",
      "   🎯 Same data_ptr? False\n",
      "\n",
      "💡 PYTORCH'S MEMORY EFFICIENCY:\n",
      "   - Reshape: FREE! (same memory, different interpretation)\n",
      "   - Slice: EFFICIENT! (same memory, different starting point)\n",
      "   - Clone: EXPENSIVE! (new memory allocation)\n"
     ]
    }
   ],
   "source": [
    "print(\"🏭 PYTORCH'S MEMORY MANAGEMENT IN ACTION\")\n",
    "print(\"=\" * 55)\n",
    "\n",
    "# Create original tensor  \n",
    "original = torch.arange(1, 13)\n",
    "print(f\"Original tensor: {original}\")\n",
    "\n",
    "# Scenario 1: Shape change (should share storage AND data_ptr)\n",
    "reshaped = original.reshape(3, 4)\n",
    "print(f\"\\n📐 SCENARIO 1: Shape Change (reshape)\")\n",
    "print(f\"   Reshaped: \\n{reshaped}\")\n",
    "\n",
    "print(f\"   📦 Same storage? {original.storage().data_ptr()==reshaped.storage().data_ptr()} \")\n",
    "print(f\"\\toriginal.storage().data_ptr()={original.storage().data_ptr()} \\n\\treshaped.storage().data_ptr()={reshaped.storage().data_ptr()}\")\n",
    "print(f\"   🎯 Same data_ptr? {original.data_ptr() == reshaped.data_ptr()}\")\n",
    "print(f\"\\toriginal.data_ptr()={original.data_ptr()} \\n\\treshaped.data_ptr()={reshaped.data_ptr()}\")\n",
    "\n",
    "# Scenario 2: Slice view (should share storage but DIFFERENT data_ptr)\n",
    "sliced = original[4:]  # Elements from index 4 onwards\n",
    "print(f\"\\n✂️ SCENARIO 2: Slice View\")\n",
    "print(f\"   Sliced tensor: {sliced}\")\n",
    "print(f\"   📦 Same storage? {original.storage().data_ptr() == sliced.storage().data_ptr()}\")\n",
    "print(f\"\\toriginal.storage().data_ptr()={original.storage().data_ptr()} \\n\\tsliced.storage().data_ptr()={sliced.storage().data_ptr()}\")\n",
    "print(f\"   🎯 Same data_ptr? {original.data_ptr() == sliced.data_ptr()}\")\n",
    "print(f\"\\toriginal.data_ptr()={original.data_ptr()} \\n\\tsliced.data_ptr()={sliced.data_ptr()}\")\n",
    "\n",
    "# Calculate the offset for sliced tensor\n",
    "element_size = original.element_size()\n",
    "offset = sliced.data_ptr() - original.data_ptr()\n",
    "print(f\"   🧮 Memory offset: {offset} bytes = {offset // element_size} elements\")\n",
    "\n",
    "# Scenario 3: True copy (different storage AND data_ptr)\n",
    "copied = original.clone()\n",
    "print(f\"\\n📋 SCENARIO 3: True Copy (clone)\")\n",
    "print(f\"   Cloned tensor: {copied}\")\n",
    "print(f\"   📦 Same storage? {original.storage().data_ptr() == copied.storage().data_ptr()}\")\n",
    "print(f\"   🎯 Same data_ptr? {original.data_ptr() == copied.data_ptr()}\")\n",
    "\n",
    "print(f\"\\n💡 PYTORCH'S MEMORY EFFICIENCY:\")\n",
    "print(f\"   - Reshape: FREE! (same memory, different interpretation)\")\n",
    "print(f\"   - Slice: EFFICIENT! (same memory, different starting point)\")  \n",
    "print(f\"   - Clone: EXPENSIVE! (new memory allocation)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69f5b8a5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## torch.view() - The Memory-Efficient Shape Changer 👁️\n",
    "\n",
    "Now that we understand WHY we need shape transformation, let's master the first tool: `torch.view()`!\n",
    "\n",
    "### 🎯 **What is torch.view() and What is it FOR?**\n",
    "\n",
    "**`torch.view()`** is PyTorch's **memory-efficient** shape transformation method. It creates a new tensor with a different shape that **shares the same underlying data** as the original tensor.\n",
    "\n",
    "**🚀 Use `view()` when:**\n",
    "- You want **maximum performance** (no data copying)\n",
    "- You know your tensor has **contiguous memory** layout  \n",
    "- You need **guaranteed memory sharing** (changes to one tensor affect the other)\n",
    "\n",
    "**⚠️ Limitations:**\n",
    "- **Requires contiguous memory** - fails if memory is scattered\n",
    "- **Throws error** rather than automatically fixing problems\n",
    "- **Purist approach** - no fallback mechanisms\n",
    "\n",
    "### 📐 **How view() Works: The Shape Mathematics**\n",
    "\n",
    "The **Golden Rule:** Total elements must remain constant!\n",
    "\n",
    "```\n",
    "Original shape: (A, B, C, D)  → Total elements: A × B × C × D\n",
    "New shape:      (W, X, Y, Z)  → Total elements: W × X × Y × Z\n",
    "\n",
    "Valid only if: A × B × C × D = W × X × Y × Z\n",
    "```\n",
    "\n",
    "**🔢 The Magic `-1` Parameter:**\n",
    "Use `-1` in one dimension to let PyTorch calculate it automatically:\n",
    "```python\n",
    "tensor.view(batch_size, -1)  # PyTorch figures out the second dimension\n",
    "```\n",
    "\n",
    "Let's see `view()` in action with real examples!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3d7f61a",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"👁️ TORCH.VIEW() MASTERCLASS\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Create a contiguous tensor for our experiments  \n",
    "data = torch.arange(24)  # 24 elements: 0, 1, 2, ..., 23\n",
    "print(f\"Original data: {data}\")\n",
    "print(f\"Shape: {data.shape}, Elements: {data.numel()}\")\n",
    "\n",
    "print(f\"\\n✅ SUCCESS SCENARIOS - view() works perfectly:\")\n",
    "\n",
    "# Scenario 1: 1D to 2D\n",
    "matrix_4x6 = data.view(4, 6)\n",
    "print(f\"   1D→2D: {data.shape} → {matrix_4x6.shape}\")\n",
    "print(f\"   Calculation: 24 elements = 4×6? {4*6 == 24} ✓\")\n",
    "\n",
    "# Scenario 2: Using -1 for automatic calculation\n",
    "auto_matrix = data.view(3, -1)  # PyTorch calculates: 24/3 = 8\n",
    "print(f\"   Auto-calc: {data.shape} → {auto_matrix.shape}\")\n",
    "print(f\"   PyTorch figured out: 24/3 = 8\")\n",
    "\n",
    "# Scenario 3: 1D to 3D (more complex)\n",
    "cube_2x3x4 = data.view(2, 3, 4)\n",
    "print(f\"   1D→3D: {data.shape} → {cube_2x3x4.shape}\")\n",
    "print(f\"   Calculation: 24 elements = 2×3×4? {2*3*4 == 24} ✓\")\n",
    "\n",
    "# Scenario 4: Memory sharing verification\n",
    "print(f\"\\n🔗 MEMORY SHARING TEST:\")\n",
    "print(f\"   Original data_ptr: {data.data_ptr()}\")\n",
    "print(f\"   Matrix data_ptr:   {matrix_4x6.data_ptr()}\")  \n",
    "print(f\"   Same memory? {data.data_ptr() == matrix_4x6.data_ptr()} ✓\")\n",
    "\n",
    "# Modify original - should affect the view!\n",
    "data[0] = 999\n",
    "print(f\"   Changed data[0] to 999...\")\n",
    "print(f\"   Matrix[0,0] is now: {matrix_4x6[0,0]} (shares memory!)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ce424dff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "❌ FAILURE SCENARIOS - view() throws errors:\n",
      "   ❌ Impossible shape (5×5=25≠24): shape '[5, 5]' is invalid for input of size 24...\n",
      "   Non-contiguous tensor: False\n",
      "   ❌ Non-contiguous memory: view size is not compatible with input tensor's size and stride (at least one dimension spans across two contiguous subspaces). Use .reshape(...) instead....\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n❌ FAILURE SCENARIOS - view() throws errors:\")\n",
    "\n",
    "# Reset data\n",
    "data = torch.arange(24) \n",
    "\n",
    "# Error 1: Impossible shape (wrong total elements)\n",
    "try:\n",
    "    impossible = data.view(5, 5)  # 5×5=25, but we have 24 elements\n",
    "    print(\"   Impossible shape: Success?!\")\n",
    "except RuntimeError as e:\n",
    "    print(f\"   ❌ Impossible shape (5×5=25≠24): {str(e)[:50]}...\")\n",
    "\n",
    "# Error 2: Non-contiguous memory (after transpose)\n",
    "matrix = data.view(4, 6)\n",
    "transposed = matrix.t()  # Creates non-contiguous memory\n",
    "print(f\"   Non-contiguous tensor: {transposed.is_contiguous()}\")\n",
    "try:\n",
    "    flattened = transposed.view(-1)\n",
    "    print(\"   view() on non-contiguous: Success?!\")\n",
    "except RuntimeError as e:\n",
    "    print(f\"   ❌ Non-contiguous memory: {str(e)}...\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b078823",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## torch.reshape() - The Diplomatic Shape Changer 🤝\n",
    "\n",
    "Now let's master `torch.reshape()` - the more forgiving, intelligent cousin of `view()`!\n",
    "\n",
    "### 🎯 What is torch.reshape() and What is it FOR?\n",
    "\n",
    "**`torch.reshape()`** is PyTorch's **diplomatic** shape transformation method. It tries to return a view when possible, but creates a copy when necessary to ensure the operation always succeeds.\n",
    "\n",
    "**🤝 Use `reshape()` when:**\n",
    "- You want **reliability over maximum performance**\n",
    "- You're not sure if your tensor memory is contiguous\n",
    "- You want PyTorch to **handle memory layout automatically**\n",
    "- You're prototyping and want to avoid memory errors\n",
    "\n",
    "**✅ Advantages:**\n",
    "- **Always succeeds** (if the shape math is valid)\n",
    "- **Automatically handles** contiguous vs non-contiguous memory\n",
    "- **Beginner-friendly** - less likely to cause frustrating errors\n",
    "- **Smart fallback** - returns view when possible, copy when necessary\n",
    "\n",
    "**⚠️ Trade-offs:**\n",
    "- **Less predictable performance** - you don't know if it creates a copy\n",
    "- **Potentially slower** than `view()` in some cases\n",
    "- **Less explicit** about memory sharing\n",
    "\n",
    "### 📊 reshape() vs view() - When to Use Which?\n",
    "\n",
    "| Scenario | Use `view()` | Use `reshape()` |\n",
    "|----------|-------------|-----------------|\n",
    "| **Performance critical** | ✅ Guaranteed no copying | ❌ Might copy data |\n",
    "| **Beginner-friendly** | ❌ Can throw errors | ✅ Always works |\n",
    "| **Prototyping** | ❌ Interrupts workflow | ✅ Smooth development |\n",
    "| **Production code** | ✅ Predictable behavior | ⚠️ Less predictable |\n",
    "| **Memory sharing required** | ✅ Guaranteed sharing | ⚠️ Depends on layout |\n",
    "\n",
    "Let's see how `reshape()` handles the scenarios where `view()` fails!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "892382e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🤝 TORCH.RESHAPE() - THE DIPLOMATIC SOLUTION\n",
      "====================================================\n",
      "Original data: torch.Size([24]) → [0, 1, 2, 3, 4, 5]... (24 elements)\n",
      "\n",
      "✅ SCENARIO 1: Contiguous tensor (reshape returns view)\n",
      "   Original data_ptr: 5037313032192\n",
      "   Reshaped data_ptr: 5037313032192\n",
      "   Same memory (view)? True ✓\n",
      "\n",
      "⚠️ SCENARIO 2: Non-contiguous tensor (reshape creates copy)\n",
      "   Transposed contiguous? False\n",
      "   Transposed data_ptr: 5037313032192\n",
      "   Reshaped data_ptr:   5037313032384\n",
      "   Same memory? False\n",
      "   Conclusion: reshape() created a COPY to make it work ✓\n",
      "\n",
      "🆚 DIRECT COMPARISON: view() vs reshape()\n",
      "   Testing on the same non-contiguous tensor...\n",
      "   view(): FAILED ❌ - view size is not compatible with input t...\n",
      "   reshape(): SUCCESS ✅ - Shape: torch.Size([24])\n",
      "\n",
      "🔍 INVESTIGATING: When does reshape() return view vs copy?\n",
      "   Contiguous reshape → View: True\n",
      "   Non-contiguous reshape → View: False (Creates copy)\n",
      "\n",
      "💡 RESHAPE() WISDOM:\n",
      "   1. Always succeeds (if math is valid)\n",
      "   2. Returns view when memory layout allows\n",
      "   3. Creates copy when necessary\n",
      "   4. Perfect for beginners and prototyping\n",
      "   5. Use view() only when you need guaranteed performance\n"
     ]
    }
   ],
   "source": [
    "print(\"🤝 TORCH.RESHAPE() - THE DIPLOMATIC SOLUTION\")\n",
    "print(\"=\" * 52)\n",
    "\n",
    "# Create test data\n",
    "data = torch.arange(24)\n",
    "print(f\"Original data: {data.shape} → {data[:6].tolist()}... (24 elements)\")\n",
    "\n",
    "print(f\"\\n✅ SCENARIO 1: Contiguous tensor (reshape returns view)\")\n",
    "matrix_4x6 = data.reshape(4, 6)\n",
    "print(f\"   Original data_ptr: {data.data_ptr()}\")\n",
    "print(f\"   Reshaped data_ptr: {matrix_4x6.data_ptr()}\")\n",
    "print(f\"   Same memory (view)? {data.data_ptr() == matrix_4x6.data_ptr()} ✓\")\n",
    "\n",
    "print(f\"\\n⚠️ SCENARIO 2: Non-contiguous tensor (reshape creates copy)\")\n",
    "# First transpose to make it non-contiguous\n",
    "transposed = matrix_4x6.t()  # Now 6x4, non-contiguous\n",
    "print(f\"   Transposed contiguous? {transposed.is_contiguous()}\")\n",
    "\n",
    "# Now reshape the non-contiguous tensor\n",
    "flattened = transposed.reshape(-1)  # This works! (unlike view)\n",
    "print(f\"   Transposed data_ptr: {transposed.data_ptr()}\")\n",
    "print(f\"   Reshaped data_ptr:   {flattened.data_ptr()}\")\n",
    "print(f\"   Same memory? {transposed.data_ptr() == flattened.data_ptr()}\")\n",
    "print(f\"   Conclusion: reshape() created a COPY to make it work ✓\")\n",
    "\n",
    "print(f\"\\n🆚 DIRECT COMPARISON: view() vs reshape()\")\n",
    "print(\"   Testing on the same non-contiguous tensor...\")\n",
    "\n",
    "# Test view() - should FAIL\n",
    "try:\n",
    "    view_result = transposed.view(-1)\n",
    "    print(\"   view(): SUCCESS (unexpected!)\")\n",
    "except RuntimeError as e:\n",
    "    print(f\"   view(): FAILED ❌ - {str(e)[:40]}...\")\n",
    "\n",
    "# Test reshape() - should SUCCEED  \n",
    "try:\n",
    "    reshape_result = transposed.reshape(-1)\n",
    "    print(f\"   reshape(): SUCCESS ✅ - Shape: {reshape_result.shape}\")\n",
    "except RuntimeError as e:\n",
    "    print(f\"   reshape(): FAILED - {e}\")\n",
    "\n",
    "print(f\"\\n🔍 INVESTIGATING: When does reshape() return view vs copy?\")\n",
    "\n",
    "# Case 1: Simple reshape of contiguous tensor\n",
    "simple_data = torch.arange(12)\n",
    "reshaped_simple = simple_data.reshape(3, 4)\n",
    "shares_memory_1 = simple_data.data_ptr() == reshaped_simple.data_ptr()\n",
    "print(f\"   Contiguous reshape → View: {shares_memory_1}\")\n",
    "\n",
    "# Case 2: Reshape after making non-contiguous\n",
    "non_contig = reshaped_simple.t()  # Non-contiguous\n",
    "reshaped_non_contig = non_contig.reshape(-1)\n",
    "shares_memory_2 = non_contig.data_ptr() == reshaped_non_contig.data_ptr()\n",
    "print(f\"   Non-contiguous reshape → View: {shares_memory_2} (Creates copy)\")\n",
    "\n",
    "print(f\"\\n💡 RESHAPE() WISDOM:\")\n",
    "print(f\"   1. Always succeeds (if math is valid)\")\n",
    "print(f\"   2. Returns view when memory layout allows\")\n",
    "print(f\"   3. Creates copy when necessary\")\n",
    "print(f\"   4. Perfect for beginners and prototyping\")\n",
    "print(f\"   5. Use view() only when you need guaranteed performance\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee7484c1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 🧩 The Element Flow Mystery: How Tensors Rearrange Themselves\n",
    "\n",
    "**THE CRUCIAL QUESTION:** When you transform a tensor from one shape to another, exactly HOW do the elements flow into their new positions? This is where many apprentices stumble—they understand the math (`6×8 = 48 = 2×3×8`) but don't visualize the **element migration patterns**!\n",
    "\n",
    "Fear not! Professor Torchenstein shall illuminate this dark mystery with surgical precision! Understanding element flow is THE difference between tensor confusion and tensor mastery!\n",
    "\n",
    "### 🔍 **The Row-Major Flow Principle**\n",
    "\n",
    "Remember our fundamental truth: **PyTorch always reads and writes elements in row-major order**—left to right, then top to bottom, like reading English text!\n",
    "\n",
    "**The Sacred Rule:** Elements always flow in this order: `[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11...]`\n",
    "\n",
    "No matter what shape transformation you perform, elements maintain their **reading order** but get **reinterpreted** into new dimensional coordinates.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "addd2d05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧩 ELEMENT FLOW MASTERCLASS - THE MIGRATION PATTERNS\n",
      "=================================================================\n",
      "📊 STARTING POINT: 6×4 Matrix (24 elements)\n",
      "   Row-major memory order: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23]\n",
      "   Visual layout:\n",
      "tensor([[ 0,  1,  2,  3],\n",
      "        [ 4,  5,  6,  7],\n",
      "        [ 8,  9, 10, 11],\n",
      "        [12, 13, 14, 15],\n",
      "        [16, 17, 18, 19],\n",
      "        [20, 21, 22, 23]])\n",
      "\n",
      "🎯 TRANSFORMATION 1: 2D → 3D (6×4 → 2×3×4)\n",
      "   Question: How do elements flow into the new 3D structure?\n",
      "   Result shape: torch.Size([2, 3, 4])\n",
      "   Element flow visualization:\n",
      "   📦 Batch 0 (elements 0-11):\n",
      "tensor([[ 0,  1,  2,  3],\n",
      "        [ 4,  5,  6,  7],\n",
      "        [ 8,  9, 10, 11]])\n",
      "   📦 Batch 1 (elements 12-23):\n",
      "tensor([[12, 13, 14, 15],\n",
      "        [16, 17, 18, 19],\n",
      "        [20, 21, 22, 23]])\n",
      "   💡 Pattern: First 12 elements → Batch 0, Next 12 elements → Batch 1\n",
      "\n",
      "🔄 TRANSFORMATION 2: 2D → 3D (6×4 → 3×2×4)\n",
      "   Same 24 elements, different 3D arrangement!\n",
      "   Result shape: torch.Size([3, 2, 4])\n",
      "   Element flow visualization:\n",
      "   📦 Batch 0 (elements 0-7):\n",
      "      tensor([[0, 1, 2, 3],\n",
      "        [4, 5, 6, 7]])\n",
      "   📦 Batch 1 (elements 8-15):\n",
      "      tensor([[ 8,  9, 10, 11],\n",
      "        [12, 13, 14, 15]])\n",
      "   📦 Batch 2 (elements 16-23):\n",
      "      tensor([[16, 17, 18, 19],\n",
      "        [20, 21, 22, 23]])\n",
      "   💡 Pattern: Every 8 elements form a new batch!\n",
      "\n",
      "🎲 TRANSFORMATION 3: 2D → 3D (6×4 → 4×3×2)\n",
      "   Yet another way to slice the same 24 elements!\n",
      "   Result shape: torch.Size([4, 3, 2])\n",
      "   Element flow visualization:\n",
      "   📦 Batch 0 (elements 0-5):\n",
      "      tensor([[0, 1],\n",
      "        [2, 3],\n",
      "        [4, 5]])\n",
      "   📦 Batch 1 (elements 6-11):\n",
      "      tensor([[ 6,  7],\n",
      "        [ 8,  9],\n",
      "        [10, 11]])\n",
      "   📦 Batch 2 (elements 12-17):\n",
      "      tensor([[12, 13],\n",
      "        [14, 15],\n",
      "        [16, 17]])\n",
      "   📦 Batch 3 (elements 18-23):\n",
      "      tensor([[18, 19],\n",
      "        [20, 21],\n",
      "        [22, 23]])\n",
      "   💡 Pattern: Every 6 elements form a new batch!\n",
      "\n",
      "🧠 THE ELEMENT FLOW ALGORITHM:\n",
      "   1. Elements are read in row-major order: 0,1,2,3,4,5...\n",
      "   2. They fill the NEW shape dimensions from right to left:\n",
      "      - Last dimension fills first: [0,1,2,3] if last dim = 4\n",
      "      - Then second-to-last: next group of 4 elements\n",
      "      - Then third-to-last: next group of groups\n",
      "   3. The memory order NEVER changes, only the interpretation!\n"
     ]
    }
   ],
   "source": [
    "print(\"🧩 ELEMENT FLOW MASTERCLASS - THE MIGRATION PATTERNS\")\n",
    "print(\"=\" * 65)\n",
    "\n",
    "# Create our test subject: 2D matrix with clearly identifiable elements\n",
    "data_2d = torch.arange(24).view(6, 4)  # 6 rows × 4 columns\n",
    "print(\"📊 STARTING POINT: 6×4 Matrix (24 elements)\")\n",
    "print(f\"   Row-major memory order: {data_2d.flatten().tolist()}\")\n",
    "print(f\"   Visual layout:\\n{data_2d}\")\n",
    "\n",
    "print(f\"\\n🎯 TRANSFORMATION 1: 2D → 3D (6×4 → 2×3×4)\")\n",
    "print(\"   Question: How do elements flow into the new 3D structure?\")\n",
    "transform_3d_v1 = data_2d.view(2, 3, 4)\n",
    "print(f\"   Result shape: {transform_3d_v1.shape}\")\n",
    "print(f\"   Element flow visualization:\")\n",
    "print(f\"   📦 Batch 0 (elements 0-11):\")\n",
    "print(transform_3d_v1[0])\n",
    "print(f\"   📦 Batch 1 (elements 12-23):\")\n",
    "print(transform_3d_v1[1])\n",
    "print(f\"   💡 Pattern: First 12 elements → Batch 0, Next 12 elements → Batch 1\")\n",
    "\n",
    "print(f\"\\n🔄 TRANSFORMATION 2: 2D → 3D (6×4 → 3×2×4)\")\n",
    "print(\"   Same 24 elements, different 3D arrangement!\")\n",
    "transform_3d_v2 = data_2d.view(3, 2, 4)\n",
    "print(f\"   Result shape: {transform_3d_v2.shape}\")\n",
    "print(f\"   Element flow visualization:\")\n",
    "for i in range(3):\n",
    "    print(f\"   📦 Batch {i} (elements {i*8}-{i*8+7}):\")\n",
    "    print(f\"      {transform_3d_v2[i]}\")\n",
    "print(f\"   💡 Pattern: Every 8 elements form a new batch!\")\n",
    "\n",
    "print(f\"\\n🎲 TRANSFORMATION 3: 2D → 3D (6×4 → 4×3×2)\")\n",
    "print(\"   Yet another way to slice the same 24 elements!\")\n",
    "transform_3d_v3 = data_2d.view(4, 3, 2)\n",
    "print(f\"   Result shape: {transform_3d_v3.shape}\")\n",
    "print(f\"   Element flow visualization:\")\n",
    "for i in range(4):\n",
    "    print(f\"   📦 Batch {i} (elements {i*6}-{i*6+5}):\")\n",
    "    print(f\"      {transform_3d_v3[i]}\")\n",
    "print(f\"   💡 Pattern: Every 6 elements form a new batch!\")\n",
    "\n",
    "print(f\"\\n🧠 THE ELEMENT FLOW ALGORITHM:\")\n",
    "print(f\"   1. Elements are read in row-major order: 0,1,2,3,4,5...\")\n",
    "print(f\"   2. They fill the NEW shape dimensions from right to left:\")\n",
    "print(f\"      - Last dimension fills first: [0,1,2,3] if last dim = 4\")\n",
    "print(f\"      - Then second-to-last: next group of 4 elements\")\n",
    "print(f\"      - Then third-to-last: next group of groups\")\n",
    "print(f\"   3. The memory order NEVER changes, only the interpretation!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed314335",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 🚀 **Real-World Neural Network Shape Challenges**\n",
    "\n",
    "Now that you understand how elements flow, let's tackle the **exact scenarios** where neural network engineers use `view()` and `reshape()` every single day! These are the problems that can ONLY be solved with shape transformations (not permutation or other operations)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "207cc838",
   "metadata": {},
   "source": [
    "### **💡 Challenge 1: CNN Feature Maps → Linear Layer**\n",
    "**The Problem:** You've extracted features from images using CNN layers, but now you need to feed them into a fully connected (Linear) layer for classification. The shapes are incompatible!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "962f9dd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 SOLVING REAL-WORLD NEURAL NETWORK SHAPE CHALLENGES\n",
      "=================================================================\n",
      "💡 CHALLENGE 1: CNN Feature Maps → Linear Layer\n",
      "-------------------------------------------------------\n",
      "📊 CNN output shape: torch.Size([16, 128, 7, 7])\n",
      "   Interpretation: 16 images, 128 feature maps, 7×7 spatial size\n",
      "\n",
      "🎯 Linear layer expects: (batch_size, 6272)\n",
      "❌ But we have: torch.Size([16, 128, 7, 7])\n",
      "\n",
      "✅ SOLUTION: view(16, -1)\n",
      "   Result shape: torch.Size([16, 6272])\n",
      "   Calculation: 128 × 7 × 7 = 6272\n",
      "   Matches? True\n"
     ]
    }
   ],
   "source": [
    "print(\"🚀 SOLVING REAL-WORLD NEURAL NETWORK SHAPE CHALLENGES\")\n",
    "print(\"=\" * 65)\n",
    "\n",
    "# =============================================================================\n",
    "# CHALLENGE 1: CNN Feature Maps → Linear Layer\n",
    "# =============================================================================\n",
    "print(\"💡 CHALLENGE 1: CNN Feature Maps → Linear Layer\")\n",
    "print(\"-\" * 55)\n",
    "\n",
    "# The scenario: You've processed a batch of images through CNN layers\n",
    "batch_size = 16\n",
    "channels = 128  # Feature maps from CNN\n",
    "height, width = 7, 7  # Spatial dimensions after convolutions\n",
    "\n",
    "# This is what you get after CNN feature extraction\n",
    "cnn_features = torch.randn(batch_size, channels, height, width)\n",
    "print(f\"📊 CNN output shape: {cnn_features.shape}\")\n",
    "print(f\"   Interpretation: {batch_size} images, {channels} feature maps, {height}×{width} spatial size\")\n",
    "\n",
    "# The problem: Linear layer expects (batch_size, input_features) -> (batch_size, output)\n",
    "print(f\"\\n🎯 Linear layer expects: (batch_size, {channels*height*width})\")\n",
    "print(f\"❌ But we have: {cnn_features.shape}\")\n",
    "\n",
    "# THE SOLUTION: Flatten spatial dimensions while keeping batch dimension\n",
    "flattened_features = cnn_features.view(batch_size, -1)\n",
    "print(f\"\\n✅ SOLUTION: view({batch_size}, -1)\")\n",
    "print(f\"   Result shape: {flattened_features.shape}\")\n",
    "\n",
    "# Verify the calculation\n",
    "expected_features = channels * height * width\n",
    "print(f\"   Calculation: {channels} × {height} × {width} = {expected_features}\")\n",
    "print(f\"   Matches? {flattened_features.shape[1] == expected_features}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ec80ed",
   "metadata": {},
   "source": [
    "\n",
    "### **⚡ Challenge 2: Multi-Head Attention Setup**  \n",
    "**The Problem:** You have embeddings for a batch of text sequences, but you need to split the embedding dimension into multiple attention heads for parallel processing.\n",
    "\n",
    "Let's solve these with code and see exactly how the transformations work:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4507dcc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🚀 MULTI-HEAD ATTENTION TRANSFORMATION - 3D → 4D\n",
      "============================================================\n",
      "🧠 TRANSFORMER EMBEDDINGS: Shape torch.Size([2, 4, 8])\n",
      "   [batch_size, sequence_length, hidden_size] = [2, 4, 8]\n",
      "   This represents 2 sequences, each with 4 tokens, each token has 8 features\n",
      "\n",
      "   Embeddings tensor:\n",
      "   Batch 0:\n",
      "      Token 0: [0, 1, 2, 3, 4, 5, 6, 7] (features for this token)\n",
      "      Token 1: [8, 9, 10, 11, 12, 13, 14, 15] (features for this token)\n",
      "      Token 2: [16, 17, 18, 19, 20, 21, 22, 23] (features for this token)\n",
      "      Token 3: [24, 25, 26, 27, 28, 29, 30, 31] (features for this token)\n",
      "   Batch 1:\n",
      "      Token 0: [32, 33, 34, 35, 36, 37, 38, 39] (features for this token)\n",
      "      Token 1: [40, 41, 42, 43, 44, 45, 46, 47] (features for this token)\n",
      "      Token 2: [48, 49, 50, 51, 52, 53, 54, 55] (features for this token)\n",
      "      Token 3: [56, 57, 58, 59, 60, 61, 62, 63] (features for this token)\n",
      "\n",
      "⚡ MULTI-HEAD TRANSFORMATION:\n",
      "   Original: [2, 4, 8] → New: [2, 4, 2, 4]\n",
      "   Translation: [batch, tokens, features] → [batch, tokens, heads, features_per_head]\n",
      "\n",
      "🔍 ELEMENT FLOW ANALYSIS:\n",
      "   Where do the original 8 features go for each token?\n",
      "\n",
      "   Batch 0, Token 0 - Original features: [0, 1, 2, 3, 4, 5, 6, 7]\n",
      "      Head 0: [0, 1, 2, 3] (original features [0:4])\n",
      "      Head 1: [4, 5, 6, 7] (original features [4:8])\n",
      "\n",
      "   Batch 0, Token 1 - Original features: [8, 9, 10, 11, 12, 13, 14, 15]\n",
      "      Head 0: [8, 9, 10, 11] (original features [0:4])\n",
      "      Head 1: [12, 13, 14, 15] (original features [4:8])\n",
      "\n",
      "   Batch 0, Token 2 - Original features: [16, 17, 18, 19, 20, 21, 22, 23]\n",
      "      Head 0: [16, 17, 18, 19] (original features [0:4])\n",
      "      Head 1: [20, 21, 22, 23] (original features [4:8])\n",
      "\n",
      "   Batch 0, Token 3 - Original features: [24, 25, 26, 27, 28, 29, 30, 31]\n",
      "      Head 0: [24, 25, 26, 27] (original features [0:4])\n",
      "      Head 1: [28, 29, 30, 31] (original features [4:8])\n",
      "\n",
      "   Batch 1, Token 0 - Original features: [32, 33, 34, 35, 36, 37, 38, 39]\n",
      "      Head 0: [32, 33, 34, 35] (original features [0:4])\n",
      "      Head 1: [36, 37, 38, 39] (original features [4:8])\n",
      "\n",
      "   Batch 1, Token 1 - Original features: [40, 41, 42, 43, 44, 45, 46, 47]\n",
      "      Head 0: [40, 41, 42, 43] (original features [0:4])\n",
      "      Head 1: [44, 45, 46, 47] (original features [4:8])\n",
      "\n",
      "   Batch 1, Token 2 - Original features: [48, 49, 50, 51, 52, 53, 54, 55]\n",
      "      Head 0: [48, 49, 50, 51] (original features [0:4])\n",
      "      Head 1: [52, 53, 54, 55] (original features [4:8])\n",
      "\n",
      "   Batch 1, Token 3 - Original features: [56, 57, 58, 59, 60, 61, 62, 63]\n",
      "      Head 0: [56, 57, 58, 59] (original features [0:4])\n",
      "      Head 1: [60, 61, 62, 63] (original features [4:8])\n",
      "\n",
      "💡 THE ATTENTION HEAD PATTERN:\n",
      "   • Each token's 8 features get split into 2 groups of 4\n",
      "   • Head 0 gets features [0:4], Head 1 gets features [4:8]\n",
      "   • This allows each attention head to focus on different aspects!\n",
      "   • The element order is preserved: [0,1,2,3,4,5,6,7] → Head0:[0,1,2,3], Head1:[4,5,6,7]\n",
      "\n",
      "🎯 WHY THIS TRANSFORMATION IS GENIUS:\n",
      "   • Same memory, but now we can process 2 attention heads in parallel\n",
      "   • Each head learns different patterns (grammar, semantics, etc.)\n",
      "   • This is the SECRET behind Transformer's incredible power!\n",
      "   • GPT, BERT, ChatGPT - they ALL use this exact transformation!\n"
     ]
    }
   ],
   "source": [
    "print(\"🚀 MULTI-HEAD ATTENTION TRANSFORMATION - 3D → 4D\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Simulate the exact scenario from real Transformers!\n",
    "batch_size, seq_len, hidden_size = 2, 4, 8  # Small example for clarity\n",
    "num_heads = 2\n",
    "head_dim = hidden_size // num_heads  # 8 // 2 = 4\n",
    "\n",
    "# Create embeddings tensor like in a real Transformer\n",
    "embeddings_3d = torch.arange(batch_size * seq_len * hidden_size).view(batch_size, seq_len, hidden_size)\n",
    "print(f\"🧠 TRANSFORMER EMBEDDINGS: Shape {embeddings_3d.shape}\")\n",
    "print(f\"   [batch_size, sequence_length, hidden_size] = [{batch_size}, {seq_len}, {hidden_size}]\")\n",
    "print(f\"   This represents {batch_size} sequences, each with {seq_len} tokens, each token has {hidden_size} features\")\n",
    "print(\"\\n   Embeddings tensor:\")\n",
    "for b in range(batch_size):\n",
    "    print(f\"   Batch {b}:\")\n",
    "    for s in range(seq_len):\n",
    "        print(f\"      Token {s}: {embeddings_3d[b, s].tolist()} (features for this token)\")\n",
    "\n",
    "# THE TRANSFORMATION: Split hidden_size into multiple attention heads\n",
    "multi_head_4d = embeddings_3d.view(batch_size, seq_len, num_heads, head_dim)\n",
    "print(f\"\\n⚡ MULTI-HEAD TRANSFORMATION:\")\n",
    "print(f\"   Original: [{batch_size}, {seq_len}, {hidden_size}] → New: [{batch_size}, {seq_len}, {num_heads}, {head_dim}]\")\n",
    "print(f\"   Translation: [batch, tokens, features] → [batch, tokens, heads, features_per_head]\")\n",
    "\n",
    "print(f\"\\n🔍 ELEMENT FLOW ANALYSIS:\")\n",
    "print(f\"   Where do the original 8 features go for each token?\")\n",
    "for b in range(batch_size):\n",
    "    for s in range(seq_len):\n",
    "        original_features = embeddings_3d[b, s]\n",
    "        print(f\"\\n   Batch {b}, Token {s} - Original features: {original_features.tolist()}\")\n",
    "        for h in range(num_heads):\n",
    "            head_features = multi_head_4d[b, s, h]\n",
    "            start_idx = h * head_dim\n",
    "            end_idx = start_idx + head_dim\n",
    "            print(f\"      Head {h}: {head_features.tolist()} (original features [{start_idx}:{end_idx}])\")\n",
    "\n",
    "print(f\"\\n💡 THE ATTENTION HEAD PATTERN:\")\n",
    "print(f\"   • Each token's {hidden_size} features get split into {num_heads} groups of {head_dim}\")\n",
    "print(f\"   • Head 0 gets features [0:{head_dim}], Head 1 gets features [{head_dim}:{hidden_size}]\")  \n",
    "print(f\"   • This allows each attention head to focus on different aspects!\")\n",
    "print(f\"   • The element order is preserved: [0,1,2,3,4,5,6,7] → Head0:[0,1,2,3], Head1:[4,5,6,7]\")\n",
    "\n",
    "print(f\"\\n🎯 WHY THIS TRANSFORMATION IS GENIUS:\")\n",
    "print(f\"   • Same memory, but now we can process {num_heads} attention heads in parallel\")\n",
    "print(f\"   • Each head learns different patterns (grammar, semantics, etc.)\")\n",
    "print(f\"   • This is the SECRET behind Transformer's incredible power!\")\n",
    "print(f\"   • GPT, BERT, ChatGPT - they ALL use this exact transformation!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e038e5e4",
   "metadata": {},
   "source": [
    "### 🎯 **Key Takeaways: When to Use view()/reshape() in Neural Networks**\n",
    "\n",
    "**✅ Perfect for view()/reshape():**\n",
    "- **CNN → Linear**: Flattening spatial dimensions `(B, C, H, W)` → `(B, C×H×W)`\n",
    "- **Multi-head attention**: Splitting features `(B, S, E)` → `(B, S, H, E/H)`  \n",
    "- **Batch reshaping**: Organizing data `(N×F)` → `(B, N/B, F)`\n",
    "- **Any scenario** where total elements stay the same and no dimension reordering is needed\n",
    "\n",
    "**❌ NOT suitable for view()/reshape():**\n",
    "- **Dimension reordering**: `(H, W, C)` → `(C, H, W)` (use `permute()` or `transpose()`)\n",
    "- **Broadcasting preparation**: Adding singleton dimensions (use `unsqueeze()`)\n",
    "- **Changing data layout**: Converting between different memory formats\n",
    "\n",
    "**🧠 Remember the Golden Rules:**\n",
    "1. **Total elements must match**: `original.numel() == reshaped.numel()`\n",
    "2. **Element flow follows row-major order**: Last dimension fills first\n",
    "3. **Memory is shared**: Changes to original affect all views\n",
    "4. **Use `-1` for automatic calculation**: Let PyTorch figure out one dimension\n",
    "\n",
    "You now possess the complete knowledge of tensor shape transformation! These patterns appear in every modern neural network architecture. 🚀\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-course-YUNTNAZF-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
