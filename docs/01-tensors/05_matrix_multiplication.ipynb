{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a77c1394",
   "metadata": {},
   "source": [
    "# Matrix Multiplication: Unleashing the Power of Tensors! âš¡\n",
    "\n",
    "> \"Behold! The sacred art of matrix multiplication - where dimensions dance and vectors bend to my will!\" â€” **Professor Victor py Torchenstein**\n",
    "\n",
    "## The Attention Formula (Preview of Things to Come)\n",
    "\n",
    "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n",
    "\n",
    "Where:\n",
    "- $Q$ is the Query matrix\n",
    "- $K$ is the Key matrix  \n",
    "- $V$ is the Value matrix\n",
    "- $d_k$ is the dimension of the key vectors\n",
    "- $\\text{softmax}$ normalizes the attention weights\n",
    "\n",
    "## Basic Matrix Operations\n",
    "\n",
    "Let's start with the fundamentals before we conquer attention mechanisms! \n",
    "\n",
    "Element-wise multiplication: \n",
    "\n",
    "$C_{ij} = A_{ij} \\times B_{ij}$\n",
    "\n",
    "Matrix multiplication: $C_{ij} = \\sum_{k} A_{ik} \\times B_{kj}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dd8f52a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrix A shape: torch.Size([3, 4])\n",
      "Matrix B shape: torch.Size([4, 2])\n",
      "Result C shape: torch.Size([3, 2])\n",
      "\n",
      "Mwahahaha! The matrices have been multiplied!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Create some matrices for experimentation\n",
    "A = torch.randn(3, 4)\n",
    "B = torch.randn(4, 2)\n",
    "\n",
    "print(\"Matrix A shape:\", A.shape)\n",
    "print(\"Matrix B shape:\", B.shape)\n",
    "\n",
    "# Matrix multiplication\n",
    "C = torch.matmul(A, B)\n",
    "print(\"Result C shape:\", C.shape)\n",
    "print(\"\\nMwahahaha! The matrices have been multiplied!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7202ef9",
   "metadata": {},
   "source": [
    "## PyTorch Matrix Multiplication Methods\n",
    "\n",
    "Professor Torchenstein's arsenal includes multiple ways to multiply matrices:\n",
    "\n",
    "1. **`torch.matmul()`** - The general matrix multiplication function\n",
    "2. **`@` operator** - Pythonic matrix multiplication (same as matmul)\n",
    "3. **`torch.mm()`** - For 2D matrices only\n",
    "4. **`torch.bmm()`** - Batch matrix multiplication\n",
    "\n",
    "### Mathematical Foundations\n",
    "\n",
    "For matrices $A \\in \\mathbb{R}^{m \\times n}$ and $B \\in \\mathbb{R}^{n \\times p}$:\n",
    "\n",
    "$$C = AB \\quad \\text{where} \\quad C_{ij} = \\sum_{k=1}^{n} A_{ik} B_{kj}$$\n",
    "\n",
    "This operation is fundamental to:\n",
    "- Linear transformations\n",
    "- Neural network forward passes  \n",
    "- Attention mechanisms in Transformers\n",
    "- And much more! ðŸ§ âš¡"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e2c39cf",
   "metadata": {},
   "source": [
    "## Pytorch doesn't support int matrix multiplication"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66fc376f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU Name: NVIDIA GeForce RTX 3080 Laptop GPU\n",
      "\n",
      "--- GPU Timings ---\n",
      "Float32 matmul on GPU took: 0.107393 seconds\n",
      "Float16 matmul on GPU took: 0.234943 seconds\n",
      "BFloat16 matmul on GPU took: 0.116011 seconds\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "\"addmm_cuda\" not implemented for 'Int'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 84\u001b[39m\n\u001b[32m     82\u001b[39m \u001b[38;5;66;03m# INT32 on GPU\u001b[39;00m\n\u001b[32m     83\u001b[39m start.record()\n\u001b[32m---> \u001b[39m\u001b[32m84\u001b[39m int32_output_gpu = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mint32_quantized_matrix_gpu\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mint32_quantized_matrix_gpu\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     85\u001b[39m end.record()\n\u001b[32m     86\u001b[39m torch.cuda.synchronize()\n",
      "\u001b[31mRuntimeError\u001b[39m: \"addmm_cuda\" not implemented for 'Int'"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch\n",
    "\n",
    "# 1. Our \"Model\" and \"Input\"\n",
    "fp32_matrix = torch.randn(2**10, 2**10)\n",
    "fp16_matrix = fp32_matrix.to(torch.float16)\n",
    "bf16_matrix = fp32_matrix.to(torch.bfloat16)\n",
    "\n",
    "\n",
    "# 2. The Symmetric Quantization Spell (zero_point = 0)\n",
    "def quantize_symmetric(tensor, dtype=torch.int8):\n",
    "    ''' \n",
    "    This function quantizes a tensor to desired integer dtype.\n",
    "    It quantizes the  tesnsor based on the absolute max value in the tensor.\n",
    "    '''\n",
    "\n",
    "    assert dtype in [torch.int8, torch.int16, torch.int32]\n",
    "\n",
    "    int_max = torch.iinfo(dtype).max\n",
    "    int_min = torch.iinfo(dtype).min\n",
    "    # Find the scale: map the absolute max of the tensor to the quantization range\n",
    "    scale = tensor.abs().max() / int_max\n",
    "    # Quantize\n",
    "    quantized_tensor = (tensor / scale).round().clamp(int_min, int_max).to(dtype)\n",
    "    return quantized_tensor, scale\n",
    "\n",
    "# 3. Quantize input and weights INDEPENDENTLY\n",
    "int8_quantized_matrix, scale_matrix = quantize_symmetric(fp32_matrix, dtype=torch.int8)\n",
    "int16_quantized_matrix, scale_matrix = quantize_symmetric(fp32_matrix, dtype=torch.int16)\n",
    "int32_quantized_matrix, scale_matrix = quantize_symmetric(fp32_matrix, dtype=torch.int32)\n",
    "\n",
    "# --- GPU Operations ---\n",
    "if torch.cuda.is_available():\n",
    "    gpu_device = torch.device(\"cuda\")\n",
    "    print(f\"GPU Name: {torch.cuda.get_device_name(0)}\")\n",
    "else:\n",
    "    gpu_device = None\n",
    "    print(\"GPU not available.\")\n",
    "if gpu_device:\n",
    "    print(\"\\n--- GPU Timings ---\")\n",
    "\n",
    "    # Move matrices to GPU\n",
    "    fp32_matrix_gpu = fp32_matrix.to(gpu_device)\n",
    "    fp16_matrix_gpu = fp16_matrix.to(gpu_device)\n",
    "    bf16_matrix_gpu = bf16_matrix.to(gpu_device)\n",
    "    int8_quantized_matrix_gpu = int8_quantized_matrix.to(gpu_device)\n",
    "    int16_quantized_matrix_gpu = int16_quantized_matrix.to(gpu_device)\n",
    "    int32_quantized_matrix_gpu = int32_quantized_matrix.to(gpu_device)\n",
    "    \n",
    "    # Correct timing for GPU operations requires synchronization\n",
    "    \n",
    "    # FP32 on GPU\n",
    "    start = torch.cuda.Event(enable_timing=True)\n",
    "    end = torch.cuda.Event(enable_timing=True)\n",
    "    start.record()\n",
    "    fp32_output_gpu = torch.matmul(fp32_matrix_gpu, fp32_matrix_gpu)\n",
    "    end.record()\n",
    "    torch.cuda.synchronize()\n",
    "    fp32_time_gpu = start.elapsed_time(end) / 1000  # Convert ms to s\n",
    "    print(f\"Float32 matmul on GPU took: {fp32_time_gpu:.6f} seconds\")\n",
    "\n",
    "    # FP16 on GPU\n",
    "    start.record()\n",
    "    fp16_output_gpu = torch.matmul(fp16_matrix_gpu, fp16_matrix_gpu)\n",
    "    end.record()\n",
    "    torch.cuda.synchronize()\n",
    "    fp16_time_gpu = start.elapsed_time(end) / 1000\n",
    "    print(f\"Float16 matmul on GPU took: {fp16_time_gpu:.6f} seconds\")\n",
    "\n",
    "    # BF16 on GPU\n",
    "    # Note: BF16 performance depends on GPU architecture (Ampere and newer)\n",
    "    try:\n",
    "        start.record()\n",
    "        bf16_output_gpu = torch.matmul(bf16_matrix_gpu, bf16_matrix_gpu)\n",
    "        end.record()\n",
    "        torch.cuda.synchronize()\n",
    "        bf16_time_gpu = start.elapsed_time(end) / 1000\n",
    "        print(f\"BFloat16 matmul on GPU took: {bf16_time_gpu:.6f} seconds\")\n",
    "    except Exception as e:\n",
    "        print(f\"BFloat16 matmul on GPU failed: {e}\")\n",
    "\n",
    "    # INT32 on GPU\n",
    "    start.record()\n",
    "    int32_output_gpu = torch.mm(int32_quantized_matrix_gpu, int32_quantized_matrix_gpu)\n",
    "    end.record()\n",
    "    torch.cuda.synchronize()\n",
    "    int32_time_gpu = start.elapsed_time(end) / 1000\n",
    "    print(f\"Int32 matmul on GPU took: {int32_time_gpu:.6f} seconds\")\n",
    "\n",
    "    # INT16 on GPU\n",
    "    start.record()\n",
    "    int16_output_gpu = torch.matmul(int16_quantized_matrix_gpu, int16_quantized_matrix_gpu)\n",
    "    end.record()\n",
    "    torch.cuda.synchronize()\n",
    "    int16_time_gpu = start.elapsed_time(end) / 1000\n",
    "    print(f\"Int16 matmul on GPU took: {int16_time_gpu:.6f} seconds\")\n",
    "\n",
    "    # INT8 on GPU\n",
    "    start.record()\n",
    "    int8_output_gpu = torch.matmul(int8_quantized_matrix_gpu, int8_quantized_matrix_gpu)\n",
    "    end.record()\n",
    "    torch.cuda.synchronize()\n",
    "    int8_time_gpu = start.elapsed_time(end) / 1000\n",
    "    print(f\"Int8 matmul on GPU took: {int8_time_gpu:.6f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56e1e50c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9e0836",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-course-QuR2iHBk-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
