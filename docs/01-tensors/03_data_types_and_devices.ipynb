{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a39a6ae",
   "metadata": {},
   "source": [
    "# Data Types and Devices\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e505c16e",
   "metadata": {},
   "source": [
    "**Module 1 | Lesson 3**\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60e7c47f",
   "metadata": {},
   "source": [
    " **Ah, the Alchemist’s Arsenal: PyTorch Data Types!**  \n",
    " \n",
    " Behold, apprentice! Not all tensors are forged alike. The *essence* of a tensor—its `.dtype`—determines what kind of numbers it can hold, and thus, what arcane computations it can perform. Choose wisely, for the wrong `dtype` can turn your beautiful model into a bubbling cauldron of errors!\n",
    " \n",
    " **Floating-Point Types**  \n",
    " *The lifeblood of neural networks! For when you wish to summon real numbers, gradients, and the very stuff of learning itself.*\n",
    " \n",
    " - `torch.float32` or `torch.float`: 32-bit floating-point. The default elixir for most tensor incantations. Trusty, reliable, and the backbone of deep learning!\n",
    " - `torch.float64` or `torch.double`: 64-bit floating-point. For those moments when you crave *precision*—perhaps to impress your rival, Dr. Hammer, with your numerical exactitude.\n",
    " - `torch.float16` or `torch.half`: 16-bit floating-point. The potion of choice for speed demons and memory misers. Use it to accelerate your experiments (especially on modern GPUs), but beware the lurking specter of numerical instability!\n",
    " - `torch.bfloat16`: 16-bit “brain” floating-point. Like float16, but with a twist—wider range, less precision. Favored by TPUs and the latest NVIDIA cauldrons.\n",
    " \n",
    " **Integer Types**  \n",
    " *For when you need to count, index, or encode the world in whole numbers. No fractions allowed!*\n",
    " \n",
    " - `torch.int8`: 8-bit signed integer. Tiny, but sometimes mighty.\n",
    " - `torch.uint8`: 8-bit unsigned integer. The pixel pusher’s favorite—perfect for images and masks!\n",
    " - `torch.int16` or `torch.short`: 16-bit signed integer. Slightly more room for your integers to stretch their legs.\n",
    " - `torch.int32` or `torch.int`: 32-bit signed integer. The workhorse of integer types.\n",
    " - `torch.int64` or `torch.long`: 64-bit signed integer. The grandmaster—often used for indices, embedding lookups, and any time you need to count *very* high.\n",
    " \n",
    " **Boolean Type**  \n",
    " *For the binary-minded: True or False, 1 or 0, on or off. The stuff of logic gates and comparison spells!*\n",
    " \n",
    " - `torch.bool`: The result of your tensor comparisons, your maskings, your “is it alive or dead?” queries.\n",
    " \n",
    " **Complex Types**  \n",
    " *For the truly mad scientist—numbers with both real and imaginary parts! Useful for signal processing, quantum shenanigans, and impressing your colleagues at tensor parties.*\n",
    " \n",
    " - `torch.complex64`: Complex numbers with 32-bit real and 32-bit imaginary parts. \n",
    " - `torch.complex128`: Complex numbers with 64-bit real and 64-bit imaginary parts. For when your calculations must be both *precise* and *mysterious*.\n",
    " \n",
    " Choose your dtype as you would choose your wand, apprentice: with care, curiosity, and a dash of reckless ambition! Mwahahaha!\n",
    " "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
