{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "34c9034b",
   "metadata": {},
   "source": [
    "# DTypes & Devices: Choose Your Weapons\n",
    "\n",
    "**Module 1 | Lesson 3**\n",
    "\n",
    "---\n",
    "\n",
    "### Professor Torchenstein's Grand Directive\n",
    "\n",
    "Mwahahaha! You've sliced, fused, and reshaped tensors with the skill of a master surgeon! You can command their *form*, but what of their *soul*? What of their very *essence*?\n",
    "\n",
    "Today, we delve deeper! We shall master the two most fundamental properties of any tensor: its **data type (`dtype`)**, which determines its precision and power, and its **device**, the very dimension it inhabits‚Äîbe it the humble CPU or the roaring, incandescent GPU! Choose your weapons wisely, for these choices dictate the speed, precision, and ultimate success of your grand experiments!\n",
    "\n",
    "![Torchenstein holding a glowing cube](/assets/images/torchenstein_presenting_cube.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81232cf0",
   "metadata": {},
   "source": [
    "### Your Mission Briefing\n",
    "\n",
    "By the end of this electrifying session, you will have mastered the arcane arts of:\n",
    "\n",
    "*   **üî¨ Identifying** a tensor's data type (`dtype`) and the computational device it resides on.\n",
    "*   **‚ú® Transmuting** tensors between different `dtypes` to balance precision and performance.\n",
    "*   **‚ö° Teleporting** tensors between the CPU and GPU to accelerate your computations.\n",
    "*   **‚ö†Ô∏è Dodging** the catastrophic errors that arise from mismatched `dtypes` and devices.\n",
    "\n",
    "**Estimated Time to Completion:** 15 minutes of alchemical mastery.\n",
    "\n",
    "**What You'll Need:**\n",
    "*   The wisdom from our previous lessons on [tensor summoning](../01_introduction_to_tensors), [surgery](../02a_tensor_manipulation), and [metamorphosis](../02b_tensor_metamorphosis).\n",
    "*   A PyTorch environment, preferably with a GPU waiting to be awakened!\n",
    "*   A thirst for computational power!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82acd456",
   "metadata": {},
   "source": [
    "### Previously in the Lab... (A Quick Recap)\n",
    "\n",
    "In our last experiment, we mastered **Tensor Metamorphosis**, transforming tensor shapes with `reshape`, `view`, `squeeze`, and `unsqueeze`. We learned that a tensor's shape is merely an illusion‚Äîa view into a contiguous block of 1D memory.\n",
    "\n",
    "Now that you command a tensor's external form, we shall master its internal essence. The journey continues!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8694640e",
   "metadata": {},
   "source": [
    "## Part 2: The Alchemist's Arsenal - Mastering Data Types (`dtype`)\n",
    "\n",
    "Behold, apprentice! Not all tensors are forged from the same ethereal stuff. The very *essence* of a tensor‚Äîits `.dtype`‚Äîdetermines what kind of numbers it can hold, its precision in the arcane arts of mathematics, and the amount of precious memory it consumes!\n",
    "\n",
    "A wise choice of `dtype` can mean the difference between a lightning-fast model and a sluggish, memory-guzzling behemoth. Let us inspect the primary weapons in our arsenal!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da3e474a",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5959079",
   "metadata": {},
   "source": [
    "### Checking the soul of your tensor\n",
    "\n",
    "We will summon tensors of different `dtypes`, transmute them, and witness the performance implications firsthand!\n",
    "\n",
    "To perform these miracles, you must master two key tools:\n",
    "*   The `.dtype` attribute: A tensor's inherent property that reveals its data type. You can't change it directly, but you can inspect it to understand your tensor's essence.\n",
    "*   The `.to()` method: This is your transmutation spell! It's a powerful and versatile method that not only changes a tensor's `dtype` but can also teleport it to a different `device` at the same time!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25b83ffa",
   "metadata": {},
   "source": [
    "\n",
    "### Floating-Point Types (The Elixirs of Learning)\n",
    "*The very lifeblood of neural networks! These are essential for representing real numbers, calculating gradients, and enabling your models to learn.*\n",
    "\n",
    "-   `torch.float32` (`torch.float`): The 32-bit workhorse. This is the default `dtype` for a reason‚Äîit offers a fantastic balance between precision and performance. Most of your initial experiments will thrive on this reliable elixir.\n",
    "-   `torch.float64` (`torch.double`): 64-bit, for when you require the utmost, surgical precision. Its use in deep learning is rare, as it doubles memory usage and can slow down computations, but for certain scientific calculations, it is indispensable. A powerful tool, but often overkill for our purposes!\n",
    "-   `torch.float16` (`torch.half`): A 16-bit potion for speed and memory efficiency. Halving the precision can dramatically accelerate training on modern GPUs and cut your memory footprint in half! But beware‚Äîits limited range can sometimes lead to numerical instability.\n",
    "-   `torch.bfloat16`: The new favorite in the high council of AI! Also 16-bit, but with a crucial difference from `float16`. It sacrifices some precision to maintain the same dynamic range as `float32`, making it far more stable for training large models like Transformers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Floating-Point Memory Footprints ---\n",
      "torch.float64: 3.14159265 | Memory: 8 bytes\n",
      "torch.float32: 3.14159274 | Memory: 4 bytes\n",
      "torch.float16: 3.14062500 | Memory: 2 bytes (Half the size of fp32!)\n",
      "torch.bfloat16: 3.14062500 | Memory: 2 bytes (Half the size of fp32!)\n",
      "--- Floating-Point Memory Footprints ---\n",
      "torch.float64: 70000.00000000 \n",
      "torch.float32: 70000.00000000 \n",
      "torch.float16: inf \n",
      "torch.bfloat16: 70144.00000000 \n"
     ]
    }
   ],
   "source": [
    "# A single number for our comparison\n",
    "pi_number = 3.14159265\n",
    "\n",
    "# Summoning tensors of different float dtypes\n",
    "tensor_fp64 = torch.tensor(pi_number, dtype=torch.float64)\n",
    "tensor_fp32 = torch.tensor(pi_number, dtype=torch.float32)\n",
    "tensor_fp16 = torch.tensor(pi_number, dtype=torch.float16)\n",
    "tensor_bf16 = torch.tensor(pi_number, dtype=torch.bfloat16)\n",
    "\n",
    "print(\"--- Floating-Point Memory Footprints ---\")\n",
    "print(f\"{tensor_fp64.dtype}: {tensor_fp64.item():.8f} | Memory: {tensor_fp64.element_size()} bytes\")\n",
    "print(f\"{tensor_fp32.dtype}: {tensor_fp32.item():.8f} | Memory: {tensor_fp32.element_size()} bytes\")\n",
    "print(f\"{tensor_fp16.dtype}: {tensor_fp16.item():.8f} | Memory: {tensor_fp16.element_size()} bytes (Half the size of fp32!)\")\n",
    "print(f\"{tensor_bf16.dtype}: {tensor_bf16.item():.8f} | Memory: {tensor_bf16.element_size()} bytes (Half the size of fp32!)\")\n",
    "\n",
    "# A large number for our comparison\n",
    "\n",
    "large_number = 70000.0\n",
    "tensor_fp64 = torch.tensor(large_number, dtype=torch.float64)\n",
    "tensor_fp32 = torch.tensor(large_number, dtype=torch.float32)\n",
    "tensor_fp16 = torch.tensor(large_number, dtype=torch.float16)\n",
    "tensor_bf16 = torch.tensor(large_number, dtype=torch.bfloat16)\n",
    "\n",
    "print(\"--- Floating-Point Memory Footprints ---\")\n",
    "print(f\"{tensor_fp64.dtype}: {tensor_fp64.item():.8f} \")\n",
    "print(f\"{tensor_fp32.dtype}: {tensor_fp32.item():.8f} \")\n",
    "print(f\"{tensor_fp16.dtype}: {tensor_fp16.item():.8f} \")\n",
    "print(f\"{tensor_bf16.dtype}: {tensor_bf16.item():.8f} \")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad96ca6",
   "metadata": {},
   "source": [
    "\n",
    "#### The Curious Case of `bfloat16` and the Number 70,144\n",
    "\n",
    "Mwahahaha! Apprentice, you have sharp eyes! You witnessed a strange transmutation: our number `70000.0` became `70144.0` when cast to `bfloat16`. Is this a bug? A flaw in our alchemy? No! This is a profound secret about the very fabric of digital reality!\n",
    "\n",
    "To understand this, we must journey into the heart of the machine and see how it stores floating-point numbers.\n",
    "\n",
    "##### The Blueprint of a Float: Scientific Notation in Binary\n",
    "\n",
    "Every floating-point number in your computer's memory is stored like a secret formula with three parts:\n",
    "\n",
    "1.  **The Sign (S)**: A single bit (0 for positive, 1 for negative).\n",
    "2.  **The Exponent (E)**: A set of bits that represent the number's magnitude or *range*, like the `10^x` part in scientific notation.\n",
    "3.  **The Mantissa (M)**: A set of bits that represent the actual digits of the number‚Äîits *precision*.\n",
    "\n",
    "The number is roughly reconstructed as: `(-1)^S * M * 2^E`.\n",
    "\n",
    "The mantissa is the key here. It's a binary fraction that always starts with an implicit `1.`, followed by the sum of fractional powers of 2. For example: `1.M = 1 + m1/2 + m2/4 + m3/8 + ...m23/2^23`, where `m1, m2, m3, ...m23` are 0 or 1 (depends on how many bits are in the mantissa, this case 23 bits).\n",
    "\n",
    "##### The Meaning of \"Precision\"\n",
    "\n",
    "When we say `bfloat16` has \"less precision\" than `float32`, we don't mean fewer decimal places in the way humans think. We mean it has **fewer bits in its mantissa**.\n",
    "\n",
    "-   `float32` has 1 bit for sign, 23 mantissa bits and 8 bits for exponent\n",
    "-   `float16` has 1 bit for sign, 10 mantissa bits and 5 bits for exponent,  more bits for mantissa means less coarse==more precision, less range (min - max)\n",
    "-   `bfloat16` has 1 bit for sign, 7 mantissa bits and 8 bits for exponent,  less bits for mantissa means more coarse==less precision, more range (min - max) then float16, same range as float32\n",
    "\n",
    "This means `bfloat16` can only represent a much smaller, **coarser set of numbers between any two powers of two**. For small numbers (like 3.14), the representable values are very close together. But for large numbers, the \"gaps\" between representable values become huge!\n",
    "\n",
    "##### Detailed explanation: Why 70,144?\n",
    "\n",
    "The number `70000` is simply not one of the numbers that can be perfectly formed with `bfloat16`'s limited 7-bit mantissa at that large exponent range.\n",
    "\n",
    "\n",
    "Lets write the number `70,000` in binary: `1 0001 0001 0111 0000`.\n",
    "\n",
    "For `70,000`, scientific notation number starts with `1.`, we move the decimal point 16 places to the left (`2^16`).\n",
    "$$ 1. \\underbrace{0001000101110000}_{\\text{16 binary digits}} \\times 2^{16} $$\n",
    "\n",
    "The **mantissa** (the part after the `1.`) is where the precision limit strikes.\n",
    "\n",
    "A **`float32`** has **23 bits** for its mantissa. It can easily store those binary digits with room to spare. The number `70,000` is stored perfectly.\n",
    "* `0`-sign bit, `0001000101110000 0000000`-23 mantissa bits, `00010000`- exponent bits (`2^16`, omits bias for simplicity)\n",
    "\n",
    "\n",
    "A **`bfloat16`** only has **7 bits** for its mantissa.\n",
    "* `0`-sign bit,`1.` `0001000`-7 mantissa bits (first 7 digits) -> rounded up to `0001001`, `00010000`- exponent bits (`2^16`, omits bias for simplicity)\n",
    "* `1*2^16 + (0*1/2 + 0*1/4 + 0*1/8 + 1*1/16 + 0*1/32 + 0*1/64 + 1*1/128)*2^16 = 65536+ 4096 + 512=70144`\n",
    "\n",
    "`bfloat16` must take that 16-digit binary sequence and round it to fit into just 7 bits. This forces a loss of information, even for a whole number.\n",
    "\n",
    "1.  **Original Mantissa:** `0001000101110000`\n",
    "2.  **`bfloat16` capacity:** Can only store the first 7 digits: `0001000`.\n",
    "3.  **Rounding:** It checks the 8th digit (`1`) and, following rounding rules, rounds the 7-bit number up. The new mantissa becomes `0001001`.\n",
    "\n",
    "So, `bfloat16` ends up storing the number as `1.0001001 \\times 2^{16}`.\n",
    "\n",
    "\n",
    "Think of it like trying to measure `70,000` millimeters with a ruler that only has markings every `256` millimeters. You can't land on `70,000` exactly. You must choose the closest mark.\n",
    "\n",
    "The two closest \"marks\" that `bfloat16` can represent in that range are:\n",
    "-   `69,888`\n",
    "-   `70,144`\n",
    "\n",
    "Since `70,000` is closer to `70,144`, the transmutation spell **rounds** it to that value. It is not an error, but the result of sacrificing precision to maintain the vast numerical range of `float32`. This robustness is exactly why it is the preferred elixir for training colossal neural networks! You have witnessed the fundamental trade-off of modern AI hardware!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8186105",
   "metadata": {},
   "source": [
    "### Why `bfloat16` is Better for Transformers: A Tale of Range and Rebellion\n",
    "\n",
    "Mwahahaha! Now for a secret that separates the masters from the mere dabblers! Both `float16` and `bfloat16` use 16 bits, but they do so with diabolically different strategies. Understanding this is key to training modern marvels like Transformers!\n",
    "\n",
    "The world of `float32` is a stable, predictable realm. But it is slow and memory-hungry! When we attempt to accelerate our dark arts with `float16`, we encounter a terrible problem: **The Tyranny of a Tiny Range**.\n",
    "\n",
    "#### The Peril of `float16`: An Unstable Concoction\n",
    "\n",
    "`float16` dedicates more bits to its mantissa (precision), but starves its exponent (range). Its numerical world is small, spanning from roughly `6.1 x 10^-5` to `65,504`. Anything outside this narrow window becomes an `inf` (infinity) or vanishes to zero.\n",
    "\n",
    "During the chaotic process of training a massive Transformer, values can fluctuate wildly. This is where the tyranny of `float16` strikes hardest:\n",
    "\n",
    "*   **Exploding Gradients**: Imagine a scenario deep within your network where a series of large gradients are multiplied. Even with normalization, an intermediate calculation can easily exceed 65,504. For instance, the Adam optimizer tracks the variance of gradients (`v` term), which can grow very large. If this value overflows to `inf`, the weight update becomes `NaN` (Not a Number), and your entire training process collapses into a fiery numerical singularity!\n",
    "*   **Vanishing Activations**: Inside a Transformer, attention scores are passed through a Softmax function. If the input values (logits) are very large negative numbers, the resulting probabilities can become smaller than `float16`'s minimum representable value. They are rounded down to zero, and that part of your model stops learning entirely!\n",
    "\n",
    "To combat this, alchemists of old used a crude technique called **loss scaling**: manually multiplying the loss to keep gradients within `float16`'s safe range. It is a messy, unreliable hack!\n",
    "\n",
    "#### The `bfloat16` Rebellion: Sacrificing Precision for Power!\n",
    "\n",
    "The great minds at Google Brain, in their quest for ultimate power, forged a new weapon: the **Brain Floating-Point Format**, or `bfloat16`! They looked at the chaos of `float16` and made a brilliant, rebellious choice.\n",
    "\n",
    "They designed `bfloat16` to have the **same number of exponent bits as `float32`** (8 bits). This gives it the exact same colossal dynamic range, spanning from `1.18 x 10^-38` to `3.4 x 10^38`. It can represent gargantuan numbers and infinitesimally small ones without breaking a sweat.\n",
    "\n",
    "The price? It has fewer mantissa bits (7 bits) than `float16` (10 bits), giving it less precision. But here is the profound secret, backed by countless experiments in the deepest labs: **neural networks are incredibly resilient to low precision.**\n",
    "\n",
    "Why do the inaccuracies not hurt?\n",
    "*   **Stochastic Nature of Training**: We train models using stochastic gradient descent on mini-batches of data. This process is inherently noisy! The tiny inaccuracies introduced by `bfloat16`'s rounding are like a single drop of rain in a hurricane‚Äîthey are statistically insignificant compared to the noise already present in the training process.\n",
    "*   **Error Accumulation is Not Catastrophic**: As researchers from [The Hardware Lottery](https://arxiv.org/abs/2009.06489) blog and other deep learning practitioners have noted, the errors from low precision tend to average out over millions of updates. The network's learning direction isn't meaningfully altered. The gradient still points downhill, even if it's a slightly wobblier path.\n",
    "\n",
    "> \"For the volatile, chaotic world of deep learning, a vast and stable **range** is far more important than surgical **precision**.\"\n",
    ">\n",
    "> ‚Äî **_Prof. Torchenstein_**\n",
    "\n",
    "#### The Transformer's Elixir of Choice\n",
    "\n",
    "For training and fine-tuning, `bfloat16` is the undisputed champion, the elixir that fuels the titans of AI.\n",
    "\n",
    "1.  **Training Stability**: Its `float32`-like range means no more exploding gradients in optimizer states or vanishing activations in softmax. You can throw away the clumsy crutch of loss scaling.\n",
    "2.  **Memory Efficiency**: Like `float16`, it cuts your model's memory footprint in half compared to `float32`. This allows you to train larger models or use larger batch sizes, accelerating your path to discovery.\n",
    "3.  **Hardware Acceleration**: It is natively supported on the most powerful instruments in any modern laboratory: Google TPUs and NVIDIA's latest GPUs (Ampere architecture and newer, like the A100 or RTX 30/40 series).\n",
    "\n",
    "**The Rogues' Gallery: Who Uses `bfloat16`?**\n",
    "The most powerful creations of our time were forged in the fires of `bfloat16`. Giants like **Google's T5 and BERT**, **Meta's Llama 2**, the **Falcon** models, and many more rely on `bfloat16` for stable and efficient training. \n",
    "\n",
    "**The Verdict for Your Lab:**\n",
    "*   **For Training & Fine-Tuning**: `bfloat16` is your weapon of choice. It is the modern standard for a reason.\n",
    "*   **For Inference**: `float16` is often perfectly acceptable. After a model is trained, the range of values it processes is more predictable, making `float16`'s higher precision and wider hardware support a safe and efficient option.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92708d1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- A Tale of Two Half-Precisions ---\n",
      "\\n--- The Range Test (Number: 70000.0) ---\n",
      "Original (Float32):  70000.0\n",
      "BFloat16: 70144.0 (Handles the large number flawlessly!)\n",
      "Float16:  inf (Overflows to infinity! A tragic failure!)\n",
      "\\n--- The Precision Test (Number: 3.1415927410125732) ---\n",
      "Original (Float32):  3.14159274\n",
      "Float16:  3.14062500 (More precise, closer to the original!)\n",
      "BFloat16: 3.14062500 (Less precise, a worthy sacrifice for range!)\n"
     ]
    }
   ],
   "source": [
    "# --- Range vs. Precision Demonstration ---\n",
    "print(\"--- A Tale of Two Half-Precisions ---\")\n",
    "\n",
    "# A number just beyond the float16 limit to demonstrate range\n",
    "high_number = torch.tensor(70000.0, dtype=torch.float32) \n",
    "# A high-precision number to demonstrate precision\n",
    "precise_number = torch.tensor(3.14159265, dtype=torch.float32)\n",
    "\n",
    "# --- The Range Test ---\n",
    "print(f\"\\\\n--- The Range Test (Number: {high_number.item()}) ---\")\n",
    "print(f\"Original (Float32):  {high_number.item()}\")\n",
    "\n",
    "bf16_high = high_number.to(torch.bfloat16)\n",
    "print(f\"BFloat16: {bf16_high.item()} (Handles the large number flawlessly!)\")\n",
    "\n",
    "fp16_high = high_number.to(torch.float16)\n",
    "print(f\"Float16:  {fp16_high.item()} (Overflows to infinity! A tragic failure!)\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afc22bef",
   "metadata": {},
   "source": [
    "### Integer Types (The Counting Stones)\n",
    "*For when you need to count, index, or represent discrete information like image pixel values or class labels.*\n",
    "\n",
    "-   `torch.int64` (`torch.long`): The 64-bit grandmaster of integers. This is the default for indexing operations and is crucial for embedding layers, where you need to look up values from a large vocabulary.\n",
    "-   `torch.int32` (`torch.int`): A solid 32-bit integer, perfectly suitable for most counting tasks.\n",
    "-   `torch.uint8`: An 8-bit unsigned integer, representing values from 0 to 255. The undisputed king for storing image data, where each pixel in an RGB channel has a value in this exact range!\n",
    "\n",
    "\n",
    "\n",
    "Now for the counting stones‚Äîthe integer types. Their purpose is not precision, but to hold whole numbers. Observe their varying sizes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Integer Memory Footprints ---\n",
      "torch.int64: Memory: 8 bytes\n",
      "torch.int32: Memory: 4 bytes\n",
      "torch.int16: Memory: 2 bytes\n",
      "torch.int8:  Memory: 1 bytes\n",
      "torch.uint8: Memory: 1 bytes\n"
     ]
    }
   ],
   "source": [
    "# Summoning tensors of different integer dtypes\n",
    "tensor_i64 = torch.tensor(1000, dtype=torch.int64)\n",
    "tensor_i32 = torch.tensor(1000, dtype=torch.int32)\n",
    "tensor_i16 = torch.tensor(1000, dtype=torch.int16)\n",
    "tensor_i8 = torch.tensor(100, dtype=torch.int8)\n",
    "tensor_ui8 = torch.tensor(255, dtype=torch.uint8)\n",
    "\n",
    "print(\"--- Integer Memory Footprints ---\")\n",
    "print(f\"torch.int64: Memory: {tensor_i64.element_size()} bytes\")\n",
    "print(f\"torch.int32: Memory: {tensor_i32.element_size()} bytes\")\n",
    "print(f\"torch.int16: Memory: {tensor_i16.element_size()} bytes\")\n",
    "print(f\"torch.int8:  Memory: {tensor_i8.element_size()} bytes\")\n",
    "print(f\"torch.uint8: Memory: {tensor_ui8.element_size()} bytes\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d67db87b",
   "metadata": {},
   "source": [
    "### The Transmutation Spell: Witnessing the Effects of Casting\n",
    "\n",
    "Now that you understand the properties of each `dtype`, witness what happens when we perform the transmutation! Casting from a higher precision `dtype` to a lower one is a **lossy** operation. You gain speed and save memory, but at the cost of precision!\n",
    "\n",
    "Observe the fate of our high-precision number as we cast it down the alchemical ladder.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original (float64): 3.141592653589793\n",
      "Casted to float32:  3.141592741012573 (Precision lost!)\n",
      "Casted to float16:  3.140625000000000 (More precision lost!)\n",
      "Casted to bfloat16: 3.140625000000000 (Less precision, but still good!)\n",
      "\\nCasted to integer: 3 (Decimal part vanished!)\n"
     ]
    }
   ],
   "source": [
    "# Our original, high-precision tensor\n",
    "pi_fp64 = torch.tensor(3.141592653589793, dtype=torch.float64)\n",
    "print(f\"Original (float64): {pi_fp64.item():.15f}\")\n",
    "\n",
    "# Cast it down\n",
    "pi_fp32 = pi_fp64.to(torch.float32)\n",
    "print(f\"Casted to float32:  {pi_fp32.item():.15f} (Precision lost!)\")\n",
    "\n",
    "pi_fp16 = pi_fp64.to(torch.float16)\n",
    "print(f\"Casted to float16:  {pi_fp16.item():.15f} (More precision lost!)\")\n",
    "\n",
    "pi_bf16 = pi_fp64.to(torch.bfloat16)\n",
    "print(f\"Casted to bfloat16: {pi_bf16.item():.15f} (Less precision, but still good!)\")\n",
    "\n",
    "# Casting floats to integers truncates the decimal part entirely!\n",
    "integer_pi = pi_fp64.to(torch.int)\n",
    "print(f\"\\\\nCasted to integer: {integer_pi.item()} (Decimal part vanished!)\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c20f057",
   "metadata": {},
   "source": [
    "Let's witness this cosmic trade-off with a simple experiment. We will test `bfloat16`'s vast **range** against `float16`'s superior **precision**.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da2d9788",
   "metadata": {},
   "source": [
    "### Boolean Type (The Oracle)\n",
    "*Represents the fundamental truths of the universe: `True` or `False`.*\n",
    "\n",
    "-   `torch.bool`: The result of all your logical incantations (`>`, `<`, `==`). Essential for creating masks to filter and select elements from your tensors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57aa0d59",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Part 3: The Lair of Computation - Mastering Devices (`device`)\n",
    "\n",
    "### 1. A Tour of Computational Realms\n",
    "\n",
    "A tensor's `dtype` is its soul, but its `.device` is its home‚Äîthe very dimension where its calculations will be performed. Choosing the right device is the key to unlocking diabolical computational speed!\n",
    "\n",
    "-   **`cpu`**: The Central Processing Unit. The reliable, ever-present brain of your machine. It's a generalist, capable of any task, but it performs calculations sequentially. For small tensors and simple operations, it's perfectly adequate.\n",
    "-   **`cuda`**: The NVIDIA GPU! This is the roaring heart of the deep learning revolution. A GPU is a specialist, containing thousands of cores designed for one purpose: massively parallel computation. Moving your tensors and models here is **essential** for training any serious neural network.\n",
    "-   **`mps`**: Metal Performance Shaders. Apple's answer to CUDA for their new M-series chips. If you are wielding a modern Mac, this device will unleash the power of its integrated GPU.\n",
    "-   \n",
    "\n",
    "#### How PyTorch Supports So Many Devices: The Magic of ATen\n",
    "\n",
    "How can a single command like `torch.matmul()` work on a CPU, an NVIDIA GPU, and an Apple chip? The secret lies in PyTorch's core library: **ATen**.\n",
    "\n",
    "Think of ATen as a grand dispatcher in our laboratory. When you issue a command, ATen inspects the tensor's `.device` and redirects the command to a highly optimized, device-specific library:\n",
    "-   If `device='cpu'`, ATen calls libraries like `oneDNN`.\n",
    "-   If `device='cuda'`, ATen calls NVIDIA's legendary `cuDNN` library.\n",
    "-   If `device='mps'`, ATen calls Apple's `Metal` framework.\n",
    "\n",
    "This brilliant design makes your PyTorch code incredibly portable. You write the incantation once, and ATen ensures it is executed with maximum power on whatever hardware you possess!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd68f5e5",
   "metadata": {},
   "source": [
    "### 2. The Ritual: Dynamic Device Placement\n",
    "\n",
    "A true PyTorch master does not hardcode their device! That is the way of the amateur. We shall write a glorious, platform-agnostic spell that automatically detects and selects the most powerful computational device available.\n",
    "\n",
    "The hierarchy is clear: `CUDA` is the sanctum sanctorum, `MPS` is the respected wizard's tower, and `CPU` is our reliable home laboratory. Our code shall seek the most powerful realm first.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Our Grand Spell for Selecting the Best Device\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Mwahahaha! We have awakened the CUDA beast!\")\n",
    "elif torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "    print(\"Behold! The power of Apple's Metal Performance Shaders!\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"The humble CPU will have to suffice for today's experiments.\")\n",
    "\n",
    "print(f\"\\\\nSelected device: {device}\\\\n\")\n",
    "\n",
    "# --- Summoning and Teleporting Tensors ---\n",
    "\n",
    "# 1. Summon a tensor directly on the chosen device\n",
    "tensor_on_device = torch.randn(2, 3, device=device)\n",
    "print(f\"Tensor summoned directly on '{tensor_on_device.device}'\")\n",
    "print(tensor_on_device)\n",
    "\n",
    "# 2. Teleport a CPU tensor to the device using the .to() spell\n",
    "cpu_tensor = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "print(f\"\\\\nA CPU tensor, minding its own business: {cpu_tensor.device}\")\n",
    "\n",
    "teleported_tensor = cpu_tensor.to(device)\n",
    "print(f\"Teleported to '{teleported_tensor.device}'!\")\n",
    "print(teleported_tensor)\n",
    "\n",
    "# IMPORTANT: Operations between tensors on different devices will FAIL!\n",
    "# This would cause a RuntimeError:\n",
    "# try:\n",
    "#     result = cpu_tensor + teleported_tensor\n",
    "# except RuntimeError as e:\n",
    "#     print(f\"\\\\nAs expected, chaos ensues: {e}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d19d5d61",
   "metadata": {},
   "source": [
    "# Part 4: Practical speed trials\n",
    "\n",
    "Testing the bfloat vs float16 and quantized operations \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "563584f1",
   "metadata": {},
   "source": [
    "### Speed Trials: Precision vs. Performance\n",
    "\n",
    "Now for a truly electrifying experiment! We shall create colossal tensors of different floating-point `dtypes` and subject them to a barrage of mathematical operations. Witness the dramatic speed-up that 16-bit formats provide!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe7de2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- CPU Speed Trials ---\n",
      "Float32 on CPU took: 0.011775 seconds\n",
      "Float16 on CPU took: 0.005535 seconds (Often slower on CPU!)\n",
      "BFloat16 on CPU took: 0.001954 seconds\\n\n",
      "--- GPU Speed Trials ---\n",
      "Float32 on GPU took: 0.054487 seconds\n",
      "Float16 on GPU took: 0.000000 seconds (Diabolical Speed!)\n",
      "BFloat16 on GPU took: 0.000000 seconds (Also incredibly fast!)\n"
     ]
    }
   ],
   "source": [
    "# A utility for our speed trials\n",
    "def time_operation(tensor):\n",
    "    start_time = time.time()\n",
    "    # A sequence of intense mathematical transformations!\n",
    "    torch.exp(torch.sin(tensor))\n",
    "    end_time = time.time()\n",
    "    return end_time - start_time\n",
    "\n",
    "# A colossal tensor for our experiment!\n",
    "large_tensor_cpu = torch.randn(2000, 2000)\n",
    "\n",
    "# --- SPEED TRIALS ON CPU ---\n",
    "print(\"--- CPU Speed Trials ---\")\n",
    "time_fp32_cpu = time_operation(large_tensor_cpu.clone())\n",
    "print(f\"Float32 on CPU took: {time_fp32_cpu:.6f} seconds\")\n",
    "\n",
    "# Float16 on CPU can be slower as it's not natively supported for computation\n",
    "time_fp16_cpu = time_operation(large_tensor_cpu.clone().to(torch.float16))\n",
    "print(f\"Float16 on CPU took: {time_fp16_cpu:.6f} seconds (Often slower on CPU!)\")\n",
    "\n",
    "# BFloat16 on CPU\n",
    "time_bf16_cpu = time_operation(large_tensor_cpu.clone().to(torch.bfloat16))\n",
    "print(f\"BFloat16 on CPU took: {time_bf16_cpu:.6f} seconds\\\\n\")\n",
    "\n",
    "\n",
    "# --- SPEED TRIALS ON GPU (if available) ---\n",
    "if torch.cuda.is_available():\n",
    "    print(\"--- GPU Speed Trials ---\")\n",
    "    large_tensor_gpu = large_tensor_cpu.to(\"cuda\")\n",
    "\n",
    "    # Make sure operations are complete before stopping the timer\n",
    "    torch.cuda.synchronize()\n",
    "    start_time = time.time()\n",
    "    torch.exp(torch.sin(large_tensor_gpu.clone()))\n",
    "    torch.cuda.synchronize()\n",
    "    time_fp32_gpu = time.time() - start_time\n",
    "    print(f\"Float32 on GPU took: {time_fp32_gpu:.6f} seconds\")\n",
    "\n",
    "    large_tensor_gpu_fp16 = large_tensor_gpu.clone().to(torch.float16)\n",
    "    torch.cuda.synchronize()\n",
    "    start_time = time.time()\n",
    "    torch.exp(torch.sin(large_tensor_gpu_fp16))\n",
    "    torch.cuda.synchronize()\n",
    "    time_fp16_gpu = time.time() - start_time\n",
    "    print(f\"Float16 on GPU took: {time_fp16_gpu:.6f} seconds (Diabolical Speed!)\")\n",
    "\n",
    "    large_tensor_gpu_bf16 = large_tensor_gpu.clone().to(torch.bfloat16)\n",
    "    torch.cuda.synchronize()\n",
    "    start_time = time.time()\n",
    "    torch.exp(torch.sin(large_tensor_gpu_bf16))\n",
    "    torch.cuda.synchronize()\n",
    "    time_bf16_gpu = time.time() - start_time\n",
    "    print(f\"BFloat16 on GPU took: {time_bf16_gpu:.6f} seconds (Also incredibly fast!)\")\n",
    "else:\n",
    "    print(\"GPU not available for speed trials. A true pity!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b2fcce",
   "metadata": {},
   "source": [
    "## Part 2.1: A Glimpse into the Dark Art of Quantization\n",
    "\n",
    "Now, my apprentice, I shall grant you a peek into a forbidden realm: **Quantization**. This is the art of shrinking your models, making them faster and more efficient for inference, by converting their weights from high-precision `float32` into low-precision integers, most commonly `int8`!\n",
    "\n",
    "**Why does this matter?**\n",
    "Imagine forging a colossal golem (`float32` model) that requires immense energy (memory and compute) to move. Quantization is like transmuting that golem into a swarm of nimble, lightning-fast sprites (`int8` model).\n",
    "\n",
    "-   **Speed:** Integer arithmetic is vastly faster on modern CPUs and specialized hardware than floating-point math.\n",
    "-   **Size:** An `int8` model can be **4x smaller** than its `float32` counterpart! This is critical for deploying models on devices with limited memory, like phones or embedded systems.\n",
    "\n",
    "We are merely scratching the surface here, but let us witness a simple demonstration of this power!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aa4bb69",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original fp32 weights size: 8.00 MB\n",
      "Quantized int8 weights size: 2.00 MB (4x smaller!)\\n\n",
      "Float32 matmul took: 0.003999 seconds\n",
      "Simulated Int8 matmul took: 0.002805 seconds (Conceptually MUCH faster on compatible hardware!)\n",
      "\\nAverage difference between fp32 and dequantized output: 5.5590\n",
      "A small price in precision for a monumental gain in speed and size! Mwahahaha!\n"
     ]
    }
   ],
   "source": [
    "# --- A Simple Quantization Demonstration ---\n",
    "\n",
    "# 1. Our \"Model\": A large float32 weight matrix\n",
    "fp32_weights = torch.randn(2048, 1024)\n",
    "# Our \"Input\": A typical input tensor\n",
    "input_tensor = torch.randn(1, 2048)\n",
    "\n",
    "# 2. The Quantization Spell\n",
    "def quantize_tensor(tensor):\n",
    "    # Find the scale and zero_point\n",
    "    q_min, q_max = -128, 127\n",
    "    scale = (tensor.max() - tensor.min()) / (q_max - q_min)\n",
    "    zero_point = q_min - (tensor.min() / scale)\n",
    "    zero_point = int(zero_point.round().item())\n",
    "    \n",
    "    # Quantize\n",
    "    quantized_tensor = (tensor / scale + zero_point).round().to(torch.int8)\n",
    "    return quantized_tensor, scale, zero_point\n",
    "\n",
    "def dequantize_tensor(quantized_tensor, scale, zero_point):\n",
    "    return (quantized_tensor.float() - zero_point) * scale\n",
    "\n",
    "int8_weights, scale, zero_point = quantize_tensor(fp32_weights)\n",
    "\n",
    "print(f\"Original fp32 weights size: {fp32_weights.element_size() * fp32_weights.nelement() / 1024**2:.2f} MB\")\n",
    "print(f\"Quantized int8 weights size: {int8_weights.element_size() * int8_weights.nelement() / 1024**2:.2f} MB (4x smaller!)\\\\n\")\n",
    "\n",
    "# 3. Performance Comparison\n",
    "\n",
    "# Time float32 matrix multiplication\n",
    "start_time = time.time()\n",
    "fp32_output = torch.matmul(input_tensor, fp32_weights)\n",
    "fp32_time = time.time() - start_time\n",
    "\n",
    "# Time int8 matrix multiplication (note: PyTorch needs specific functions for this,\n",
    "# but we simulate the concept by casting for the matmul)\n",
    "# In a real scenario, you'd use a quantized kernel for max speed.\n",
    "int8_input = torch.round(input_tensor / scale + zero_point).to(torch.int8) # In reality input is quantized too\n",
    "start_time = time.time()\n",
    "# We need to use a function that supports int8 matmul, this is a conceptual demonstration\n",
    "# For a real speedup, one would use `torch.nn.quantized.functional.linear`\n",
    "# Here we just cast to a larger int type for the matmul to run\n",
    "int8_output_simulated = torch.matmul(int8_input.to(torch.int32), int8_weights.to(torch.int32))\n",
    "int8_time = time.time() - start_time\n",
    "\n",
    "print(f\"Float32 matmul took: {fp32_time:.6f} seconds\")\n",
    "print(f\"Simulated Int8 matmul took: {int8_time:.6f} seconds (Conceptually MUCH faster on compatible hardware!)\")\n",
    "\n",
    "# 4. Compare the results\n",
    "dequantized_output = dequantize_tensor(int8_output_simulated.to(torch.int8), scale, zero_point) # This is not correct way to dequantize the output of matmul\n",
    "# A proper dequantization of matmul output would be `output * input_scale * weight_scale`\n",
    "# This is just for demonstration purposes\n",
    "dequantized_output_proper = (int8_output_simulated.float() * scale * scale) # A simplification\n",
    "\n",
    "# Let's check the difference\n",
    "average_diff = torch.mean(torch.abs(fp32_output - dequantized_output_proper))\n",
    "print(f\"\\\\nAverage difference between fp32 and dequantized output: {average_diff:.4f}\")\n",
    "print(\"A small price in precision for a monumental gain in speed and size! Mwahahaha!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1ed9fe",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5a3b9f3",
   "metadata": {},
   "source": [
    "## Your Mission: Forge Your Own Creation!\n",
    "\n",
    "A true master never ceases to practice. I leave you with these challenges to solidify your newfound power. Do not be afraid to experiment‚Äîto the lab!\n",
    "\n",
    "1.  **The Alchemist's Transmutation**:\n",
    "    *   Create a `(4, 4)` tensor of random numbers on the `cpu` with the default `float32` dtype.\n",
    "    *   Print its `dtype` and `device`.\n",
    "    *   Transmute this tensor into a `bfloat16` tensor and teleport it to the most powerful device your machine possesses.\n",
    "    *   Print its new `dtype` and `device`.\n",
    "\n",
    "2.  **The Precision Analyst**:\n",
    "    *   Create a `float64` tensor containing the number `[3.141592653589793]`.\n",
    "    *   Cast it down to `float32` and then to `float16`.\n",
    "    *   Print the value at each stage. Observe the loss of precision as you step down the ladder of alchemy!\n",
    "\n",
    "3.  **The Device-Aware Calculator**:\n",
    "    *   Create two large `(1000, 1000)` random tensors, `a` and `b`.\n",
    "    *   Write a piece of code that checks if a powerful device (`cuda` or `mps`) is available.\n",
    "    *   If it is, move both tensors to that device before performing a matrix multiplication (`a @ b`).\n",
    "    *   If not, perform the operation on the `cpu`.\n",
    "    *   Print the device on which the operation was performed. This is the foundation of all robust neural network code!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your code for the final challenges goes here!\n",
    "\n",
    "# Challenge 1: The Alchemist's Transmutation\n",
    "print(\"--- Challenge 1: The Alchemist's Transmutation ---\")\n",
    "cpu_tensor_fp32 = torch.randn(4, 4)\n",
    "print(f\"Original tensor device: {cpu_tensor_fp32.device}, dtype: {cpu_tensor_fp32.dtype}\")\n",
    "\n",
    "# Use the device we selected earlier!\n",
    "transmuted_tensor = cpu_tensor_fp32.to(device=device, dtype=torch.bfloat16)\n",
    "print(f\"Transmuted tensor device: {transmuted_tensor.device}, dtype: {transmuted_tensor.dtype}\\\\n\")\n",
    "\n",
    "\n",
    "# Challenge 2: The Precision Analyst\n",
    "print(\"--- Challenge 2: The Precision Analyst ---\")\n",
    "pi_fp64 = torch.tensor([3.141592653589793], dtype=torch.float64)\n",
    "pi_fp32 = pi_fp64.to(torch.float32)\n",
    "pi_fp16 = pi_fp64.to(torch.float16)\n",
    "\n",
    "print(f\"Float64 (Original): {pi_fp64.item():.15f}\")\n",
    "print(f\"Float32 (Reduced):  {pi_fp32.item():.15f}\")\n",
    "print(f\"Float16 (Sacrificed): {pi_fp16.item():.15f}\\\\n\")\n",
    "\n",
    "\n",
    "# Challenge 3: The Device-Aware Calculator\n",
    "print(\"--- Challenge 3: The Device-Aware Calculator ---\")\n",
    "a = torch.randn(1000, 1000)\n",
    "b = torch.randn(1000, 1000)\n",
    "\n",
    "# We already have our best `device` from the spell cast earlier!\n",
    "a_on_device = a.to(device)\n",
    "b_on_device = b.to(device)\n",
    "\n",
    "# Perform the operation\n",
    "result = a_on_device @ b_on_device\n",
    "print(f\"Matrix multiplication performed on: {result.device}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "240fb5a4",
   "metadata": {},
   "source": [
    "------\n",
    "\n",
    "\n",
    "\n",
    "## Professor Torchenstein's Outro\n",
    "\n",
    "Mwahahaha! Do you feel it? The hum of raw computational power at your fingertips? You have transcended the mundane world of default settings and seized control of the very essence of your tensors. You are no longer a mere summoner; you are an **alchemist** and a **dimensional traveler**!\n",
    "\n",
    "You have learned to choose your weapons‚Äîthe precise `dtype` for the task at hand and the mightiest `device` for your computations. This knowledge is the bedrock upon which all great neural architectures are built.\n",
    "\n",
    "But do not rest easy! Our journey has just begun. The tensors are humming, eager for the next lesson where we shall unleash their raw mathematical power with **Elemental Tensor Alchemy**!\n",
    "\n",
    "Until then, keep your learning rates high and your devices hotter! The future... is computational!\n",
    "\n",
    "<video controls width=\"100%\"  src=\"/assets/images/torchenstein_maniacal_laugh_close_up.mp4\" title=\"Professor Torchenstein's maniacal laugh\">\n",
    "  Your browser does not support the video tag. Please update your browser to view this content.\n",
    "</video>\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-course-QuR2iHBk-py3.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
